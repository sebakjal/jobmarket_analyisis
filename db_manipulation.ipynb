{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34fb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from python_scripts import *\n",
    "from config import *\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4d3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pl.read_parquet('jobs_from_7.0_days_2025-04-18.parquet').rename(\n",
    "    {\"Seniority level\": \"seniority_level\",\n",
    "     \"Employment type\": \"employment_type\",\n",
    "     \"Job function\": \"job_function\",\n",
    "     \"Industries\": \"industries\",}\n",
    ")\n",
    "df2 = pl.read_csv('joblist_2025-04-13_.csv').drop('cloud_focus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487f8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "db_file = 'my_project.duckdb' # Use .duckdb extension by convention\n",
    "table_name = 'test_table' #Choose a name for your SQL table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0ab44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 10)\n",
      "┌───────┬─────────┬──────────┬──────┬───┬───────────────┬──────────────┬──────────────┬────────────┐\n",
      "│ title ┆ company ┆ location ┆ date ┆ … ┆ seniority_lev ┆ employment_t ┆ job_function ┆ industries │\n",
      "│ ---   ┆ ---     ┆ ---      ┆ ---  ┆   ┆ el            ┆ ype          ┆ ---          ┆ ---        │\n",
      "│ u32   ┆ u32     ┆ u32      ┆ u32  ┆   ┆ ---           ┆ ---          ┆ u32          ┆ u32        │\n",
      "│       ┆         ┆          ┆      ┆   ┆ u32           ┆ u32          ┆              ┆            │\n",
      "╞═══════╪═════════╪══════════╪══════╪═══╪═══════════════╪══════════════╪══════════════╪════════════╡\n",
      "│ 31    ┆ 31      ┆ 31       ┆ 31   ┆ … ┆ 31            ┆ 31           ┆ 31           ┆ 31         │\n",
      "└───────┴─────────┴──────────┴──────┴───┴───────────────┴──────────────┴──────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04c2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"my_project.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648bcd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┐\n",
       "│ count_star() │\n",
       "│    int64     │\n",
       "├──────────────┤\n",
       "│           65 │\n",
       "└──────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"SELECT COUNT(*) FROM test_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e1a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────────────────────────────────────┬─────────────────────┬───────────────────────────────────────────────┬────────────┬────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────┬─────────────────┬─────────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────┐\n",
       "│                     title                      │       company       │                   location                    │    date    │                    job_url                     │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           job_description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           │ seniority_level  │ employment_type │                    job_function                     │                                   industries                                    │\n",
       "│                    varchar                     │       varchar       │                    varchar                    │    date    │                    varchar                     │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               varchar                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               │     varchar      │     varchar     │                       varchar                       │                                     varchar                                     │\n",
       "├────────────────────────────────────────────────┼─────────────────────┼───────────────────────────────────────────────┼────────────┼────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┼─────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Data Engineer Junior                           │ LISIT               │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205294902/ │ En Lisit, nos dedicamos a crear, desarrollar e implementar herramientas y servicios de software que automatizan y optimizan procesos, siempre con un fuerte enfoque en la innovación y los desafíos que se presentan. Nuestro objetivo es fomentar la eficacia operativa de nuestros clientes, ayudándoles a alcanzar sus metas de transformación mediante un acompañamiento consultivo integral. Actualmente, estamos en búsqueda de un Data Engineer Junior que se una a nuestro equipo apasionado por la tecnología y el aprendizaje continuo.\\r\\n Funciones del Rol\\r\\nComo Data Engineer Junior, Serás Parte Esencial Del Equipo Encargado De Manejar y Optimizar El Flujo De Datos De La Organización. Tus Principales Responsabilidades Incluirán\\r\\n- Colaborar en la recopilación y procesamiento de datos relacionales y no relacionales.\\r\\n- Trabajar con lenguajes de programación, especialmente Python, para crear soluciones de datos efectivas.\\r\\n- Implementar y mantener los procesos de integración en ambientes cloud como GCP o Azure.\\r\\n- Realizar consultas y manipulación de bases de datos utilizando SQL.\\r\\n- Aprender y adaptarte a nuevas tecnologías y herramientas en el entorno de la nube.\\r\\n Descripción del Perfil\\r\\nBuscamos Un Perfil Proactivo, Con Conocimientos Intermedios En Python y Disposición Para Aprender Sobre Nuevas Tecnologías. El Candidato Ideal Deberá Tener\\r\\n- Experiencia básica a intermedia en programación Python.\\r\\n- Habilidades en el uso y tratamiento de datos en ambientes tanto relacionales como no relacionales.\\r\\n- Conocimientos fundamentales en tecnologías de nube, incluyendo GCP o Azure.\\r\\n- Experiencia en el uso del lenguaje SQL.\\r\\n- Bajo es requisito pero se valorará el conocimiento en Power BI.\\r\\n Habilidades Deseables\\r\\nSería excelente contar con conocimientos adicionales en herramientas de visualización de datos como Power BI. Además, habilidad para trabajar en equipo y una mentalidad orientada al aprendizaje continuo son altamente valoradas.\\r\\n Beneficios de Trabajar con Nosotros\\r\\nEn Lisit, Promovemos Un Ambiente De Trabajo Excepcional\\r\\n- Acceso a oportunidades de desarrollo profesional continuo en tecnologías emergentes.\\r\\n- Un equipo apasionado por la innovación y el aprendizaje, donde tu entusiasmo será bienvenido.        \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       │ Entry level      │ Full-time       │ Information Technology                              │ Technology, Information and Internet and Information Technology & Services      │\n",
       "│ Data Engineer                                  │ Xepelin             │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-09 │ https://www.linkedin.com/jobs/view/4205474021/ │ Somos una FinTech que busca democratizar los servicios financieros para todo tipo de empresas. Nos apalancamos en la mejor tecnología para crear soluciones ágiles, personalizadas y transparentes. Nuestro objetivo es ser la FinTech B2B más grande de Latam y convertirnos en el CFO digital de todas las empresas en la región.\\r\\nXepelin nace en Chile en 2019 y, desde entonces, hemos levantado más de USD145 millones en equity y USD300 millones en asset-backed facilities para potenciar el crecimiento en toda la región, especialmente en los países donde hoy operamos, Chile y México. La última ronda de equity fue de USD111 millones, record en Chile y una de las más grandes en América Latina para una FinTech.\\r\\n¿Por qué trabajar en Xepelin?\\r\\n💪 Desafío\\r\\nEstamos sacudiendo una de las industrias más poderosas y competitivas, eliminando fricciones para darle a las Pymes acceso a capital y apalancado en la última tecnología disponible. Todo esto poniendo siempre a nuestras Pymes en el centro.\\r\\nConstruir un banco digital desde cero es un gran proyecto; el producto es complejo y no se puede romper, existen regulaciones estrictas y tendremos que ser mejores que algunas de las corporaciones más grandes y consolidadas del mundo. Pero superar estos desafíos significa que habremos construido algo duradero.\\r\\n💥 Impacto\\r\\nTrabajar en un equipo de clase mundial y automotivado significa autonomía, amplia experiencia en todos los proyectos y ver que tus contribuciones afectan directamente al producto e impactan a nuestras Pymes.\\r\\nTrabajarás con todos nuestros productos, tendrás que tomar decisiones fundamentales. El correcto posicionamiento de ellos marcará su futuro.\\r\\n✔️ Calidad\\r\\nNuestro posicionamiento de marca y productos es algo diferenciador frente al resto del mercado por lo que invertimos mucho en crear productos de calidad que nuestras Pymes respeten y valoren.\\r\\nNos damos el tiempo para pensar fuera de la caja y volver con propuestas innovadoras. Estamos aquí para cambiar la industria!\\r\\n¿Qué estamos buscando? \\r\\nEn Xepelin estamos buscando personas creativas y visionarias que piensen fuera de la caja para sumarse a nuestro equipo. Si te apasiona resolver desafíos interesantes de alto impacto y quieres ser parte de un entorno dinámico que está transformando la industria financiera, ¡Esta oportunidad es para ti!\\r\\nEl rol se integrará a nuestro equipo de \\r\\nData Platform\\r\\n. Si te motiva el desafío de construir soluciones innovadoras en un entorno de rápido cambio, queremos conocerte.\\r\\nUnete a nosotros, crezcamos juntos!\\r\\nPrincipales responsabilidades...\\r\\n-  Diseñar, crear y mantener pipelines de datos\\r\\n-  Mantener y optimizar la infraestructura de datos necesaria para una extracción precisa, transformación y carga de datos de una amplia variedad de fuentes de datos\\r\\n-  Automatizar los flujos de trabajo de datos, como la ingesta de datos, la agregación y el procesamiento de ETL o ELT\\r\\n-  Preparar datos sin procesar en almacenes de datos en un conjunto de datos consumibles para fines técnicos y partes interesadas no técnicas\\r\\n-  Crear, mantener e implementar productos de datos para equipos de análisis y ciencia de datos en Plataformas en la nube, preferentemente en GCP, y/o AWS\\r\\n-  Desarrollar sistemas y arquitectura que soporten las diferentes etapas del flujo de Machine Learning\\r\\n¿Qué necesitas para brillar?\\r\\n- Conocimientos en alguna Nube, preferentemente GCP o AWS (en ese orden de preferencia)\\r\\n- Conocimientos intermedio/avanzado en Python\\r\\n- Conocimiento intermedio/avanzado de SQL\\r\\n- Experiencia trabajando almacenamiento en la nube como GCS o AWS S3\\r\\n- Experiencia desplegando aplicaciones en ambientes serverless como Cloud Functions o AWS Lambda\\r\\n- Conocimientos administrando y desplegando algún orquestador, por ejemplo: Dagster, Apache Airflow, Prefect, etc\\r\\n- Excelentes habilidades para trabajar en equipo. Ser humilde y saber colaborar, un Team Player!\\r\\n- Saber escuchar a tus stakeholders y poder traducir eso en requerimientos y ejecutarlos con tu equipo\\r\\n- Trabajo proactivo y responsable\\r\\n- Conocimientos en DBT\\r\\n- Conocimientos y manejo de lenguajes de programación y/o frameworks, NodeJS, Golang, por ejemplo\\r\\n- Experiencia en MLOps\\r\\n- No tener miedo a tomar decisiones y liderar proyectos\\r\\n- Foco en impacto e historia consistente entregando resultados para usuarios y el negocio\\r\\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\\r\\n- Te sientes cómodo cuestionando el status-quo de los servicios financieros, adaptándose rápidamente a los cambios, y presentando claramente tus ideas y conceptos para debatirlos en equipo\\r\\nNuestros Beneficios:\\r\\n🌴 Xepelin Balance\\r\\nVacaciones:\\r\\n 15 días hábiles. Por cada año que cumplas en Xepelin, te damos un día extra de vacaciones.\\r\\nBalance days:\\r\\n 10 días libres adicionales al año, para disfrutar como quieras.\\r\\nTrabajo híbrido y flexibilidad horaria según el rol. Trabajamos por objetivos.\\r\\nBeneficios Flexibles: \\r\\nPuntos flexibles en tu moneda local al mes para gastar en lo que quieras.\\r\\nXepelin Fun:\\r\\n Actividades de encuentro financiadas por Xepelin para divertirnos juntos.\\r\\n🚀 Xepelin Performance & Career\\r\\nPlataformas de capacitación:\\r\\n Convenios con las mejores plataformas, como Reforge, Udemy y DataCamp.\\r\\nKit de Bienvenida: \\r\\ntodo lo que necesitas para comenzar tu viaje en Xepelin 😊\\r\\n🤝 Xepelin Cares\\r\\nCobertura de salud:\\r\\n contamos con convenios de salud con proveedores de calidad o reembolsos según el país donde te encuentres.\\r\\nPost Natal:\\r\\n te damos una semana extra de licencia post natal. ¡Nos interesa que estés con tu familia y seres queridos!\\r\\nMatrimonio plus:\\r\\n Lleva tus planes al siguiente nivel, con una gift card y extendiendo tu permiso legal por matrimonio con dos días de regalo por Xepelin.        \\r\\n            \\r\\n                            \\r\\n             │ Not Applicable   │ Full-time       │ Information Technology                              │ Software Development, IT Services and IT Consulting, and Biotechnology Research │\n",
       "│ Data Engineer                                  │ BC Tecnología       │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205297472/ │ Could not find Job Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      │ NULL             │ NULL            │ NULL                                                │ NULL                                                                            │\n",
       "│ Data Engineer                                  │ 2Brains             │ Chile                                         │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205294882/ │ 2Brains es una empresa dedicada a construir y desarrollar el Futuro Digital de nuestros clientes, con una visión excepcional que radica en la integración sinérgica de estrategia, diseño y tecnología, un tríptico poderoso que impulsa el crecimiento de empresas y disruptores tecnológicos.\\r\\nContamos con un nutrido equipo de más de 200 profesionales, verdaderos artífices de la innovación digital. En el corazón de nuestra labor, destacamos como líderes indiscutibles, canalizando años de experiencia hacia la creación de plataformas tecnológicas adaptables y productos digitales de clase mundial.\\r\\nEn 2Brains, no solo somos consultores, somos arquitectos de experiencias digitales. Aspiramos a ir más allá de las expectativas, estableciendo nuevos estándares en la industria. Descubre cómo damos vida a la innovación, cómo convertimos ideas en resultados tangibles y cómo, junto a nosotros, puedes forjar un futuro digital brillante.\\r\\n El/la Data Engineer de 2Brains \\r\\nSe encarga de participar en el diseño y desarrollo de los nuevos modelos de información de gestión y las mantenciones evolutivas de los existentes. Participar en las iniciativas de Analítica avanzada del área, apoyando las exploración de modelos de información internos y externos (Data Discovery). Obtener datos históricos desde múltiples fuentes de información interna para apoyar las iniciativas de analítica avanzada del equipo.\\r\\nEl/la Data Engineer de 2Brains debe\\r\\n- Construir y optimizar pipelines de datos para la ingesta, transformación y carga eficiente de información.\\r\\n- Manejar infraestructuras en la nube (AWS, GCP, Azure), asegurando escalabilidad y eficiencia en costos.\\r\\n- Automatizar y monitorear procesos mediante herramientas de DevOps como Airflow, Terraform o Kubernetes.\\r\\n- Implementar controles de calidad y gobernanza para garantizar la integridad y disponibilidad de los datos.\\r\\n- Colaborar con equipos de Data Science, Producto y Desarrollo para diseñar soluciones alineadas con las necesidades del negocio.\\r\\n Qué conocimientos buscamos en/la Data Engineer\\r\\n- Excluyente Experiencia trabajando con tecnologías de BI\\r\\n- Experiencia en la construcción/operación de sistemas distribuidos de extracción, ingestión y procesamiento de grandes conjuntos de datos de gran disponibilidad.\\r\\n- Capacidad demostrable en modelado de datos, desarrollo de ETL y almacenamiento de datos.\\r\\n- Experiencia en el uso de herramientas de informes de inteligencia empresarial (Power BI)\\r\\n- Excluyente conocimiento en consumo de microservicios de APIs Rest\\r\\n- Excluyente conocimiento en Git , Bitbucket, Docker,Jenkins,Webhooks\\r\\n- Programación con Python y bases sólidas de ingeniería de software.\\r\\n- Automatización y scripting.\\r\\n- Uso de librerías de Python para manipulación y análisis de datos y Apache Spark.\\r\\n- Conocimientos en bases de datos SQL y NoSQL.\\r\\n- Conocimiento en CI/CD, Dataflow\\r\\n- Conocimiento en S3, Redshift y Glue AWS\\r\\n Que competencias buscamos en/la Data Engineer \\r\\n- Empatía\\r\\n- Buena capacidad de comunicación.\\r\\n- Colaboración y trabajo en equipo.\\r\\n- Proactividad.\\r\\n- Autonomía.\\r\\n- Foco en los objetivos de proyectos.\\r\\n Condiciones\\r\\nTrabajar con un equipo de alto rendimiento, aprendemos y nos desarrollamos juntos\\r\\nAcceso a grandes clientes y proyectos desafiantes\\r\\nAprendizaje y crecimiento permanente, organizamos meetups, capacitaciones y actividades culturales\\r\\nUn entorno de trabajo flexible y dinámico\\r\\nBeneficios especiales: día libre para tu cumpleaños, días de descanso a convenir.\\r\\n                \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      │ Mid-Senior level │ Full-time       │ Information Technology                              │ Technology, Information and Internet and Information Technology & Services      │\n",
       "│ Data Engineer                                  │ Falabella           │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-11 │ https://www.linkedin.com/jobs/view/4207045620/ │ Descripción Empresa\\r\\nSomos más de 80 mil personas que cada día trabajamos por el firme Propósito - Simplificar y Disfrutar más la Vida. Estamos presentes en 9 países y compuestos por grandes marcas posicionadas de diversas industrias. Falabella Retail, Sodimac, Banco Falabella, Tottus, Mallplaza, Falabella.com, Falabella Inmobiliario. Cada una de éstas nos hace ser quienes somos, y es entre todos, como Un Solo Equipo, que buscamos diariamente reinventarnos y superar las experiencias de nuestros clientes.\\r\\nSi eres trabajador de Falabella, revisa todos los cursos disponibles en la Academia Falabella, que te ayudarán a seguir impulsando tu desarrollo y preparar tu próxima aventura con nosotros!\\r\\nSOMOS UNA EMPRESA QUE APOYA LA LEY 21015, APOYAMOS LA DIVERSIDAD Y LA INCLUSIÓN EN TODAS SUS FORMAS, SIN IMPORTAR RELIGIÓN, RAZA, GÉNERO, SITUACIÓN DE DISCAPACIDAD, NACIONALIDAD.\\r\\nFunciones Del Cargo\\r\\n¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti!\\r\\nEn Falabella Retail buscamos a nuestro/a próximo/a Data Engineer, con base en Santiago, Chile.\\r\\nSomos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\\r\\n¿Cuál es el principal objetivo del cargo?\\r\\nLiderar la construcción y mantención de estructuras de datos, así como la arquitectura tecnológica requerida para el procesamiento de apps.\\r\\n¿Qué harás en el día a día?\\r\\n-  Desarrollo, implementación de procesos ETL.\\r\\n-  Levantamiento de requerimientos funcionales y técnicos relacionados con los clientes internos.\\r\\n-  Implementar modelos de datos automatizados para transformar datos de acuerdo a los requisitos del negocio.\\r\\n-  Migración de datos desde entornos on-premise a entornos Cloud.\\r\\n-  Trabajar con tecnologías Google Cloud Platform (Big Query).\\r\\n¿Qué necesitas para postular?\\r\\n-  Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\\r\\n-  Conocimiento en SQL (excluyente)\\r\\n-  Sólidos conocimientos en Google Cloud Platform (excluyente)\\r\\n-  Conocimiento avanzado en Python (excluyente)\\r\\n-  Conocimiento y experiencia trabajando en GIT (excluyente)\\r\\n-  Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\\r\\nEn Nuestro Equipo Encontrarás\\r\\n-  Espacios para crear e innovar.\\r\\n-  Serás parte de un lugar lleno de oportunidades de desarrollo.\\r\\n-  Tener un trabajo con sentido y donde se promueve la calidad de vida.\\r\\n-  Participar en voluntariados.\\r\\n-  ¡Pertenecer a una empresa llena de energía!\\r\\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\\r\\nSomos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\\r\\nRequisitos\\r\\n- Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\\r\\n- Conocimiento en SQL (excluyente)\\r\\n- Sólidos conocimientos en Google Cloud Platform (excluyente)\\r\\n- Conocimiento avanzado en Python (excluyente)\\r\\n- Conocimiento y experiencia trabajando en GIT (excluyente)\\r\\n- Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\\r\\nCondiciones Oferta\\r\\nDescripción proceso de selección:\\r\\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\\r\\nPara Postular Solo Necesitas\\r\\n-  Postular a la oferta\\r\\n-  Revisar tu email\\r\\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\\r\\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\\r\\n                \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    │ Mid-Senior level │ Full-time       │ Information Technology                              │ Retail                                                                          │\n",
       "│ Data Engineer                                  │ NeuralWorks         │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205500303/ │ NeuralWorks es una compañía de alto crecimiento fundada hace 3 años. Estamos trabajando a toda máquina en cosas que darán que hablar.\\r\\nSomos un equipo donde se unen la creatividad, curiosidad y la pasión por hacer las cosas bien. Nos arriesgamos a explorar fronteras donde otros no llegan: un modelo predictor basado en monte carlo, una red convolucional para detección de caras, un sensor de posición bluetooth, la recreación de un espacio acústico usando finite impulse response.\\r\\nEstos son solo algunos de los desafíos, donde aprendemos, exploramos y nos complementamos como equipo para lograr cosas impensadas.\\r\\nTrabajamos en proyectos propios y apoyamos a corporaciones en partnerships donde codo a codo combinamos conocimiento con creatividad, donde imaginamos, diseñamos y creamos productos digitales capaces de cautivar y crear impacto.\\r\\n👉 Conoce más sobre nosotros\\r\\n Descripción del trabajo\\r\\nEl equipo de Data y Analytics trabaja en diferentes proyectos que combinan volúmenes de datos enormes e IA, como detectar y predecir fallas antes que ocurran, optimizar pricing, personalizar la experiencia del cliente, optimizar uso de combustible, detectar caras y objetos usando visión por computador.\\r\\nDentro del equipo multidisciplinario con Data Scientist, Translators, DevOps, Data Architect, tu rol será clave en construir y proveer los sistemas e infraestructura que permiten el desarrollo de estos servicios, formando los cimientos sobre los cuales se construyen los modelos que permiten generar impacto, con servicios que deben escalar, con altísima disponibilidad y tolerantes a fallas, en otras palabras, que funcionen. Además, mantendrás tu mirada en los indicadores de capacidad y performance de los sistemas.\\r\\nEn cualquier proyecto que trabajes, esperamos que tengas un gran espíritu de colaboración, pasión por la innovación y el código y una mentalidad de automatización antes que procesos manuales.\\r\\nComo Data Engineer, Tu Trabajo Consistirá En\\r\\n- Participar activamente durante el ciclo de vida del software, desde inception, diseño, deploy, operación y mejora.\\r\\n- Apoyar a los equipos de desarrollo en actividades de diseño y consultoría, desarrollando software, frameworks y capacity planning.\\r\\n- Desarrollar y mantener arquitecturas de datos, pipelines, templates y estándares.\\r\\n- Conectarse a través de API a otros sistemas (Python)\\r\\n- Manejar y monitorear el desempeño de infraestructura y aplicaciones.\\r\\n- Asegurar la escalabilidad y resiliencia.\\r\\n Calificaciones clave\\r\\n- Estudios de Ingeniería Civil en Computación o similar.\\r\\n- Experiencia práctica de al menos 3 años en entornos de trabajo como Data Engineer, Software Engineer entre otros.\\r\\n- Experiencia con Python. Entendimiento de estructuras de datos con habilidades analíticas relacionadas con el trabajo con conjuntos de datos no estructurados, conocimiento avanzado de SQL, incluida optimización de consultas.\\r\\n- Pasión en problemáticas de procesamiento de datos.\\r\\n- Experiencia con servidores cloud (GCP, AWS o Azure), especialmente el conjunto de servicios de procesamiento de datos.\\r\\n- Buen manejo de inglés, sobre todo en lectura donde debes ser capaz de leer un paper, artículos o documentación de forma constante.\\r\\n- Habilidades de comunicación y trabajo colaborativo.\\r\\n¡En NeuralWorks nos importa la diversidad! Creemos firmemente en la creación de un ambiente laboral inclusivo, diverso y equitativo. Reconocemos y celebramos la diversidad en todas sus formas y estamos comprometidos a ofrecer igualdad de oportunidades para todos los candidatos.\\r\\n“Los hombres postulan a un cargo cuando cumplen el 60% de las calificaciones, pero las mujeres sólo si cumplen el 100%.” D. Gaucher , J. Friesen and A. C. Kay, Journal of Personality and Social Psychology, 2011.\\r\\nTe invitamos a postular aunque no cumplas con todos los requisitos.\\r\\n Nice to have\\r\\n- Agilidad para visualizar posibles mejoras, problemas y soluciones en Arquitecturas.\\r\\n- Experiencia en Infrastructure as code, observabilidad y monitoreo.\\r\\n- Experiencia en la construcción y optimización de data pipelines, colas de mensajes y arquitecturas big data altamente escalables.\\r\\n- Experiencia en procesamiento distribuido utilizando servicios cloud.\\r\\n Beneficios\\r\\n- MacBook Air M2 o similar (con opción de compra hiper conveniente)\\r\\n- Bono por desempeño\\r\\n- Bono de almuerzo mensual y almuerzo de equipo los viernes\\r\\n- Seguro complementario de salud y dental\\r\\n- Horario flexible\\r\\n- Flexibilidad entre oficina y home office\\r\\n- Medio día libre el día de tu cumpleaños\\r\\n- Financiamiento de certificaciones\\r\\n- Inscripción en Coursera con plan de entrenamiento a medida\\r\\n- Estacionamiento de bicicletas\\r\\n- Vestimenta informal\\r\\n- Programa de referidos\\r\\n- Salida de “teambuilding” mensual        \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 │ Not Applicable   │ Full-time       │ Information Technology                              │ Technology, Information and Internet and Information Technology & Services      │\n",
       "│ Ingeniero de Datos                             │ Devaid              │ Chile                                         │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205299283/ │ En Devaid> nos apasionan los desafíos tecnológicos y nuestros clientes lo saben. Por lo anterior, nos plantean problemáticas que nos obligan a estar constantemente probando e implementando nuevas tecnologías.\\r\\nTrabajamos fuertemente en la nube ya que somos Partner Premier de Google Cloud en Chile, por lo que tendrás la oportunidad de formarte como un profesional cloud.\\r\\nDependiendo de las necesidades del cliente, ofrece soluciones web, móviles, integración de sistemas, entre otros. Esto permite acceder a la herramienta sin importar el dispositivo ni el lugar dónde se encuentra. Permitimos el trabajo colaborativo entre múltiples usuarios manteniendo una base centralizada de información.\\r\\n Funciones del cargo\\r\\nEsperamos Que Puedas Desempeñarte En Las Siguientes Actividades\\r\\n- Creación de pipelines de carga y transformación de datos.\\r\\n- Modelamiento de datos y creación de Data Warehouse y Data Lakes.\\r\\n- Integración de sistemas.\\r\\n- Creación de modelos de machine learning con herramientas low code autoML.\\r\\nVas a participar como ingeniero de datos en equipos de consultores que prestan servicios a empresas importantes en Chile. En estos equipos participan distintos perfiles, tales como desarrolladores de software, arquitectos de datos y data scientists. Los servicios se prestan de forma remota y son prestados por proyecto (no es outsourcing de recursos), por lo que puedes trabajar desde tu casa sin problemas. Diariamente vas a tener reuniones con tu equipo para coordinar actividades y resolver temas complejos que vayan surgiendo.\\r\\n Requerimientos del cargo\\r\\nLos requisitos para un buen desempeño de las funciones son:\\r\\n- 1 año de experiencia como Data Engineer. \\r\\n- Programación en lenguaje Python, NodeJS o Java (al menos uno de los 3). \\r\\n- Conocimiento de soluciones de Data Warehouse y ETL. \\r\\n- Conocimiento de plataformas de procesamiento de datos como Apache Spark, Dataflow o similares. \\r\\n- Haber trabajado previamente con alguna nube pública (AWS, Azure o GCP).\\r\\nSi no cumples alguno de estos puntos no te desanimes, queremos conocerte igualmente.\\r\\nEl trabajo es 100% remoto, pero es necesario que tengas RUT y/o papeles al día en Chile.\\r\\n Deseables\\r\\nSuman puntos en tu postulación si cumples alguna de las siguientes habilidades, ninguno de estos son excluyentes:\\r\\n- Conocimiento de herramientas Google Cloud, entre ellas Google BigQuery, Dataflow, Data Fusion y Pub Sub. \\r\\n- Experiencia en plataformas de deployment de infraestructura como Terraform. \\r\\n- Experiencia utilizando la herramienta de consola gcloud. \\r\\n Beneficios\\r\\nPrometemos un ambiente muy grato de trabajo, lleno de desafíos y donde podrás ver los proyectos en los que estas involucrada/o siendo utilizados en un corto tiempo activamente por nuestros clientes, lo que siempre es muy gratificante.\\r\\nOtras Actividades\\r\\n- Actividades mensuales (Cupones de Food delivery, juegos en línea, actividades grupales).\\r\\n- Actividad paseo anual: La empresa se junta por 2 días en algún lugar turístico para realizar actividades grupales y unir al equipo.\\r\\n- Día libre flexible en tu cumpleaños.\\r\\n- Capacitaciones en lo que más te guste.\\r\\n- Certificaciones Google Cloud: Programa de certificación en distintas ramas profesionales de GCP, gracias a que somos Partner Premier de Google Cloud en Chile.        \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           │ Entry level      │ Full-time       │ Information Technology                              │ Technology, Information and Internet and Information Technology & Services      │\n",
       "│ Data Engineer (GCP & DataFlow & Bigquery)      │ Option              │ Santiago Metropolitan Area                    │ 2025-04-12 │ https://www.linkedin.com/jobs/view/4208792219/ │ ¿Quiénes somos?\\nEn \\nOption\\n, creemos en un mundo donde las soluciones tecnológicas no tienen límites. Nuestra misión es transformar los desafíos en oportunidades mediante la creación de soluciones innovadoras que potencien la Aceleración Digital. Nuestro equipo es dinámico, colaborativo y apasionado por la tecnología. Únete a una organización que está redefiniendo cómo el mundo utiliza los datos y la tecnología para resolver problemas complejos.\\n¿Qué buscamos?\\nEstamos en la búsqueda de un/a \\nIngeniero/a de Datos\\n para unirse al equipo de Data Services. Este rol será clave en el levantamiento, análisis y migración de procesos ETL desde un Data Lake mal gobernado hacia una arquitectura moderna sobre Google Cloud Platform (GCP). ¡Te estamos buscando!\\n¿Qué te ofrece este puesto?\\n- Participación en un proceso estratégico de migración a la nube.\\n- Un entorno de trabajo colaborativo y con líderes técnicos accesibles.\\n- Uso de tecnologías modernas como GCP, Dataflow y BigQuery.\\n- Trabajo conjunto con equipos de analítica, desarrollo y operación.\\n¿Cuáles serán tus principales responsabilidades?\\n- Levantar y documentar los ETLs actuales en Data Services.\\n- Analizar el ambiente de datos y planificar su migración a GCP.\\n- Tomar iniciativa en la migración de ETLs críticos.\\n- Resolver incidencias relacionadas a ETLs mediante la mesa de ayuda.\\n- Participar en el diseño del plan de migración.\\n- Colaborar con los líderes técnicos y equipos multidisciplinarios.\\n¿Qué necesitas para ser nuestro próximo Ingeniero de Datos?\\nHabilidades Técnicas Excluyentes\\n- Oracle\\n- Python\\n- Dataflow (GCP)\\n- Dataform (GCP)\\n- GitLab\\n- BigQuery (GCP)\\n- Data Modeling\\n- Composer (GCP)\\nHabilidades Técnicas Deseables\\n- Oracle Data Integrator (ODI)\\nUbicación: LATAM\\nModalidad de trabajo:\\n 100% Remoto\\n¡Únete a nuestro equipo y transforma el futuro con nosotros!\\nhttps://www.option.tech        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ Entry level      │ Full-time       │ Information Technology                              │ Information Technology & Services                                               │\n",
       "│ Data Engineer                                  │ ICONSTRUYE          │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-10 │ https://www.linkedin.com/jobs/view/4205295718/ │ En ICONSTRUYE, hemos estado a la vanguardia de la tecnología en la construcción durante más de 20 años. Nuestra robusta plataforma tecnológica es un testimonio de nuestra experiencia y compromiso con la industria. Con más de 4,000 clientes en Chile, Colombia y Perú, nos enorgullecemos de proporcionar soluciones integrales que simplifican la cadena de abastecimiento. Buscamos un Ingeniero de Datos que se una a nosotros en la transformación de la industria de la construcción, siendo el puente entre los datos brutos y aquellos que toman decisiones críticas.\\r\\nTus Funciones Principales\\r\\nTu misión:\\r\\n Ser el puente entre los datos brutos y quienes necesitan realizar análisis y/o tomar decisiones con esos datos.\\r\\n- Garantizar la calidad, integridad y seguridad de los datos.\\r\\n- Colaborar con diversos stakeholders para comprender sus necesidades de datos.\\r\\n- Desarrollar procesos de extracción, transformación y carga (ETL) de datos para nuestro data lake, proporcionando información valiosa para el análisis y toma de decisiones.\\r\\n- Implementar nuevas bases de datos y/o data warehouses para satisfacer las necesidades de la empresa.\\r\\n- Contribuir a la definición de políticas de gobernanza de datos.\\r\\n- Ser una autoridad en la creación, implementación y operación de soluciones escalables y de bajo costo, facilitando el flujo de datos desde sistemas de producción hasta el data lake.\\r\\nRequerimientos Técnicos\\r\\n- Dominio de Python o Go. \\r\\n- Dominio de SQL. \\r\\n- Conocimiento de base de datos relacionales y no relacionales (NoSQL). \\r\\n- Conocimiento de AirFlow, Luigi, Dagster. \\r\\n- Conocimientos de Kafka y/o RabbitMQ. \\r\\n- Conocimiento en Docker y Kubernetes. \\r\\nBeneficios Que Ofrecemos\\r\\n- 🌴 5 días extras de descanso al año.\\r\\n- 🍔 Tarjeta amipass para utilizar en restaurantes, delivery y supermercados.\\r\\n- 👨‍⚕️ Seguro complementario de salud, dental y de vida.\\r\\n- 🏠 Modalidad de trabajo híbrido.\\r\\n- 📠 Flexibilidad con permisos para trámites y asuntos familiares.\\r\\n- 👩‍👦 Jornada reducida en días de vacaciones escolares (viernes medio día).\\r\\n- 🎂 Tarde libre en tu cumpleaños.\\r\\n¡Únete a nosotros y sé parte de nuestra misión de transformar la industria de la construcción!\\r\\n                \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        │ Not Applicable   │ Full-time       │ Information Technology                              │ Technology, Information and Internet and Information Technology & Services      │\n",
       "│ Ingeniero de Datos ($1.400.000 líquidos)       │ Vector              │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-07 │ https://www.linkedin.com/jobs/view/4203857279/ │ Somos una empresa Líder en el rubro TI. Nos encontramos en la búsqueda de Ingeniero de Datos, con experiencia en extracción de datos desde los sistemas fuente de estándares industriales y eléctricos y su almacenaje en los destinos correspondientes, así como también el asegurar la continuidad del funcionamiento de aplicaciones y redes OT que proporcionan datos a PMAC y ROCC.\\r\\nPrincipales Tareas\\r\\n- Sistematizar el traspaso de datos desde los distintos sistemas de la compañía hacia los repositorios correspondientes para su consumo.\\r\\n- Participar en proyectos y entregar soluciones técnicas, procesos y requisitos.\\r\\n- Enfoque en la creación de indicadores y KPI relevantes de las diferentes aplicaciones para reportar a nivel técnico y administrativo.\\r\\n- Realizar revisiones continuas de monitoreo para asegurarse de la continuidad del servicio.\\r\\n- Monitorear y controlar solicitudes y requerimientos de soporte (gestión de tickets).\\r\\n- Gestionar cualquier proceso de solicitud de cambio, asegurándose de que todas las solicitudes estén debidamente documentadas y rastreadas. \\r\\n- Entregar soporte para que los servicios de OT se entreguen de acuerdo con los procedimientos operativos estándar y/o SLAs acordados; con enfoque en el servicio operativo, resolución de problemas y respuesta ágil para el usuario final.\\r\\n- Escalar consultas complejas a la organización de soporte especializado correspondiente.\\r\\n- Identificar oportunidades de mejora del servicio a partir del análisis de tendencias de datos y las necesidades y aportes de los clientes/usuarios.\\r\\n- Involucrarse con stakeholders clave y para comprender sus requisitos y transmitir al área de resolución correspondiente. \\r\\nConocimientos y experiencia \\r\\n- Estudios técnicos o profesionales en Telecomunicaciones, Instrumentación, Sistemas, Computación, Informática, Electrónica o carreras afines).\\r\\n- Instrumentista con conocimiento en programación y transferencia de datos, o Ingeniero de Datos con conocimiento en protocolos industriales y sus estándares de comunicación.\\r\\n- Conocimientos de redes de comunicaciones y soluciones de transporte de datos.\\r\\n- Gestión de datos y visualización usando herramientas cloud (Google, Microsoft).\\r\\n- Conocimientos de un lenguaje de scripting, preferiblemente Python.\\r\\n- Manejo básico de base de datos SQL.\\r\\n- Conocimientos básicos en gestión de accesos e identidades.\\r\\n- Conocimientos básicos en administración de servidores.\\r\\n- Experiencia laboral de 5 años preferiblemente en empresas industriales.\\r\\n- Deseable manejo de ingles a nivel técnico intermedio.\\r\\nConocimientos específicos\\r\\n- Conocimiento de protocolos industriales del sector eléctrico (IEC-60870-5-104, Modbus, DNP 3.0).\\r\\n- Protocolos Industriales de comunicación: Modbus TCP/IP, JSON, CSV, DNP3, SQL.\\r\\n- Conocimiento de sistemas SCADA.\\r\\n- Conocimientos de protocolos de transición con industria 4.0 (Modbus TCP, OPC UA, MQTT, HTTP, etc.).\\r\\nFundamentos de ciberseguridad.\\r\\nHorario\\r\\nLunes a viernes 08:00 a 18:00 / 08:30 a 18:30\\r\\nBeneficios\\r\\n-  Reajuste anual de sueldo de acuerdo a IPC\\r\\n-  Bonificación anual por desempeño laboral.\\r\\n-  Posibilidad de acceder a cursos de capacitación en las diversas temáticas del cargo.\\r\\n-  Convenios de Salud, Dentales y ópticos, accesos a descuentos preferenciales y facilidades de pagos mediante descuentos por planilla sin interés.\\r\\n-  Posibilidad de entrega de aguinaldo de fiestas patrias y Navidad conforme a cumplimiento de antigüedad.\\r\\n-  Convenio seguro Oncológico a valor preferencial y con aporte de la organización y del colaborador.\\r\\n-  Regalo de gift card por nacimiento de hijo (a).\\r\\n-  Convenios bancarios (scotiabank y Banco de Chile) para planes de tarjetas de cuentas vista y corriente a valores preferenciales.        \\r\\n            \\r\\n                            \\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       │ Not Applicable   │ Contract        │ Information Technology                              │ Information Technology & Services                                               │\n",
       "│                   ·                            │   ·                 │                       ·                       │     ·      │                       ·                        │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ·                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  │       ·          │    ·            │           ·                                         │               ·                                                                 │\n",
       "│                   ·                            │   ·                 │                       ·                       │     ·      │                       ·                        │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ·                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  │       ·          │    ·            │           ·                                         │               ·                                                                 │\n",
       "│                   ·                            │   ·                 │                       ·                       │     ·      │                       ·                        │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ·                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  │       ·          │    ·            │           ·                                         │               ·                                                                 │\n",
       "│ Data Engineer - Databricks - Mid Level         │ Lumenalta           │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-13 │ https://www.linkedin.com/jobs/view/4209420542/ │ Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\\nWhat we're working on:\\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\\nRequirements\\n- 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\\n- You are comfortable manipulating large data sets and handling raw SQL\\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\\n- Experience creating ETL Pipeline from scratch\\n- E-commerce and Financial Services industry experience preferred\\n- English fluency, verbal and written\\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\\nWhy Lumenalta is an amazing place to work at\\nAt Lumenalta, you can expect that you will:\\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\\n- Be a part of a team of talented and friendly senior-level developers.\\n- Work on projects that allow you to use leading tech.\\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\\nWhat's it like to work at Lumenalta?        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ Mid-Senior level │ Full-time       │ Engineering, Information Technology, and Consulting │ IT Services and IT Consulting                                                   │\n",
       "│ Sr. Data Engineer (Snowflake/dbt)              │ Sparq               │ Chile                                         │ 2025-04-15 │ https://www.linkedin.com/jobs/view/4210996594/ │ Team Sparq is committed to creating high-quality tech careers while helping clients accelerate their digital transformation journey. We are committed to being an inclusive workplace, maintaining a culture of equitable, diverse employment and advancement company-wide.\\nWhy You’ll Love This Role\\n- Work with cutting-edge cloud data technologies in a dynamic, collaborative environment\\n- Tackle enterprise-scale data challenges, working with billions of rows of data\\n- Opportunities for career growth and skill development through mentorship and certification programs\\n- Fully remote work flexibility\\nAbout The Role\\nWe are seeking a Senior Data Engineer with expertise in Snowflake and dbt, with a strong focus on scalability and optimization. The ideal candidate has experience working with massive datasets at the enterprise level and can fine-tune and optimize Snowflake environments to enhance performance, cost efficiency, and best practices.\\nResponsibilities\\n- Design and build scalable data pipelines in Snowflake and dbt, ensuring they can handle billions of rows of data efficiently\\n- Optimize Snowflake storage, compute performance, and query execution to improve processing speed and cost efficiency\\n- Lead efforts in migrating and refining legacy data processes in Snowflake using dbt, ensuring optimized transformations and modeling\\n- Collaborate with business and data teams to understand requirements and translate them into high-performance data solutions\\n- Implement best practices for Snowflake optimization, including clustering, partitioning, indexing, materialized views, and workload management\\n- Troubleshoot and resolve bottlenecks in existing Snowflake-based ETL/ELT workflows\\n- Provide technical leadership and mentorship, ensuring the team follows best practices for scalable data engineering\\n- Create and maintain technical documentation, including architecture diagrams and optimization guidelines\\nWhat You Bring\\n- 3+ years of experience in data engineering, with a focus on cloud-based enterprise-scale data solutions\\n- Proven experience working with massive datasets (billions of rows) in Snowflake\\n- Hands-on expertise in Snowflake performance tuning, storage optimization, and cost management\\n- Deep experience with dbt for data transformation, testing, and workflow orchestration\\n- Strong proficiency in SQL and Python for data manipulation, automation, and optimization\\n- Ability to identify, diagnose, and optimize inefficient queries and processing workflows\\n- Experience working both with and without an architect to optimize Snowflake performance\\n- Strong understanding of data governance, security best practices, and role-based access control in Snowflake\\n- Excellent problem-solving and communication skills, with the ability to collaborate across teams\\nBonus Points For\\n- Experience with orchestration tools like Airflow or Prefect\\n- Exposure to AWS, GCP, or Azure for cloud data integration\\n- Familiarity with streaming data pipelines (Kafka, Kinesis, etc.)\\nEqual Employment Opportunity Policy: Sparq is proud to offer equal employment opportunity without regard to age, color, disability, gender, gender identity, genetic information, marital status, military status, national origin, race, religion, sexual orientation, veteran status, or any other legally protected characteristic.\\nC2C is not available        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ Mid-Senior level │ Full-time       │ Information Technology                              │ Software Development                                                            │\n",
       "│ Senior Data Engineer                           │ Grupo Falabella     │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-14 │ https://www.linkedin.com/jobs/view/4209819635/ │ 🚀¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti! En Falabella Tecnología buscamos a nuestro/a próximo/a Senior Data Engineer, con base en Santiago, Chile.\\n📍Somos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\\nMisión del cargo: Construir y mantener las estructuras de datos y las arquitecturas tecnológicas necesarias para el procesamiento, ingestión e implementación\\n¿Qué harás en el día a día?\\n• Ser el contacto con el área de arquitectura para acordar requerimientos necesarios para las distintas iniciativas. Con el fin de actuar como puente entre distintas áreas corporativas, para definiciones contractuales.\\n• Analizar e implementar modelos de bases de datos (estructurados y no estructurados). Con el objetivo de asegurar la implementación de modelos adecuados según corresponda.\\n• Manejar las expectativas de los clientes internos y adaptarse a los cambios en el negocio, operaciones y tecnología. Para proveer habilidades blandas para el manejo político de las expectativas de los clientes vs lo real.\\n• Sugerir mejoras para evolucionar la arquitectura de las soluciones, en post de obtener procesos desacoplados, serverless y stateless. Con el objetivo de asegurar mejoras en Arquitectura con el conocimiento adquirido en post de entregar mejoras en los procesos.\\n• Mantener el nivel de servicio para los entregables del equipo y para las herramientas de datos desarrolladas por el mismo. Con el objetivo de asegurar fluidez en sus entregables sin perder la calidad y/o características de lo desarrollado.\\n¿Qué necesitas para postular?\\n• Profesional de carreras STEM\\n• SQL avanzado (excluyente)\\n• Nivel intermedio en Python (excluyente)\\n• Nivel de conocimiento intermedio en BigData (excluyente)\\n• Nivel de conocimiento intermedio en GIT (excluyente)\\n• Conocimiento en GCP (deseable)\\n• Conocimiento en Java (deseable)\\n• Disponibilidad para asistir a la oficina al menos 2 veces por semana en Santiago Centro (excluyente)\\nEn nuestro equipo encontrarás:\\n• Espacios para crear e innovar.\\n• Serás parte de un lugar lleno de oportunidades de desarrollo.\\n• Tener un trabajo con sentido y donde se promueve la calidad de vida.\\n• Participar en voluntariados.\\n• ¡Pertenecer a una empresa llena de energía!\\n✨Si disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\\n✨Somos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        │ Mid-Senior level │ Full-time       │ Engineering                                         │ Consumer Services                                                               │\n",
       "│ Data Engineer - Databricks - Senior            │ Lumenalta           │ Chile                                         │ 2025-04-13 │ https://www.linkedin.com/jobs/view/4209421507/ │ Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\\nWhat we're working on:\\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\\nRequirements\\n- 7+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\\n- You are comfortable manipulating large data sets and handling raw SQL\\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\\n- Experience creating ETL Pipeline from scratch\\n- E-commerce and Financial Services industry experience preferred\\n- English fluency, verbal and written\\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\\nWhy Lumenalta is an amazing place to work at\\nAt Lumenalta, you can expect that you will:\\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\\n- Be a part of a team of talented and friendly senior-level developers.\\n- Work on projects that allow you to use leading tech.\\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\\nWhat's it like to work at Lumenalta?        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ Mid-Senior level │ Full-time       │ Engineering, Information Technology, and Consulting │ IT Services and IT Consulting                                                   │\n",
       "│ Data Engineer - Snowflake - Mid Level          │ Lumenalta           │ Chile                                         │ 2025-04-14 │ https://www.linkedin.com/jobs/view/4209447525/ │ Experience Remote done Right. With over 20 years of remote experience, our 500+ team members are 100% remote, and we continue to cultivate vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\\nWhat we're working on:\\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions, and data points. The challenges we solve daily are real and require creativity, grit, and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of the problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media, or other complex multifactor industries.\\nRequirements\\n- 3+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\\n- You are comfortable manipulating large data sets and handling raw SQL\\n- Experience using technologies such as Snowflake, AWS, and ETL pipelines is essential.\\n- Have extensive experience with data warehousing and working with scalability of large volumes of structured data\\n- Financial Services industry experience preferred\\n- English fluency, verbal and written\\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\\nWhy Lumenalta is an amazing place to work at\\nAt Lumenalta, you can expect that you will:\\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\\n- Be a part of a team of talented and friendly senior-level developers.\\n- Work on projects that allow you to use leading tech.\\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\\nWhat's it like to work at Lumenalta?        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 │ Mid-Senior level │ Full-time       │ Engineering, Information Technology, and Consulting │ IT Services and IT Consulting                                                   │\n",
       "│ Ingeniero de datos                             │ Entelgy             │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-25 │ https://www.linkedin.com/jobs/view/4217481896/ │ INGENIERO DE DATOS\\nBuscamos profesionales que quieran reinventarse cada día, que enfrentarse a los retos vaya en su ADN y que sientan pasión por trabajar con la tecnología/ciberseguridad. Esto nos hace únicos, diferentes y auténticos.\\n¿QUÉ ESTAMOS BUSCANDO?\\nBuscamos a un Ingeniero de Datos de carreras relacionadas a las áreas de la \\ncomputación, informática, industrial, comercial, o control de gestión\\n; y con un mínimo de \\ncuatro (04) años de experiencia\\n profesional en labores de análisis e Ingeniería de datos en proyectos BI.\\n¿QUÉ NECESITAS PARA POSTULAR?\\nEstudios relacionados a las áreas de la \\ncomputación, informática, industrial, comercial, o control de gestión, de al menos ocho (8) semestres de duración como mínimo.\\nExperiencia demostrable de \\nal menos al menos 4 años\\n desempeñando roles de \\nanálisis e ingeniería de datos.\\nExperiencia de al menos cuatro (04) años trabajando con las siguientes herramientas:\\nExcluyentes:\\n- MS PowerBI\\n \\n- Lenguaje Python\\n \\n- Azure DataFactory\\n \\n- Azure Synapse\\n \\n- MS SQL Server\\n \\n- Sharepoint Online (listas, navegación y menús desplegables)\\n \\n- Suite MS ofimática (Excel,PowerPoint,Word,Teams entre otros)\\n \\nDeseables:\\n- Oracle\\n \\n- PostgreSQL\\n \\n- Power Designer\\n \\n- Azure Machine Learning\\n \\n- Azure Databricks\\n \\nAl menos una de las siguientes \\ncertificaciones (excluyentes):\\n- Microsoft Azure Fundamentals \\n- Microsoft Azure Data Fundamentals \\n¿CUÁLES SERÁN TUS PRINCIPALES FUNCIONES?\\nJunto con el Jefe de Proyectos, deberán realizar las siguientes actividades:\\n- Creación, mantención y mejoras de consultas (\\nquerys\\n) \\nSQL\\n que originan datos. \\n- Exportación de estos datos en formato \\nExcel\\n para análisis de usuarios de negocio. \\n- Conexión a bases de datos \\nAzure SQL Server\\n y \\nOnPremises\\n \\nPostgre\\n y \\nOracle\\n. \\n- Creación de \\npipelines\\n de datos para el procesamiento de datos extraídos desde distintos orígenes. \\n- Mantención de modelos dimensionales. \\n- Deployment\\n de pipelines en ambientes productivos. \\n- Creación, mantención y mejoras de paneles \\nPowerBI\\n. \\n- Administración de accesos de usuarios a estos paneles \\nPowerBI\\n, configuración de la actualización de modelos de datos, actualización de orígenes de datos a modelos, actualización de credenciales de bases de datos, usando el servicio de \\nPowerBI\\n. \\n- Actualización de pipelines para que cuenten con \\nCI\\n/\\nCD\\n. \\n- Planificación de tareas en MS Project. \\n- Realización de actas de entrega y minutas de reunión. \\n- Presentaciones a usuarios. \\n- Seguimiento de avance de productos comprometidos. \\n- Validaciones técnicas de productos desarrollados. \\nEl servicio se prestará en modalidad híbrida (presencial y remoto) en la comuna de Santiago.\\nDesde las 08:30 horas hasta las 17:30 horas de lunes a viernes.\\nPORQUE EN ENTELGY TODO COMIENZA CONTIGO\\nCuidarte en todos los aspectos es una máxima en Entelgy, por ello, la conciliación es algo que valoramos y facilitamos con nuestro modelo de trabajo flexible.\\nNuestra cultura se vive en el día a día, somos como una gran familia para la que hemos desarrollado un conjunto de iniciativas orientadas a potenciar que tengas una vida sana y equilibrada, como nuestra plataforma digital de Salud y Bienestar Health Training.\\nAdemás, contamos con Beneficios pensados en tu bienestar económico los que podrás conocer en nuestra intranet local.\\nASÍ ES COMO PONEMOS TODO NUESTRO TALENTO EN POTENCIAR EL TUYO\\nVen para formar parte de nuestro equipo, y descubre que no es sólo lo que hacemos, sino cómo lo hacemos. ¡Atrévete a vivir una experiencia Genuine!\\n- Proyectos retadores para grandes clientes: Trabajarás en proyectos de gran relevancia para nuestros clientes, junto a un gran equipo de profesionales desde el primer día.\\n- Partners de las mejores tecnologías: AWS, Splunk, Red Hat, Microsoft, Opentext y Oracle, Celonis y más.\\n- Entelgy College: nuestra universidad pone a tu disposición una amplia oferta formativa en tecnologías, habilidades, certificaciones, 12 idiomas y mucho más. Tú decides en que quieres formarte.\\n- Un progreso acelerado: Tu crecimiento estará garantizado y potenciado. Vamos contigo en lo que necesites.\\nTe esperamos!        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           │ mid-senior_level │ full-time       │ information_technology                              │ it_services_and_it_consulting                                                   │\n",
       "│ Ingeniero de Datos                             │ Cognitio Tecnología │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-24 │ https://www.linkedin.com/jobs/view/4216740751/ │ - MS PowerBI\\n- Lenguaje Python\\n- Azure DataFactory\\n- Azure Synapse\\n- MS SQL Server\\n- Sharepoint Online (listas, navegación y menús desplegables)\\n- Suite MS ofimática (Excel, Power Point ,Word ,Teams entre otros)\\nDeseable\\n- Oracle\\n- PostgreSQL\\n- Power Designer\\n- Azure Machine Learning\\n- Azure Databricks\\nExcluyente:\\n- Certificado (uno de los dos)\\n- Microsoft Azure Fundamentals\\n- Microsoft Azure Data Fundamentals\\nModalidad de trabajo:\\n- Híbrido (3x2)\\nFunciones: \\n- querys\\nSQL\\n- Excel\\n- Azure SQL Server\\nOnPremises\\nPostgre\\nOracle\\n- pipelines\\n- - Deployment\\n- PowerBI\\n- PowerBI\\nPowerBI\\n- CI\\nCD\\n- - - - -  ¡Aplica ahora!\\n¡Esperamos conocerte pronto!        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    │ mid-senior_level │ full-time       │ information_technology                              │ technology,_information_and_media                                               │\n",
       "│ Data Engineer Senior💡                         │ Apiux Tech          │ Chile                                         │ 2025-04-25 │ https://www.linkedin.com/jobs/view/4215109315/ │ ¿Quiénes somos?\\n 🙂\\nEn APIUX, somos una fábrica de talento con un enfoque estratégico, dedicada a impulsar el crecimiento de las nuevas generaciones a través de ideas innovadoras y modelos de negocio disruptivos. Nos comprometemos a brindar soluciones con una propuesta de valor que cubra siempre las necesidades de nuestros clientes, enfocándonos en ofrecer el mejor talento para generar resultados impactantes.\\n¿Qué hacemos?\\n 🚀\\nNos especializamos en posicionar a los mejores profesionales, entendiendo a fondo la cultura y los valores de cada cliente para lograr el match perfecto. Nuestra misión es garantizar una experiencia de candidato y Employee Experience ejemplar para todos nuestros futuros Happeners. 🤟\\nSer Happeners significa pertenecer a una familia que valora la agilidad, colaboración, innovación y pasión por hacer las cosas cada vez mejor. 💛\\nObjetivo del Cargo:🎯\\nDiseñar, desarrollar y mantener soluciones de ingeniería de datos robustas y escalables que respalden las operaciones y toma de decisiones en el banco. Además, liderar iniciativas estratégicas para la transformación digital y modernización de plataformas de datos.\\nFunciones del Cargo: 😎\\nDesarrollo de Datalake en \\nGCP\\n (Modelado y construcción).\\nDesarrollo de reportes para usuarios utilizando \\nPower BI\\n.\\nDesarrollo y mantenimiento de \\nDatawarehouse Microsoft\\n (SQLServer 2014/2019, SSIS, Analysis Services).\\nLo que esperamos de ti para hacer el match perfecto 💘\\nTransact SQL.\\nDesarrollo y mantenimiento de Datawarehouse Microsoft (SQLServer 2014/2019, SSIS, Analysis Services).\\nExperiencia en ETL/ELT y desarrollo de procesos (SSDT - Pipelines).\\nModelado y transformación de datos, entendiendo buenas prácticas para optimizar datos y evitar redundancias.\\nConocimiento de motores de bases de datos (Oracle - SQL Server) para el consumo de información y construcción de objetos.\\nExperiencia en desarrollo de Datalake en GCP.\\nCapacidad para desarrollar reportes y paneles de control en Power BI.\\nLo que te haría destacar aún más con nosotros 😍\\nMás de 5 años de experiencia en proyectos de datos.\\nExperiencia en inteligencia de negocios, aportando una visión estratégica basada en datos.\\nParticipación en transformación digital, integrando datos con enfoque en eficiencia y escalabilidad.\\nConocimiento del sector financiero, entendiendo sus requerimientos y regulaciones.\\nPerfil analítico, orientado a resultados y con enfoque en la toma de decisiones informadas.\\nCertificación como Scrum Product Owner, facilitando la gestión de proyectos ágiles.\\nModalidad de trabajo: \\nREMOTO\\nBeneficios: 🤙🏻💛\\nDía libre en tu cumpleaños.\\nDía libre por mudanza.\\nSeguro dental, médico y de vida.\\nBono por nacimiento, matrimonio y recomendación.\\nBono por Navidad y fiestas patrias.        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      │ not_applicable   │ other           │ information_technology                              │ it_services_and_it_consulting                                                   │\n",
       "│ Developer Data Engineer AWS                    │ Haibu Solutions     │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-25 │ https://www.linkedin.com/jobs/view/4215138142/ │ Company Description: Haibu Solutions\\nJob Description: Se necesita un Developer Data Engineer para importante cliente del rubro de Retail, para unirse a nuestro equipo dinámico y en crecimiento. Este profesional trabajará estrechamente con equipos multidisciplinarios para entregar soluciones de alta calidad que cumplan con los requisitos del cliente.\\nLos Skills Son Los Siguientes\\n-  Se requiere ingeniero de datos con conocimientos avanzados de Aws Glue, Airflow y Python.\\n-  Conocimientos avanzados de SQL, PySpark y Scala.\\n-  Experiencia en herramientas ETL y de orquestación como Apache NiFi, Databricks (FOCO PRINCIPAL), Dbt, Apache Airflow, AWS Glue/Lambda/S3/EMR/MWAA.\\nSAP Hana enfocado en dominio avanzado de su lenguaje SQL que le permita construir consultas complejas,\\n-  procedimientos almacenados,\\n-  vistas\\n-  triggers (Deseable pero no excluyente)\\n-  Manejo Lenguaje Python, específicamente a nivel de librerías de manejo de datos (Pyspark, Pandas, etc).\\nDominio de la teoría de arquitecturas de datos (data warehouse, data lake, lakehouse y data mesh) y bases de datos relaciónales y nosql.\\nCapacidad para modelado de datos y diagnóstico de fallas en pipelines de datos.\\nSkills Deseables\\n-  Tecnologías de Big Data: ecosistema Hadoop: la experiencia en Apache Hadoop, incluidos HDFS, MapReduce y YARN, es esencial para procesar y gestionar conjuntos de datos masivos.\\n-  Spark: un potente marco informático distribuido para procesamiento y análisis de datos a gran escala. Kafka: una plataforma de transmisión distribuida para la ingesta y el procesamiento de datos en tiempo real.\\nRégimen Laboral: De lunes a viernes en horario de oficina, modo hibrido 2x3 , con disponibilidad para ir a las oficina en Santiago.\\nPara Postular Correctamente Al Proceso De Reclutamiento Se Necesita\\n-  Indicar disponibilidad de trabajo en días.\\n-  Indicar si se encuentra trabajando.        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 │ entry_level      │ full-time       │ information_technology                              │ information_technology_&_services                                               │\n",
       "│ Lead, Data Engineer - AI, Insights & Solutions │ Tech Economy        │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-25 │ https://www.linkedin.com/jobs/view/4216909434/ │ Applications must be submitted in English\\nWhat Makes Us a Great Place To Work\\nWe are proud to be consistently recognized as one of the world’s best places to work, a champion of diversity and a model of social responsibility. We are currently the #1 ranked consulting firm on Glassdoor’s Best Places to Work list and have maintained a spot in the top four for the last 13 years.\\nWe believe diversity, inclusion, and collaboration are key to building extraordinary teams. We hire people with exceptional talents and potential, and create an environment where you can thrive professionally and personally. We’re recognized by Fortune, Vault, Working Mother, the Human Rights Campaign, and more for being a great place to work for diversity, women, LGBTQ+, and parents.\\nWho You’ll Work With\\nAs a member of Bain’s \\nAI, Insights & Solutions (AIS)\\n team, you’ll join a talented, cross-functional group of Data Engineers, Data Scientists, Machine Learning Engineers, and Consultants. Together, we design and implement robust, scalable solutions that address our clients' most pressing data challenges across diverse industries. Our collaborative and supportive environment enables creativity and continuous learning, helping us consistently deliver exceptional results.\\nWHERE YOU’LL FIT WITHIN THE TEAM\\nAs a \\nLead Data Engineer\\n, you’ll play a central role in developing and deploying scalable data solutions that fuel insight-driven transformation for our clients. You’ll lead high-impact projects that span cloud platforms, data governance, and modern data architectures.\\nWhat You’ll Do\\n- Design and implement scalable data engineering solutions and infrastructure\\n- Lead data governance projects and develop robust frameworks for client adoption\\n- Build end-to-end data architectures on cloud platforms (AWS, Azure, GCP)\\n- Engage in full engineering lifecycle: from design to deployment\\n- Drive containerized, serverless data pipelines using Terraform and orchestration frameworks\\n- Optimize performance and schema design in data lakes and warehouses\\n- Collaborate across regions and disciplines to deliver high-impact outcomes\\nAbout You\\nTechnical Skills and Knowledge:\\n- 5+ years of data engineering experience (3+ years at senior or staff level)\\n- Expertise in Python\\n- Strong skills in modern ETL tools (e.g., Airflow, Beam, Spark)\\n- Hands-on with cloud platforms, Kubernetes, and Terraform\\n- Proficiency in SQL/NoSQL (e.g., PostgreSQL, MongoDB, Snowflake)\\n- Background in DevOps, CI/CD, and Git workflows\\n- Strong computer science fundamentals (algorithms, data structures, testing)\\n- Agile development experience\\n- Experience with data governance practices, including data cataloging, lineage tracking, metadata management, data quality frameworks\\nand knowledge of Master Data Management (MDM) principles (desired)\\nInterpersonal Skills\\n- Professional proficiency in English\\n- Strong communication and cross-functional collaboration skills\\n- Critical thinking, curiosity, and a proactive mindset\\nEducation\\n- Bachelor’s or Master’s degree in Computer Science, Engineering, or related field        \\n            \\n                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         │ mid-senior_level │ full-time       │ information_technology                              │ business_consulting_and_services                                                │\n",
       "├────────────────────────────────────────────────┴─────────────────────┴───────────────────────────────────────────────┴────────────┴────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────┴─────────────────┴─────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ 65 rows (20 shown)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         10 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM test_table\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_db = con.execute(\"\"\"SELECT * \n",
    "                         FROM test_table t1\n",
    "\n",
    "                         WHERE job_url NOT IN (SELECT job_url FROM genai_table)\n",
    "\n",
    "                         \"\"\").fetchdf()\n",
    "df_from_db = pl.DataFrame(df_from_db)\n",
    "print(df_from_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_list: list[dict] = []\n",
    "for row in df_from_db.rows(named=True):\n",
    "    \n",
    "    genai_data: dict = {}\n",
    "    genai_data.update(classify_job_description(row['job_description']))\n",
    "    genai_data.update({'job_url': row['job_url']})\n",
    "    genai_list.append(genai_data)\n",
    "    \n",
    "print(genai_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573a35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genai = pl.DataFrame(genai_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2818be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing UPSERT for table 'genai_table' on key 'job_url' ---\n",
      "Registered DataFrame as view 'df_upsert_view'\n",
      "Executed: CREATE TABLE IF NOT EXISTS genai_table(\n",
      "                                    job_url VARCHAR PRIMARY KEY,\n",
      "                                    task_clarity VARCHAR,\n",
      "                                    seniority_level_ai VARCHAR,\n",
      "                                    requires_degree_it VARCHAR,\n",
      "                                    mentions_certifications VARCHAR,\n",
      "                                    years_of_experience VARCHAR,\n",
      "                                    is_in_english VARCHAR,\n",
      "                                    cloud_preference VARCHAR,\n",
      "                                    skills_mentioned VARCHAR[]\n",
      "                                    );\n",
      "                                     (if table didn't exist)\n",
      "\n",
      "Executing UPSERT SQL:\n",
      "\n",
      "        INSERT INTO genai_table (\"task_clarity\", \"seniority_level_ai\", \"requires_degree_it\", \"mentions_certifications\", \"years_of_experience\", \"is_in_english\", \"cloud_preference\", \"skills_mentioned\", \"job_url\")\n",
      "        SELECT \"task_clarity\", \"seniority_level_ai\", \"requires_degree_it\", \"mentions_certifications\", \"years_of_experience\", \"is_in_english\", \"cloud_preference\", \"skills_mentioned\", \"job_url\" FROM df_upsert_view\n",
      "        ON CONFLICT (job_url) DO UPDATE SET\n",
      "            \"task_clarity\" = excluded.\"task_clarity\", \"seniority_level_ai\" = excluded.\"seniority_level_ai\", \"requires_degree_it\" = excluded.\"requires_degree_it\", \"mentions_certifications\" = excluded.\"mentions_certifications\", \"years_of_experience\" = excluded.\"years_of_experience\", \"is_in_english\" = excluded.\"is_in_english\", \"cloud_preference\" = excluded.\"cloud_preference\", \"skills_mentioned\" = excluded.\"skills_mentioned\";\n",
      "        \n",
      "UPSERT execution successful.\n",
      "Unregistered view 'df_upsert_view'\n",
      "\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "insert_into_db(df_genai, \"genai_table\", \"my_project.duckdb\", \"genai_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "596f17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "746c63ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────┬──────────────┬───────────────────────────────────────────────┬────────────┬────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────┬─────────────────┬───────────────────────────────────────────────────┬─────────────────────────┬────────────────────────────────────────────────┬──────────────┬────────────────────┬────────────────────┬─────────────────────────┬─────────────────────┬───────────────┬──────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│        title         │   company    │                   location                    │    date    │                    job_url                     │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               job_description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               │ seniority_level │ employment_type │                   job_function                    │       industries        │                    job_url                     │ task_clarity │ seniority_level_ai │ requires_degree_it │ mentions_certifications │ years_of_experience │ is_in_english │ cloud_preference │                                                                       skills_mentioned                                                                        │\n",
       "│       varchar        │   varchar    │                    varchar                    │    date    │                    varchar                     │                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   varchar                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   │     varchar     │     varchar     │                      varchar                      │         varchar         │                    varchar                     │   varchar    │      varchar       │      varchar       │         varchar         │       varchar       │    varchar    │     varchar      │                                                                           varchar[]                                                                           │\n",
       "├──────────────────────┼──────────────┼───────────────────────────────────────────────┼────────────┼────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┼─────────────────┼───────────────────────────────────────────────────┼─────────────────────────┼────────────────────────────────────────────────┼──────────────┼────────────────────┼────────────────────┼─────────────────────────┼─────────────────────┼───────────────┼──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ Senior Data Engineer │ Ottomatik.io │ Santiago, Santiago Metropolitan Region, Chile │ 2025-04-12 │ https://www.linkedin.com/jobs/view/4207190424/ │ Hi there! We are South and our client is looking for a \\nSenior Data Engineer\\n!\\nNote To Applicants\\n- Eligibility: This position is open to candidates residing in Latin America. \\n- Application Language: Please submit your CV in English. Applications submitted in other languages will not be considered. \\n- Professional Presentation: We encourage you to showcase your professional experience by including a Loom video in the application form. While this is optional, candidates who provide a video presentation will be given priority. \\nDuties & Responsibilities\\n- Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization, and advanced business intelligence\\n- Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)\\n- Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects\\n- Ensure proper testing and monitoring are in place to provide trusted and reliable data from the data stack.\\n- Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction\\n- Identify and document standard methodologies, standards, and architecture guidelines\\n- Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery\\n- Participating in a shared on-call rotation monitoring the health of the team’s systems\\nEssential Knowledge, Skills, And Abilities\\n- 2+ years of data architecture related experience such as data analysis, data modeling, and data integration.\\n- 4+ years of Experience in custom ETL design, implementation, and maintenance\\n- Advanced Knowledge of programming languages (Python and/or R)\\n- Hands-on experience with SQL database design\\n- Experience working on AWS\\n- Experience working with relational databases (Snowflake, Redshift)\\n- Bachelor’s or Master’s Degree in Computer Science, Information Systems, or related field\\nBonus Points\\n- Experience working with reporting tools such as Tableau\\n- Experience with event streaming with modern event streaming toolings like Kafka or Kinesis.\\n- Experience with subscription service products\\n- Knowledge of accounting, FP&A, and marketing functions\\nSchedule\\n: Monday to Friday, between 7:30 or 8:00 AM to 4:00 or 4:30 PM - \\nPST\\nCompensation\\n: Paid in USD\\nLocation\\n: 100% remote opportunity\\nIf this opportunity sounds good to you, \\nsend us your resume!        \\n            \\n                            \\n             │ Executive       │ Full-time       │ Business Development, Consulting, and Engineering │ IT System Data Services │ https://www.linkedin.com/jobs/view/4207190424/ │ Medium       │ Senior             │ Yes                │ No                      │ 2+                  │ Yes           │ AWS              │ [Develop pipelines or ETL/ELT processes, Data modeling, Data analysis or visualization, Data monitoring, Collaboration with data scientists or analysts, AWS] │\n",
       "└──────────────────────┴──────────────┴───────────────────────────────────────────────┴────────────┴────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────┴─────────────────┴───────────────────────────────────────────────────┴─────────────────────────┴────────────────────────────────────────────────┴──────────────┴────────────────────┴────────────────────┴─────────────────────────┴─────────────────────┴───────────────┴──────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"\n",
    "        SELECT t1.*, t2.*\n",
    "        FROM test_table t1\n",
    "\n",
    "        JOIN genai_table t2\n",
    "        ON t1.job_url = t2.job_url\n",
    "\n",
    "        WHERE t1.job_url = 'https://www.linkedin.com/jobs/view/4207190424/'\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259df3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"my_project.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22027c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "job_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seniority_level",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "job_function",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "industries",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "job_url_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_clarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seniority_level_ai",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "requires_degree_it",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mentions_certifications",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "years_of_experience",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_in_english",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cloud_preference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "skills_mentioned",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "31eab4f8-1765-44f7-9223-217f01478f10",
       "rows": [
        [
         "0",
         "Analista Data Engineer AzureDataBricks",
         "Accenture Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-13 00:00:00",
         "https://www.linkedin.com/jobs/view/4212693987/",
         "WORK AT THE HEART OF THE CHANGE\nEmprende una Carrera que te inspire, súmate a nuestra área de \nAccenture Technology \ny sé parte del cambio!\n“En Accenture vivimos el futuro hoy, porque no solo aplicamos tecnología, la creamos. Aquí construimos el futuro”. \nCombinemos tu ingenio con las últimas tecnologías y sé parte del impacto positivo que puedes generar.\nTe invitamos a abrir las puertas del mundo digital donde podrás desarrollar tu talento para crecer, aprender y certificarte en las tecnologías más avanzadas, proporcionar innovación continua, ágil y participar de nuevos negocios.\nCada día, en todo el mundo, nuestros equipos innovan para crear un cambio significativo. Sé parte de Accenture!\nNos encontramos en búsqueda de profesionales para el role de \nData Engineer para Soporte.\nSé parte de nuestro equipo dinámico mientras nos embarcamos en un viaje para convertirnos en expertos en Ingeniería de Datos. Ayudarás a darle forma al futuro colaborando, gestionando e interactuando con varios equipos para proporcionar soluciones innovadoras y contribuir a las decisiones clave.\n ¡Vamos a crear juntos un mundo basado en datos!\n¿Qué te hará tener éxito? 🚀\nExperiencia con Azure DataBricks, al menos 1 ó 2 años.\nExperiencia con Python en Azure.\nMínimo 1 año de experiencia en roles similares utilizando estas tecnologías.\nBONUS POINTS IF YOU HAVE ⭐\nExperiencia con Power BI y SQL Server.\n¿Por qué elegir Accenture? \nUn lugar de trabajo único, descubre algunos de los beneficios que tenemos para ti\n:\n💪 Desarrollo de carrera\n💯 Jornadas Flex\n📚 +40 mil capacitaciones y cursos disponibles (online y presencial)\n📒Bibliotecas, libros y podcasts\n🗣️Programa de idiomas\n⭐ ¡Certificaciones gratuitas mediante nuestros partners! +900 certificados en Chile\n🤖GenAI Academy, con programas exclusivos para Accenture.\n⭐⭐Reconocidos por Great Place To Work Chile 2023 en el puesto #10 entre las mejores compañías para trabajar de más de 1.000 colaboradores. ¡Sí, estamos en el Top 10 de Chile!\n🎉 ¡Experiencia de onboarding global! +6.000 personas recibidas en el Metaverso a nivel LATAM\n🏆 Bonos y aguinaldos\n👩‍⚕️ Seguro complementario de salud (sin deducible ni copago)\n🎂 Día de cumpleaños libre\n👨‍👩Licencias de Paternidad & Maternidad Extendida\n🌎 Red global de conocimiento\n🌈Elegida la compañía más diversa e inclusiva del mundo, según el Índice de Diversidad e Inclusión de Refinitiv\n♻️Sostenibilidad, un motor de cambio, conoce nuestro compromiso\nSobre nosotros: \n 733K colaboradores a nivel global.\n 9K clientes en 120 países\n + de 1.900 Talentos en Chile.\n Proyectos en diversas industrias        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4212693987/",
         "Medium",
         "Junior",
         "No",
         "Yes",
         "1",
         "No",
         "Azure",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data analysis or visualization'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "1",
         "Chief Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-04-17 00:00:00",
         "https://www.linkedin.com/jobs/view/4212167524/",
         "We are seeking an experienced Chief Data Engineer to join our remote team and take charge of delivering innovative and scalable data solutions. In this role, you will lead technical teams, drive system architecture design, and contribute to impactful projects that solve complex data challenges. The ideal candidate will have a strong technical background, leadership skills, and a passion for mentoring and innovation.\nResponsibilities\n- Lead the design and implementation of scalable and efficient data architectures\n- Develop and maintain data pipelines and workflows to ensure data quality and accessibility\n- Collaborate with stakeholders to gather requirements and deliver data-driven solutions\n- Provide technical guidance and mentorship to engineering teams\n- Optimize system performance, scalability, and reliability\n- Drive technical best practices and ensure high-quality code delivery\n- Conduct code reviews and contribute to technical discussions\n- Support pre-sales activities, client engagements, and SWAT team initiatives\n- Promote knowledge sharing and participate in meetups and technical talks\n- Stay informed on emerging technologies and integrate innovative solutions\nRequirements\n- A degree in Engineering, Computer Science, or a related field\n- At least 7 years of experience as a Data Engineer\n- Minimum of 2 years of leadership experience, including team management and project direction\n- Experience participating in at least 2 full-cycle projects or multiple phases of development lifecycles\n- Proven hands-on expertise in coding and solving complex technical problems\n- Proficiency in Python for data processing, analysis, and automation\n- Deep knowledge of Amazon Web Services (AWS) for cloud-based data solutions\n- Experience with Databricks for big data analytics and machine learning workflows\n- Strong skills in system design and architecture, with the ability to balance high-level vision and detailed implementation\n- A track record of delivering significant technical impact within projects and across organizations\n- Broad cross-disciplinary knowledge of multiple technical domains and stacks\n- A mindset for mentoring and sharing expertise within teams and the broader community\n- Demonstrated ability to innovate by implementing new tools, frameworks, or methodologies\n- Adaptability to switch between programming languages, technical stacks, and domains as needed\n- Fluent communication skills in English at a B2 level or higher\nNice to have\n- Experience with real-time data processing and streaming technologies\n- Knowledge of advanced machine learning and AI frameworks\n- Familiarity with serverless computing solutions for data workflows\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Engineering, Information Technology, and Business Development",
         "Software Development, IT Services and IT Consulting, and Technology, Information and Internet",
         "https://www.linkedin.com/jobs/view/4212167524/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "7+",
         "Yes",
         "AWS",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data quality' 'Knowledge of Machine Learning or MLOps' 'CI/CD'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Version control (GIT or similar)' 'APIs'\n 'Spark knowledge']"
        ],
        [
         "2",
         "Chief Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4220611480/",
         "We are seeking an experienced \nChief Data Engineer\n to join our remote team and lead the delivery of innovative and scalable data solutions.\nThis role involves managing technical teams, designing system architecture, and contributing to meaningful projects that solve complex data challenges. The ideal candidate brings a strong technical background, leadership qualities, and a dedication to mentoring and innovation.\nResponsibilities\n- Take the lead in designing and implementing scalable and efficient data architectures\n- Build and maintain data pipelines and workflows to ensure data quality and accessibility\n- Collaborate with stakeholders to define requirements and deliver data-driven solutions\n- Mentor engineering teams and offer technical guidance\n- Improve system performance, scalability, and reliability\n- Enforce technical best practices and ensure the delivery of high-quality code\n- Provide feedback through code reviews and participate in technical discussions\n- Support client engagements, pre-sales activities, and SWAT team efforts\n- Foster knowledge sharing within the team and participate in meetups and technical talks\n- Keep up with emerging technologies to incorporate innovative solutions\nRequirements\n- A degree in Engineering, Computer Science, or a related field\n- At least 7 years of experience in a Data Engineering role\n- Minimum of 2 years' experience in leadership roles, including team and project management\n- Participation in at least 2 full-cycle projects or multiple phases of development life cycles\n- Expertise in coding and solving complex technical challenges\n- Proficiency in Python for data processing, analysis, and automation\n- Deep knowledge of Amazon Web Services (AWS) for cloud-based data solutions\n- Experience with Databricks for big data analytics and machine learning workflows\n- Strong skills in system design and architecture with the ability to align vision and implementation\n- Achievements delivering significant impact on projects and organizational goals\n- Broad cross-disciplinary knowledge covering various technical domains and stacks\n- Commitment to mentoring and sharing expertise within teams and beyond\n- Capacity to drive innovation by adopting new tools, frameworks, or methodologies\n- Flexibility to work with multiple programming languages, stacks, and domains as required\n- Fluent communication in English at a B2 level or higher\nNice to have\n- Background in real-time data processing and streaming technologies\n- Knowledge of advanced machine learning and AI frameworks\n- Familiarity with serverless computing solutions for data workflows\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4220611480/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "7+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Knowledge of Machine Learning or MLOps' 'CI/CD'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Spark knowledge']"
        ],
        [
         "3",
         "Consultor Data Engineer",
         "MAS Analytics",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4223641021/",
         "MAS Analytics\n es una consultora de datos e inteligencia artificail que nos dedicamos a desarrollar proyectos de Data Science (DS), Inteligencia Artificial, Cloud Architecture y Business Intelligence (BI). Hoy en día tenemos muchos proyectos con clientes de muchos rubros distintos y necesitamos ingeniero/as apasionados que estén buscando aprender y desafiarse trabajando en equipo.\nSi estás buscando profundizar y trabajar aplicando tus conocimientos día a día, dentro de un equipo de mucho talento, y alto rendimiento, este puesto es lo que estás buscando.\n Funciones del cargo\nEstamos buscando ingenier@s que sepan trabajar con datos, mover datos, interpretar datos y buscar datos. El cargo está orientado hacia la ingeniería de datos utilizando herramienta de ETL, Bases de datos y herramientas de reportería.\nSi no manejas alguno de estos conceptos, no te preocupes, nos importa que tengas las bases para poder aprender y desafiarte día a día.\nTendrás Que Realizar Las Siguientes Tareas (entre Otras)\n- Reuniones de negocio con clientes\n- Creación de queries SQL\n- Creación de pipelines ETL (on premise y cloud)\n- Creación de visualizaciones en herramientas de BI\nEl éxito de tu trabajo se evaluará en el nivel de satisfacción de los modelos entregados a los clientes y en el cumplimiento de plazos.\nTrabajarás dentro de los equipos de desarrollo de MAS Analytics teniendo reuniones de avance semanal con los clientes y otros miembros del equipo de MAS Analytics.\n Requerimientos del cargo\nAl ser consultoría en tecnología, necesitamos que tengas habilidades técnicas y habilidades blandas. Se privilegiarán perfiles de ingeniería civil TI o Computación.\nEn cuanto a las habilidades técnicas, se espera que puedas trabajar con las siguientes herramientas:\n- Queries SQL (CRUD)\n- Procesamiento: Dataflow (GCP, Azure), Lambda, Glue (AWS)\n- Visualizaciones: Qlik, Tableau, Power BI\n- Modelamiento de datos: modelos dimensionales OLAP\nCon respecto a las habilidades blandas, necesitamos que estés preparada/o para:\n- Llevar comunicación con los clientes\n- Levantar requerimientos\n- Trabajo en equipo\n- Proactividad\n- Pensamiento analítico\n Opcionales\nConocimientos en herramientas Cloud (GCP, AWS, Azure)\n Algunos beneficios\n- Horario flexible\n- Financiamiento de cursos y certificaciones 👨‍🎓👩‍🎓\n- Ambiente laboral joven\n- Viernes medio día\n- Vacaciones extra 🌞🏝\n- Celebraciones de cumpleaños 🎁🎊\n- Actividades de empresa 🍻⚽\n- Y muchos más que podrás conocer…        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4223641021/",
         "Medium",
         "Junior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization']"
        ],
        [
         "4",
         "Data Engineer",
         "2Brains",
         "Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205294882/",
         "2Brains es una empresa dedicada a construir y desarrollar el Futuro Digital de nuestros clientes, con una visión excepcional que radica en la integración sinérgica de estrategia, diseño y tecnología, un tríptico poderoso que impulsa el crecimiento de empresas y disruptores tecnológicos.\r\nContamos con un nutrido equipo de más de 200 profesionales, verdaderos artífices de la innovación digital. En el corazón de nuestra labor, destacamos como líderes indiscutibles, canalizando años de experiencia hacia la creación de plataformas tecnológicas adaptables y productos digitales de clase mundial.\r\nEn 2Brains, no solo somos consultores, somos arquitectos de experiencias digitales. Aspiramos a ir más allá de las expectativas, estableciendo nuevos estándares en la industria. Descubre cómo damos vida a la innovación, cómo convertimos ideas en resultados tangibles y cómo, junto a nosotros, puedes forjar un futuro digital brillante.\r\n El/la Data Engineer de 2Brains \r\nSe encarga de participar en el diseño y desarrollo de los nuevos modelos de información de gestión y las mantenciones evolutivas de los existentes. Participar en las iniciativas de Analítica avanzada del área, apoyando las exploración de modelos de información internos y externos (Data Discovery). Obtener datos históricos desde múltiples fuentes de información interna para apoyar las iniciativas de analítica avanzada del equipo.\r\nEl/la Data Engineer de 2Brains debe\r\n- Construir y optimizar pipelines de datos para la ingesta, transformación y carga eficiente de información.\r\n- Manejar infraestructuras en la nube (AWS, GCP, Azure), asegurando escalabilidad y eficiencia en costos.\r\n- Automatizar y monitorear procesos mediante herramientas de DevOps como Airflow, Terraform o Kubernetes.\r\n- Implementar controles de calidad y gobernanza para garantizar la integridad y disponibilidad de los datos.\r\n- Colaborar con equipos de Data Science, Producto y Desarrollo para diseñar soluciones alineadas con las necesidades del negocio.\r\n Qué conocimientos buscamos en/la Data Engineer\r\n- Excluyente Experiencia trabajando con tecnologías de BI\r\n- Experiencia en la construcción/operación de sistemas distribuidos de extracción, ingestión y procesamiento de grandes conjuntos de datos de gran disponibilidad.\r\n- Capacidad demostrable en modelado de datos, desarrollo de ETL y almacenamiento de datos.\r\n- Experiencia en el uso de herramientas de informes de inteligencia empresarial (Power BI)\r\n- Excluyente conocimiento en consumo de microservicios de APIs Rest\r\n- Excluyente conocimiento en Git , Bitbucket, Docker,Jenkins,Webhooks\r\n- Programación con Python y bases sólidas de ingeniería de software.\r\n- Automatización y scripting.\r\n- Uso de librerías de Python para manipulación y análisis de datos y Apache Spark.\r\n- Conocimientos en bases de datos SQL y NoSQL.\r\n- Conocimiento en CI/CD, Dataflow\r\n- Conocimiento en S3, Redshift y Glue AWS\r\n Que competencias buscamos en/la Data Engineer \r\n- Empatía\r\n- Buena capacidad de comunicación.\r\n- Colaboración y trabajo en equipo.\r\n- Proactividad.\r\n- Autonomía.\r\n- Foco en los objetivos de proyectos.\r\n Condiciones\r\nTrabajar con un equipo de alto rendimiento, aprendemos y nos desarrollamos juntos\r\nAcceso a grandes clientes y proyectos desafiantes\r\nAprendizaje y crecimiento permanente, organizamos meetups, capacitaciones y actividades culturales\r\nUn entorno de trabajo flexible y dinámico\r\nBeneficios especiales: día libre para tu cumpleaños, días de descanso a convenir.\r\n                \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "Technology, Information and Internet and Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4205294882/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Data quality' 'Data governance' 'CI/CD'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)' 'APIs'\n 'Spark knowledge' 'Version control (GIT or similar)']"
        ],
        [
         "5",
         "Data Engineer",
         "2Brains",
         "Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4216874808/",
         "2Brains es una empresa dedicada a construir y desarrollar el Futuro Digital de nuestros clientes, con una visión excepcional que radica en la integración sinérgica de estrategia, diseño y tecnología, un tríptico poderoso que impulsa el crecimiento de empresas y disruptores tecnológicos.\nContamos con un nutrido equipo de más de 200 profesionales, verdaderos artífices de la innovación digital. En el corazón de nuestra labor, destacamos como líderes indiscutibles, canalizando años de experiencia hacia la creación de plataformas tecnológicas adaptables y productos digitales de clase mundial.\nEn 2Brains, no solo somos consultores, somos arquitectos de experiencias digitales. Aspiramos a ir más allá de las expectativas, estableciendo nuevos estándares en la industria. Descubre cómo damos vida a la innovación, cómo convertimos ideas en resultados tangibles y cómo, junto a nosotros, puedes forjar un futuro digital brillante.\n El/la Data Engineer de 2Brains debe\nParticipar en el diseño y desarrollo de los nuevos modelos de información de gestión y las mantenciones evolutivas de los existentes. Participar en las iniciativas de Analítica avanzada del área, apoyando las exploración de modelos de información internos y externos (Data Discovery). Obtener datos históricos desde múltiples fuentes de información interna para apoyar las iniciativas de analítica avanzada del equipo.\n Qué conocimientos buscamos en/la Data Engineer\n- Desarrollo de pipelines de datos para extracción, transformación y carga (ETL/ELT).\n- Dominio de herramientas de orquestación como Apache Airflow o Luigi.\n- Automatización y programación de flujos de datos complejos.\n- Gestión de bases de datos relacionales (SQL).\n- Manejo de bases de datos NoSQL (MongoDB, Cassandra, etc.).\n- Experiencia en Cloud Databases (BigQuery, Redshift, Snowflake, etc.).\n- Diseño y construcción de data warehouses, data lakes y lakehouses.\n- Modelado de datos para entornos analíticos y operacionales.\n- Uso de herramientas y tecnologías para procesamiento distribuido (ej. Spark, Hadoop, Kafka).\n- Experiencia en servicios de datos en la nube: Microsoft Azure, Google Cloud Platform, AWS.\n- Conocimientos en integración de flujos de datos con modelos de machine learning (deseable).\n- Manejo de prácticas DevOps para automatización, CI/CD y monitoreo en pipelines de datos.\n- +4 años de experiencia en roles de Ingeniería de Datos.\n- Certificaciones como Professional Data Engineer en Microsoft Azure o Google Cloud Platform.\n- Deseable experiencia desarrollando soluciones de análisis de datos o BI (Business Intelligence).\n Que competencias buscamos en/la Data Engineer \n- Empatía\n- Buena capacidad de comunicación.\n- Colaboración y trabajo en equipo.\n- Proactividad.\n- Autonomía.\n- Foco en los objetivos de proyectos.\n Te ofrecemos\n- Trabajar con un equipo de alto rendimiento, aprendemos y nos desarrollamos juntos\n- Acceso a grandes clientes y proyectos desafiantes\n- Aprendizaje y crecimiento permanente, organizamos meetups, capacitaciones y actividades culturales\n- Un entorno de trabajo flexible y dinámico\n- Día libre para tu cumpleaños.\n- 4 semanas de vacaciones al año.\n- Cursos de especialización.\n- Espacio para charlas internas.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4216874808/",
         "Medium",
         "Senior",
         "No",
         "Yes",
         "4",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Spark knowledge' 'CI/CD' 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "6",
         "Data Engineer",
         "ARKHO",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4223640231/",
         "ARKHO es una consultora experta en tecnologías de la información, que ofrece servicios expertos de TI en el marco de modernización de aplicaciones, analítica de datos y migración a la nube. Nuestro trabajo facilita y acelera la adopción de la cloud en múltiples industrias.\nNos destacamos por ser Partner Advanced de Amazon Web Services con foco estratégico en la generación de soluciones usando tecnología en la nube, somos obsesionados por lograr los objetivos propuestos y tenemos especial énfasis en el grupo humano que compone ARKHO (nuestros Archers), reconociendo a las personas y equipo como un componente vital para el logro de los resultados.\n¿Te motivas? ¡Te esperamos!\n Funciones\nEstamos en busca de un o una profesional con experiencia en el desarrollo de pipelines de analytics, con una visión en la arquitectura de datos End to End, que logre aplicar sus conocimientos técnicos en la ejecución de proyectos tecnológicos. La vacante considera que el o la aspirante sea un referente orientando a la mejora continua y el cumplimiento de los objetivos y la creación de productos tecnológicos de alta calidad. Para ello, la experiencia en el uso de diversas herramientas, lenguajes de programación, infraestructura y otros, son la base para apoyar desde el conocimiento al equipo de desarrollo, participando en el desarrollo de entregables de alto impacto y calidad.\n- Trabajar de forma integrada y colaborativa con el líder de equipo, y el equipo de implementación proporcionando un contexto claro del estado de las iniciativas desde el ámbito tecnológico.\n- Apoyar al equipo en la definición, estimación y planificación de tareas/actividades para el desarrollo de productos de analítica en el marco ágil del desarrollo.\n- Gobernar técnicamente la solución con el cliente, participando activamente de decisiones claves y proporcionando información relevante en el aspecto técnico.\n- Trabajar con los integrantes del equipo en el diseño e implementación de arquitecturas de soluciones de analítica de datos en la nube y para la solución completa.\n- Involucramiento en el modelado de los datos, generando modelos analíticos corporativos.\n- Profundización en aspectos tecnológicos desconocidos y que se requieren para el logro de los objetivos.\n Perfil del archer\nEn Nuestra Compañía Valoramos a Las Personas Auto-gestionadas, Proactivas e Innovadoras. Debes Ser Capaz De Organizar Tu Propia Carga De Trabajo, Tomar Decisiones, Cumpliendo Tus Plazos y Objetivos. Buscamos Personas Con\n- Experiencia relevante en Python con datos.\n- Dominio de SQL y bases de datos no relacionales.\n- Conocimiento en tecnologías de ETL.\n- Capacidades de documentación de negocio y tecnológica.\n- Desarrollo de APIs.\n- Experiencia en AWS cloud.\n- Experiencia en proyectos BI y modelando bases de datos para Data Warehouse (Estrella, Copo de nieve).\n- Habilidades de priorización de requerimientos y resultados de negocio.\n Habilidades opcionales\nSi posees experiencia en algunas de las tecnologías a continuación es un plus:\n- Conocimiento de Machine Learning\n- Nivel de inglés Intermedio - Avanzado\n- Experiencia en metodologías ágiles como Scrum\n- Git\n Beneficios del Archer\n- Día administrativo semestral hasta los 12 meses \n- Week off: 5 días de vacaciones extra\n- ¡Celebra tu cumpleaños!\n- Path de entrenamiento\n- Certificaciones AWS\n- Aguinaldos Fiestas Patrias y Navidad\n- Bono de salud\n- Flexibilidad (trabajo híbrido o remoto)\n- Regalo por casamiento + 5 días hábiles libres\n- Regalo por nacimiento de hijos\n- Kit escolar\n- Beneficio paternidad\n- Bonda        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4223640231/",
         "High",
         "Mid-Senior",
         "No",
         "Yes",
         "Not Specified",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'APIs'\n 'Migration' 'Version control (GIT or similar)']"
        ],
        [
         "7",
         "Data Engineer",
         "BC Tecnología",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205297472/",
         "Could not find Job Description",
         null,
         null,
         null,
         null,
         "https://www.linkedin.com/jobs/view/4205297472/",
         "Low",
         "Not Specified",
         "No",
         "No",
         "Not Specified",
         "No",
         "No Mention",
         "[]"
        ],
        [
         "8",
         "Data Engineer",
         "ENGIE Impact",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-12 00:00:00",
         "https://www.linkedin.com/jobs/view/4226289082/",
         "DATA ENGINEER \nLocation: HYBRID/ SANTIAGO CHILE\nBi Lingual- English /Spanish Required \nINTERESTED APPLICANTS PLEASE SUBMIT AN ENGLISH VERSION RESUME\nSUMMARY: \n- In this position, you will be responsible for designing, developing, implementing, deploying, documenting, managing, and supporting enterprise cloud ETL processes and environments.\nPRIMARY FUNCTIONS AND ESSENTIAL RESPONSIBILITIES:\n- Support ENGIE Insight’s computer systems and ETL (“Extract, Transform, Load”) data processes.\n- Translate business requirements into data models that drive data lake or mart or warehouse design and configuration.\n- Actively participate in estimating the level of effort and documenting the work breakdown.\n- Actively participate in team ceremonies (daily stand-up, backlog refinement, review, retrospectives, etc.)\n- Work with cross-functional teams to gather, document, and approve business requirements for data analysis and reporting projects.\n- Actively participate in translating business requirements into a system design specification to manage Unstructured, Transactional, Hierarchical, Master and metadata.\n- Actively participate in designing, developing, and implementing data integration (ETL) processes to transform unstructured and disparate source data into target data stores, lakes, marts, or warehouses.\n- Actively participate in designing, developing, and implementing test automation processes.\n- Review implementation and maintenance of ETL processes that support feature development and testing.\n- Monitor data lake, mart, or warehouse ETL processes and implement tuning to address scalability, recoverability, and performance impediments.\n- Provide on-call support to Business Intelligence environments for internal and external users.\n- Plan and conduct development work on data integration projects necessitating the origination and application of new and unique approaches.\nQUALIFICATIONS AND REQUIREMENTS: \n- Education/ Certifications/ Experience\n- Bachelor's Degree required in one of the following fields: Computer Science, Computer Engineering, Electrical & Computer Engineering, Information Systems, or a closely related field\n- 0-2 years of data engineering or software engineering experience preferred\n- Azure or AWS (Amazon Web Services) data engineer certification a plus\n- May consider a combination of relevant experience with educational and other skills and abilities in lieu of educational requirements\n- Competencies/ Skills/ Abilities\n- Strong verbal, written and interpersonal skills.\n- Experience with the Microsoft Office suite of products required\n- Familiarity with systems: SQL Server, Snowflake, or other database technology\n- Basic understanding of data modeling preferred\n- Basic understanding of C# or any OOP programming language preferred\n- Basic understanding of SQL query language required\n- Basic understanding of OOP concepts preferred\n- Basic understanding of various data formats (XML, JSON, parquet, etc..) required\n- Azure Data Factory, Informatica, or another cloud ETL tool experience preferred\n- Familiarity or work experience with Azure Data Factory a plus\n- Other\n- Passion to drive Engie Impact’s mission and values\nWORK ENVIRONMENT:\n- Work schedules are determined by business need and manager discretion; full time employment is considered 40 hours per week\n- Remote work is permitted from a residence within commuting distance of ENGIE’s Spokane worksite at manager discretion\n- Incumbent must be available for on-call responsibilities during scheduled time\n- Off-site travel to local events may be required up to 10 days per year.\n- Health & Safety Working Requirements\n- Adequate working surface (can fit two monitors, a keyboard, mouse, and docking station)\n- Adjustable ergonomic chair\n- Proper Lighting\n- Heating, air conditioning and ventilation to create a comfortable environment\n- Appropriate internet and bandwidth to conduct business\n- Incumbent may be exposed to frequent noise caused by telephones, office machines, and nearby oral communications among fellow employees\n- As a global organization, attending meetings and events during early mornings and evenings may be required\n- Performing duties and attending events during the evening and on the weekend occurs occasionally and may be required\n- Business travel may be required up to 10% of the time\nREQUIRED PHYSICAL ACTIVITIES:\n- Manual and physical dexterity needed to operate a computer keyboard and handle paper documents\n- Adequate hearing and verbal abilities to communicate effectively in person, by telephone, and by video call\n- Sufficient near vision acuity to read information appearing on computer display screen, in handwritten forms, and printed on paper\n- Entering text or data into a computer or other machine with a traditional keyboard. Traditional Keyboard refers to a panel of keys used as the primary input device on a computer, typographic machine, or 10-Key numeric keypad.\n- Clarity of vision at approximately 20 inches or less (i.e., working with small objects or reading small print), including use of computers.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "services_for_renewable_energy_and_technology,_information_and_media",
         "https://www.linkedin.com/jobs/view/4226289082/",
         "High",
         "Junior",
         "Yes",
         "Yes",
         "0",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Data monitoring']"
        ],
        [
         "9",
         "Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225594752/",
         "We are seeking a highly skilled and motivated \nData Engineer\n to join our team.\nThe ideal candidate will be responsible for maintaining existing data systems, enhancing our current Data Platform, and implementing future changes as our clients' consumption needs and ingestion evolve. This role requires expertise in handling complex datasets and collaborating with senior team members on high-impact projects.\nResponsibilities\n- Maintain and enhance existing data systems\n- Develop enhancements for the existing Data Platform\n- Implement future changes to accommodate growing consumption needs and ingestion\n- Work with complex datasets, including the addition of new datasets from scratch\n- Collaborate with senior team members on complex projects, particularly when adding new large datasets\n- Grow with the technological landscape, adapting to new tools and methodologies as they emerge\nRequirements\n- 2+ experience in Data Software Engineering\n- Proficiency in PySpark\n- Experience with Azure Databricks, Azure DevOps, and Azure Event Hubs\n- Knowledge of the Azure cloud stack\n- Experience in working with large and complex datasets\n- Ability to work in a team and collaborate with various stakeholders\n- Familiarity with Databricks Unity Catalog and Terraform\nNice to have\n- Experience in Azure Analytics Engineering\n- Familiarity with DevOps practices\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4225594752/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "2",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge' 'CI/CD']"
        ],
        [
         "10",
         "Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-15 00:00:00",
         "https://www.linkedin.com/jobs/view/4230961634/",
         "We are looking for a talented and driven \nData Engineer \nto join our team.\nIn this role, you will play a key part in maintaining and improving our data systems while ensuring they align with evolving business needs. You will work with complex datasets and collaborate on impactful projects to enhance our Data Platform, adapting to technological advancements along the way.\nResponsibilities\n- Support and improve existing data infrastructure\n- Build and implement updates to the current Data Platform\n- Adapt systems to meet increasing data consumption and ingestion demands\n- Handle complex datasets, including creating new datasets from the ground up\n- Work closely with senior team members on key projects, particularly those involving large datasets\n- Stay current with emerging tools and technologies to ensure continuous improvement\nRequirements\n- At least 2 years of experience in Data Software Engineering\n- Strong expertise in PySpark\n- Hands-on experience with Azure Databricks, Azure DevOps, and Azure Event Hubs\n- Solid understanding of the Azure cloud ecosystem\n- Proven ability to work with large and complex datasets\n- Strong teamwork and collaboration skills to engage with various stakeholders\n- Familiarity with Databricks Unity Catalog and Terraform\nNice to have\n- Background in Azure Analytics Engineering\n- Understanding of DevOps principles and practices\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4230961634/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "2",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge' 'CI/CD'\n 'Version control (GIT or similar)']"
        ],
        [
         "11",
         "Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-16 00:00:00",
         "https://www.linkedin.com/jobs/view/4231045548/",
         "We are seeking a highly skilled and motivated \nData Engineer\n to join our team.\nThe ideal candidate will be responsible for maintaining existing data systems, enhancing our current Data Platform, and implementing future changes as our clients' consumption needs and ingestion evolve. This role requires expertise in handling complex datasets and collaborating with senior team members on high-impact projects.\nResponsibilities\n- Maintain and enhance existing data systems\n- Develop enhancements for the existing Data Platform\n- Implement future changes to accommodate growing consumption needs and ingestion\n- Work with complex datasets, including the addition of new datasets from scratch\n- Collaborate with senior team members on complex projects, particularly when adding new large datasets\n- Grow with the technological landscape, adapting to new tools and methodologies as they emerge\nRequirements\n- 2+ experience in Data Software Engineering\n- Proficiency in PySpark\n- Experience with Azure Databricks, Azure DevOps, and Azure Event Hubs\n- Knowledge of the Azure cloud stack\n- Experience in working with large and complex datasets\n- Ability to work in a team and collaborate with various stakeholders\n- Familiarity with Databricks Unity Catalog and Terraform\nNice to have\n- Experience in Azure Analytics Engineering\n- Familiarity with DevOps practices\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4231045548/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "2",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge' 'CI/CD']"
        ],
        [
         "12",
         "Data Engineer",
         "Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-11 00:00:00",
         "https://www.linkedin.com/jobs/view/4207045620/",
         "Descripción Empresa\r\nSomos más de 80 mil personas que cada día trabajamos por el firme Propósito - Simplificar y Disfrutar más la Vida. Estamos presentes en 9 países y compuestos por grandes marcas posicionadas de diversas industrias. Falabella Retail, Sodimac, Banco Falabella, Tottus, Mallplaza, Falabella.com, Falabella Inmobiliario. Cada una de éstas nos hace ser quienes somos, y es entre todos, como Un Solo Equipo, que buscamos diariamente reinventarnos y superar las experiencias de nuestros clientes.\r\nSi eres trabajador de Falabella, revisa todos los cursos disponibles en la Academia Falabella, que te ayudarán a seguir impulsando tu desarrollo y preparar tu próxima aventura con nosotros!\r\nSOMOS UNA EMPRESA QUE APOYA LA LEY 21015, APOYAMOS LA DIVERSIDAD Y LA INCLUSIÓN EN TODAS SUS FORMAS, SIN IMPORTAR RELIGIÓN, RAZA, GÉNERO, SITUACIÓN DE DISCAPACIDAD, NACIONALIDAD.\r\nFunciones Del Cargo\r\n¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti!\r\nEn Falabella Retail buscamos a nuestro/a próximo/a Data Engineer, con base en Santiago, Chile.\r\nSomos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\r\n¿Cuál es el principal objetivo del cargo?\r\nLiderar la construcción y mantención de estructuras de datos, así como la arquitectura tecnológica requerida para el procesamiento de apps.\r\n¿Qué harás en el día a día?\r\n-  Desarrollo, implementación de procesos ETL.\r\n-  Levantamiento de requerimientos funcionales y técnicos relacionados con los clientes internos.\r\n-  Implementar modelos de datos automatizados para transformar datos de acuerdo a los requisitos del negocio.\r\n-  Migración de datos desde entornos on-premise a entornos Cloud.\r\n-  Trabajar con tecnologías Google Cloud Platform (Big Query).\r\n¿Qué necesitas para postular?\r\n-  Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\r\n-  Conocimiento en SQL (excluyente)\r\n-  Sólidos conocimientos en Google Cloud Platform (excluyente)\r\n-  Conocimiento avanzado en Python (excluyente)\r\n-  Conocimiento y experiencia trabajando en GIT (excluyente)\r\n-  Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\r\nEn Nuestro Equipo Encontrarás\r\n-  Espacios para crear e innovar.\r\n-  Serás parte de un lugar lleno de oportunidades de desarrollo.\r\n-  Tener un trabajo con sentido y donde se promueve la calidad de vida.\r\n-  Participar en voluntariados.\r\n-  ¡Pertenecer a una empresa llena de energía!\r\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\r\nSomos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\r\nRequisitos\r\n- Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\r\n- Conocimiento en SQL (excluyente)\r\n- Sólidos conocimientos en Google Cloud Platform (excluyente)\r\n- Conocimiento avanzado en Python (excluyente)\r\n- Conocimiento y experiencia trabajando en GIT (excluyente)\r\n- Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\r\nCondiciones Oferta\r\nDescripción proceso de selección:\r\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\r\nPara Postular Solo Necesitas\r\n-  Postular a la oferta\r\n-  Revisar tu email\r\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\r\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\r\n                \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "Retail",
         "https://www.linkedin.com/jobs/view/4207045620/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Version control (GIT or similar)' 'Migration']"
        ],
        [
         "13",
         "Data Engineer",
         "Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225586690/",
         "Descripción Empresa\nSomos más de 80 mil personas que cada día trabajamos por el firme Propósito - Simplificar y Disfrutar más la Vida. Estamos presentes en 9 países y compuestos por grandes marcas posicionadas de diversas industrias. Falabella Retail, Sodimac, Banco Falabella, Tottus, Mallplaza, Falabella.com, Falabella Inmobiliario. Cada una de éstas nos hace ser quienes somos, y es entre todos, como Un Solo Equipo, que buscamos diariamente reinventarnos y superar las experiencias de nuestros clientes.\nSi eres trabajador de Falabella, revisa todos los cursos disponibles en la Academia Falabella, que te ayudarán a seguir impulsando tu desarrollo y preparar tu próxima aventura con nosotros!\nSOMOS UNA EMPRESA QUE APOYA LA LEY 21015, APOYAMOS LA DIVERSIDAD Y LA INCLUSIÓN EN TODAS SUS FORMAS, SIN IMPORTAR RELIGIÓN, RAZA, GÉNERO, SITUACIÓN DE DISCAPACIDAD, NACIONALIDAD.\nFunciones Del Cargo\n¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti!\nEn Falabella Retail buscamos a nuestro/a próximo/a Data Engineer, con base en Santiago, Chile.\nSomos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\n¿Cuál es el principal objetivo del cargo?\nLiderar la construcción y mantención de estructuras de datos, así como la arquitectura tecnológica requerida para el procesamiento de apps.\n¿Qué harás en el día a día?\n-  Desarrollo, implementación de procesos ETL.\n-  Levantamiento de requerimientos funcionales y técnicos relacionados con los clientes internos.\n-  Implementar modelos de datos automatizados para transformar datos de acuerdo a los requisitos del negocio.\n-  Migración de datos desde entornos on-premise a entornos Cloud.\n-  Trabajar con tecnologías Google Cloud Platform (Big Query).\n¿Qué necesitas para postular?\n-  Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n-  Conocimiento en SQL (excluyente)\n-  Sólidos conocimientos en Google Cloud Platform (excluyente)\n-  Conocimiento avanzado en Python (excluyente)\n-  Conocimiento y experiencia trabajando en GIT (excluyente)\n-  Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nEn Nuestro Equipo Encontrarás\n-  Espacios para crear e innovar.\n-  Serás parte de un lugar lleno de oportunidades de desarrollo.\n-  Tener un trabajo con sentido y donde se promueve la calidad de vida.\n-  Participar en voluntariados.\n-  ¡Pertenecer a una empresa llena de energía!\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\nSomos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\nRequisitos\n- Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n- Conocimiento en SQL (excluyente)\n- Sólidos conocimientos en Google Cloud Platform (excluyente)\n- Conocimiento avanzado en Python (excluyente)\n- Conocimiento y experiencia trabajando en GIT (excluyente)\n- Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nCondiciones Oferta\nDescripción proceso de selección:\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\nPara Postular Solo Necesitas\n-  Postular a la oferta\n-  Revisar tu email\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "retail",
         "https://www.linkedin.com/jobs/view/4225586690/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "14",
         "Data Engineer",
         "Genesys",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4218817757/",
         "Somos una empresa con más de 34 años de experiencia en el mercado, especializada en fábrica de desarrollo de software y Outsourcing de profesionales TI. Nuestra casa matriz está en Concepción y también operamos en Santiago. Actualmente, buscamos incorporar a nuestro equipo un/a Data Engineer\nCondiciones de contratación\n- Ubicación: híbrido en Providencia, Santiago;\n- Modalidad: híbrida;\n- Contrato: por proyecto (6 meses);\n- Jornada: completa (44 horas semanales).\nResponsabilidades\n- Diseñar, desarrollar y mantener pipelines de datos eficientes y escalables (ETL/ELT) para integrar diversas fuentes de datos, respondiendo a las necesidades de los equipos de ciencia de datos y análisis\n- Implementar y administrar arquitecturas de almacenamiento de datos (Data Warehouses, Data Lakes, etc.).\n- Garantizar la calidad, integridad y seguridad de los datos procesados.\n- Colaborar con equipos de analítica y ciencia de datos para proporcionar datos limpios y procesables.\n- Optimizar el rendimiento y la escalabilidad de las infraestructuras de datos.\n- Implementar soluciones de monitoreo y diagnóstico para los sistemas de datos.\n- Documentar arquitecturas, procesos y flujos de trabajo relacionados con los datos.\nRequisitos\n- Desde 3 años a 5 años de experiencia;\n- Experiencia en la creación de pipelines de datos utilizando herramientas como Apache Airflow, Apache Nifi, Step Functions o similares.\n- Conocimientos avanzados de bases de datos relacionales y no relacionales (SQL, PostgreSQL, MongoDB, etc.). Dominio de PL-SQL.\n- Dominio de lenguajes de programación como Python o Scala.\n- Experiencia trabajando con plataformas de Big Data como Hadoop o Spark.\n- Familiaridad con arquitecturas en la nube (Idealmente sobre AWS) y herramientas relacionadas (S3, EMR, Athena, Glue, Redshift, etc.).\n- Experiencia en la implementación de procesos de control de calidad de datos y depuración..\n¡Postula!        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "software_development_and_financial_services",
         "https://www.linkedin.com/jobs/view/4218817757/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "3",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Spark knowledge']"
        ],
        [
         "15",
         "Data Engineer",
         "ICONSTRUYE",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205295718/",
         "En ICONSTRUYE, hemos estado a la vanguardia de la tecnología en la construcción durante más de 20 años. Nuestra robusta plataforma tecnológica es un testimonio de nuestra experiencia y compromiso con la industria. Con más de 4,000 clientes en Chile, Colombia y Perú, nos enorgullecemos de proporcionar soluciones integrales que simplifican la cadena de abastecimiento. Buscamos un Ingeniero de Datos que se una a nosotros en la transformación de la industria de la construcción, siendo el puente entre los datos brutos y aquellos que toman decisiones críticas.\r\nTus Funciones Principales\r\nTu misión:\r\n Ser el puente entre los datos brutos y quienes necesitan realizar análisis y/o tomar decisiones con esos datos.\r\n- Garantizar la calidad, integridad y seguridad de los datos.\r\n- Colaborar con diversos stakeholders para comprender sus necesidades de datos.\r\n- Desarrollar procesos de extracción, transformación y carga (ETL) de datos para nuestro data lake, proporcionando información valiosa para el análisis y toma de decisiones.\r\n- Implementar nuevas bases de datos y/o data warehouses para satisfacer las necesidades de la empresa.\r\n- Contribuir a la definición de políticas de gobernanza de datos.\r\n- Ser una autoridad en la creación, implementación y operación de soluciones escalables y de bajo costo, facilitando el flujo de datos desde sistemas de producción hasta el data lake.\r\nRequerimientos Técnicos\r\n- Dominio de Python o Go. \r\n- Dominio de SQL. \r\n- Conocimiento de base de datos relacionales y no relacionales (NoSQL). \r\n- Conocimiento de AirFlow, Luigi, Dagster. \r\n- Conocimientos de Kafka y/o RabbitMQ. \r\n- Conocimiento en Docker y Kubernetes. \r\nBeneficios Que Ofrecemos\r\n- 🌴 5 días extras de descanso al año.\r\n- 🍔 Tarjeta amipass para utilizar en restaurantes, delivery y supermercados.\r\n- 👨‍⚕️ Seguro complementario de salud, dental y de vida.\r\n- 🏠 Modalidad de trabajo híbrido.\r\n- 📠 Flexibilidad con permisos para trámites y asuntos familiares.\r\n- 👩‍👦 Jornada reducida en días de vacaciones escolares (viernes medio día).\r\n- 🎂 Tarde libre en tu cumpleaños.\r\n¡Únete a nosotros y sé parte de nuestra misión de transformar la industria de la construcción!\r\n                \r\n            \r\n                            \r\n            ",
         "Not Applicable",
         "Full-time",
         "Information Technology",
         "Technology, Information and Internet and Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4205295718/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes' 'Data quality' 'Data governance'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)']"
        ],
        [
         "16",
         "Data Engineer",
         "Mediastream",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4206504298/",
         "Description\r\nMediastream is a leading streaming technology company that has been in business for over 16 years. We collaborate with major companies and broadcasters around the world to offer a quality end-to-end solution for content administration, distribution, audiovisual production, and user experience. Our goal is to connect our customers with their audience in a simple and effective way, creating new revenue streams for their businesses.\r\nRole Description\r\nThis is a hibrid role for a Data Egineer. The Data Engineer will be responsible for proposing advanced applications of our data, reviewing patterns, deviations to detect trends to optimize dashboards and optimize advertising strategies. Promote new strategies and technologies to process, analyze and leverage customer data and improve user engagement. With the main objective of: Process, analyze and visualize audience, behavioral and engagement data to obtain valuable insights, which serve for product roadmap, Develop and manage efficient data pipelines and ETL workflows for the integration of diverse data sources and Use machine learning models and recommendation algorithms to personalize strategies.\r\nResponsibilities\r\n- Create, implement and maintain the company's data architecture.\r\n- Process, analyze and visualize audience, behavioral and engagement data to obtain valuable insights for product roadmap.\r\n- Develop and manage data pipelines and ETL workflows for the integration of diverse data sources.\r\n- Use machine learning models and recommendation algorithms to personalize strategies.\r\n- Create interactive visualizations and dashboards for studies to monitor performance and make data-driven decisions.\r\n- Collaborate closely with the development team to integrate, create features and roadmap proposals. \r\n- Have a product vision and work closely with the marketing and development teams to align strategies with business objectives.\r\n\">\r\nMediastream is a leading streaming technology company that has been in business for over 16 years. We collaborate with major companies and broadcasters around the world to offer a quality end-to-end solution for content administration, distribution, audiovisual production, and user experience. Our goal is to connect our customers with their audience in a simple and effective way, creating new revenue streams for their businesses.\r\nRole Description\r\nThis is a hibrid role for a Data Egineer. The Data Engineer will be responsible for proposing advanced applications of our data, reviewing patterns, deviations to detect trends to optimize dashboards and optimize advertising strategies. Promote new strategies and technologies to process, analyze and leverage customer data and improve user engagement. With the main objective of: Process, analyze and visualize audience, behavioral and engagement data to obtain valuable insights, which serve for product roadmap, Develop and manage efficient data pipelines and ETL workflows for the integration of diverse data sources and Use machine learning models and recommendation algorithms to personalize strategies.\r\nResponsibilities\r\n- Create, implement and maintain the company's data architecture.\r\n- Process, analyze and visualize audience, behavioral and engagement data to obtain valuable insights for product roadmap.\r\n- Develop and manage data pipelines and ETL workflows for the integration of diverse data sources.\r\n- Use machine learning models and recommendation algorithms to personalize strategies.\r\n- Create interactive visualizations and dashboards for studies to monitor performance and make data-driven decisions.\r\n- Collaborate closely with the development team to integrate, create features and roadmap proposals.\r\n- Have a product vision and work closely with the marketing and development teams to align strategies with business objectives.\r\nMinimum Requirements\r\n- At least 3 years of experience in Data Engineer roles.\r\n- Bachelor's degree in computer science or related field.\r\n- Knowledge in: SQL/No SQL: Advanced, Power BI, Looker, tableau or others: Intermediate, Python and development knowledge: Basic, machine learning, AI or similar technologies: Basic, Identification of cluster patterns, trends, deviations: Intermediate, cloud technologies: Intermediate.\r\nSoft Skills:\r\n- Teamwork\r\n- Decision-making\r\n- Attention to detail\r\n- Adaptability \r\n\">\r\n- At least 3 years of experience in Data Engineer roles.\r\n- Bachelor's degree in computer science or related field.\r\n- Knowledge in: SQL/No SQL: Advanced, Power BI, Looker, tableau or others: Intermediate, Python and development knowledge: Basic, machine learning, AI or similar technologies: Basic, Identification of cluster patterns, trends, deviations: Intermediate, cloud technologies: Intermediate.\r\nSoft Skills:\r\n- Teamwork\r\n- Decision-making\r\n- Attention to detail\r\n- Adaptability        \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4206504298/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "No",
         "3",
         "Yes",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "17",
         "Data Engineer",
         "NeuralWorks",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205500303/",
         "NeuralWorks es una compañía de alto crecimiento fundada hace 3 años. Estamos trabajando a toda máquina en cosas que darán que hablar.\r\nSomos un equipo donde se unen la creatividad, curiosidad y la pasión por hacer las cosas bien. Nos arriesgamos a explorar fronteras donde otros no llegan: un modelo predictor basado en monte carlo, una red convolucional para detección de caras, un sensor de posición bluetooth, la recreación de un espacio acústico usando finite impulse response.\r\nEstos son solo algunos de los desafíos, donde aprendemos, exploramos y nos complementamos como equipo para lograr cosas impensadas.\r\nTrabajamos en proyectos propios y apoyamos a corporaciones en partnerships donde codo a codo combinamos conocimiento con creatividad, donde imaginamos, diseñamos y creamos productos digitales capaces de cautivar y crear impacto.\r\n👉 Conoce más sobre nosotros\r\n Descripción del trabajo\r\nEl equipo de Data y Analytics trabaja en diferentes proyectos que combinan volúmenes de datos enormes e IA, como detectar y predecir fallas antes que ocurran, optimizar pricing, personalizar la experiencia del cliente, optimizar uso de combustible, detectar caras y objetos usando visión por computador.\r\nDentro del equipo multidisciplinario con Data Scientist, Translators, DevOps, Data Architect, tu rol será clave en construir y proveer los sistemas e infraestructura que permiten el desarrollo de estos servicios, formando los cimientos sobre los cuales se construyen los modelos que permiten generar impacto, con servicios que deben escalar, con altísima disponibilidad y tolerantes a fallas, en otras palabras, que funcionen. Además, mantendrás tu mirada en los indicadores de capacidad y performance de los sistemas.\r\nEn cualquier proyecto que trabajes, esperamos que tengas un gran espíritu de colaboración, pasión por la innovación y el código y una mentalidad de automatización antes que procesos manuales.\r\nComo Data Engineer, Tu Trabajo Consistirá En\r\n- Participar activamente durante el ciclo de vida del software, desde inception, diseño, deploy, operación y mejora.\r\n- Apoyar a los equipos de desarrollo en actividades de diseño y consultoría, desarrollando software, frameworks y capacity planning.\r\n- Desarrollar y mantener arquitecturas de datos, pipelines, templates y estándares.\r\n- Conectarse a través de API a otros sistemas (Python)\r\n- Manejar y monitorear el desempeño de infraestructura y aplicaciones.\r\n- Asegurar la escalabilidad y resiliencia.\r\n Calificaciones clave\r\n- Estudios de Ingeniería Civil en Computación o similar.\r\n- Experiencia práctica de al menos 3 años en entornos de trabajo como Data Engineer, Software Engineer entre otros.\r\n- Experiencia con Python. Entendimiento de estructuras de datos con habilidades analíticas relacionadas con el trabajo con conjuntos de datos no estructurados, conocimiento avanzado de SQL, incluida optimización de consultas.\r\n- Pasión en problemáticas de procesamiento de datos.\r\n- Experiencia con servidores cloud (GCP, AWS o Azure), especialmente el conjunto de servicios de procesamiento de datos.\r\n- Buen manejo de inglés, sobre todo en lectura donde debes ser capaz de leer un paper, artículos o documentación de forma constante.\r\n- Habilidades de comunicación y trabajo colaborativo.\r\n¡En NeuralWorks nos importa la diversidad! Creemos firmemente en la creación de un ambiente laboral inclusivo, diverso y equitativo. Reconocemos y celebramos la diversidad en todas sus formas y estamos comprometidos a ofrecer igualdad de oportunidades para todos los candidatos.\r\n“Los hombres postulan a un cargo cuando cumplen el 60% de las calificaciones, pero las mujeres sólo si cumplen el 100%.” D. Gaucher , J. Friesen and A. C. Kay, Journal of Personality and Social Psychology, 2011.\r\nTe invitamos a postular aunque no cumplas con todos los requisitos.\r\n Nice to have\r\n- Agilidad para visualizar posibles mejoras, problemas y soluciones en Arquitecturas.\r\n- Experiencia en Infrastructure as code, observabilidad y monitoreo.\r\n- Experiencia en la construcción y optimización de data pipelines, colas de mensajes y arquitecturas big data altamente escalables.\r\n- Experiencia en procesamiento distribuido utilizando servicios cloud.\r\n Beneficios\r\n- MacBook Air M2 o similar (con opción de compra hiper conveniente)\r\n- Bono por desempeño\r\n- Bono de almuerzo mensual y almuerzo de equipo los viernes\r\n- Seguro complementario de salud y dental\r\n- Horario flexible\r\n- Flexibilidad entre oficina y home office\r\n- Medio día libre el día de tu cumpleaños\r\n- Financiamiento de certificaciones\r\n- Inscripción en Coursera con plan de entrenamiento a medida\r\n- Estacionamiento de bicicletas\r\n- Vestimenta informal\r\n- Programa de referidos\r\n- Salida de “teambuilding” mensual        \r\n            \r\n                            \r\n            ",
         "Not Applicable",
         "Full-time",
         "Information Technology",
         "Technology, Information and Internet and Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4205500303/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "3",
         "Yes",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'APIs'\n 'Version control (GIT or similar)'\n 'Collaboration with data scientists or analysts' 'Data monitoring']"
        ],
        [
         "18",
         "Data Engineer",
         "NeuralWorks",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4216875700/",
         "NeuralWorks es una compañía de alto crecimiento fundada hace 3 años. Estamos trabajando a toda máquina en cosas que darán que hablar.\nSomos un equipo donde se unen la creatividad, curiosidad y la pasión por hacer las cosas bien. Nos arriesgamos a explorar fronteras donde otros no llegan: un modelo predictor basado en monte carlo, una red convolucional para detección de caras, un sensor de posición bluetooth, la recreación de un espacio acústico usando finite impulse response.\nEstos son solo algunos de los desafíos, donde aprendemos, exploramos y nos complementamos como equipo para lograr cosas impensadas.\nTrabajamos en proyectos propios y apoyamos a corporaciones en partnerships donde codo a codo combinamos conocimiento con creatividad, donde imaginamos, diseñamos y creamos productos digitales capaces de cautivar y crear impacto.\n👉 Conoce más sobre nosotros\n Descripción del trabajo\nEl equipo de Data y Analytics trabaja en diferentes proyectos que combinan volúmenes de datos enormes e IA, como detectar y predecir fallas antes que ocurran, optimizar pricing, personalizar la experiencia del cliente, optimizar uso de combustible, detectar caras y objetos usando visión por computador.\nDentro del equipo multidisciplinario con Data Scientist, Translators, DevOps, Data Architect, tu rol será clave en construir y proveer los sistemas e infraestructura que permiten el desarrollo de estos servicios, formando los cimientos sobre los cuales se construyen los modelos que permiten generar impacto, con servicios que deben escalar, con altísima disponibilidad y tolerantes a fallas, en otras palabras, que funcionen. Además, mantendrás tu mirada en los indicadores de capacidad y performance de los sistemas.\nEn cualquier proyecto que trabajes, esperamos que tengas un gran espíritu de colaboración, pasión por la innovación y el código y una mentalidad de automatización antes que procesos manuales.\nComo Data Engineer, Tu Trabajo Consistirá En\n- Participar activamente durante el ciclo de vida del software, desde inception, diseño, deploy, operación y mejora.\n- Apoyar a los equipos de desarrollo en actividades de diseño y consultoría, desarrollando software, frameworks y capacity planning.\n- Desarrollar y mantener arquitecturas de datos, pipelines, templates y estándares.\n- Conectarse a través de API a otros sistemas (Python)\n- Manejar y monitorear el desempeño de infraestructura y aplicaciones.\n- Asegurar la escalabilidad y resiliencia.\n Calificaciones clave\n- Estudios de Ingeniería Civil en Computación o similar.\n- Experiencia práctica de al menos 3 años en entornos de trabajo como Data Engineer, Software Engineer entre otros.\n- Experiencia con Python. Entendimiento de estructuras de datos con habilidades analíticas relacionadas con el trabajo con conjuntos de datos no estructurados, conocimiento avanzado de SQL, incluida optimización de consultas.\n- Pasión en problemáticas de procesamiento de datos.\n- Experiencia con servidores cloud (GCP, AWS o Azure), especialmente el conjunto de servicios de procesamiento de datos.\n- Buen manejo de inglés, sobre todo en lectura donde debes ser capaz de leer un paper, artículos o documentación de forma constante.\n- Habilidades de comunicación y trabajo colaborativo.\n¡En NeuralWorks nos importa la diversidad! Creemos firmemente en la creación de un ambiente laboral inclusivo, diverso y equitativo. Reconocemos y celebramos la diversidad en todas sus formas y estamos comprometidos a ofrecer igualdad de oportunidades para todos los candidatos.\n“Los hombres postulan a un cargo cuando cumplen el 60% de las calificaciones, pero las mujeres sólo si cumplen el 100%.” D. Gaucher , J. Friesen and A. C. Kay, Journal of Personality and Social Psychology, 2011.\nTe invitamos a postular aunque no cumplas con todos los requisitos.\n Nice to have\n- Agilidad para visualizar posibles mejoras, problemas y soluciones en Arquitecturas.\n- Experiencia en Infrastructure as code, observabilidad y monitoreo.\n- Experiencia en la construcción y optimización de data pipelines, colas de mensajes y arquitecturas big data altamente escalables.\n- Experiencia en procesamiento distribuido utilizando servicios cloud.\n Beneficios\n- MacBook Air M2 o similar (con opción de compra hiper conveniente)\n- Bono por desempeño\n- Bono de almuerzo mensual y almuerzo de equipo los viernes\n- Seguro complementario de salud y dental\n- Horario flexible\n- Flexibilidad entre oficina y home office\n- Medio día libre el día de tu cumpleaños\n- Financiamiento de certificaciones\n- Inscripción en Coursera con plan de entrenamiento a medida\n- Estacionamiento de bicicletas\n- Vestimenta informal\n- Programa de referidos\n- Salida de “teambuilding” mensual        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4216875700/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "3",
         "Yes",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'APIs' 'Data monitoring'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "19",
         "Data Engineer",
         "Perform",
         "Chile",
         "2025-05-13 00:00:00",
         "https://www.linkedin.com/jobs/view/4227361352/",
         "We are seeking a skilled and motivated Data Engineer to join our Business Technology team. The team is responsible for driving business data analytics and ensuring robust data engineering practices. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining scalable data pipelines and infrastructure.\nRequired Skills:\n- Strong experience with \nPython\n programming for data engineering tasks.\n- Proficiency in data manipulation and transformation.\n- Advanced\n SQL\n skills for database management and querying.\n- Hands-on experience with \nAWS\n.\n- Experience with \nTerraform\n for infrastructure automation.\n- Proficiency in \nLooker\n and \nSnowflake\n.\n- Strong understanding of data modeling principles and best practices.\n- Upper Intermediate English level.\nKey Responsibilities:\n- Design, develop, and maintain scalable data pipelines and ETL processes to support business data analytics.\n- Perform data manipulation, transformation, and cleansing to ensure data quality and integrity.\n- Develop and maintain database solutions using SQL.\n- Implement data models and optimize data storage solutions in Snowflake.\n- Utilize AWS cloud services for data storage, processing, and analytics.\n- Use Terraform for infrastructure as code to manage and deploy cloud resources.\n- Create and maintain reports and dashboards using Looker.\n- Collaborate with the team to continuously improve data engineering practices and processes.\nThis position is open to Latin America only        \n            \n                            \n            ",
         "not_applicable",
         "other",
         "information_technology",
         "information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4227361352/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Data quality']"
        ],
        [
         "20",
         "Data Engineer",
         "Seeds",
         "Santiago Metropolitan Region, Chile",
         "2025-04-14 00:00:00",
         "https://www.linkedin.com/jobs/view/4189755534/",
         "¿Sos \nData Engineer\n? Entonces… ¿Qué estás esperando para sumarte a nuestra comunidad de Seeders? ¡Aplica a nuestra comunidad y accede a trabajo on-demand en las empresas líderes, sumate al Present of Work!\n¿Quiénes somos?\nSomos una \ncomunidad\n que reúne al mejor talento on-demand de Latinoamérica, y lo conecta con las empresas líderes de la región. Gestionamos el match perfecto entre las necesidades de las empresas y el talento con las competencias y la experiencia buscada, fomentando flexibilidad y el desarrollo profesional de nuestra comunidad.\nNo somos una plataforma más de freelancers, Seeds lidera un dream team de profesionales altamente calificados que eligen dónde, cómo y para quién trabajar, disfrutando así de contribuir a una misión más grande, definiendo y moldeando la forma en que trabajamos.\nEstamos buscando sumar a nuestro Talent Pool roles de \nData Engineer\n para nuestra comunidad de Seeders.\nEstas son algunas de las responsabilidades usuales del rol:\n- Diseñar, construir y mantener arquitecturas de datos robustas y escalables.\n- Desarrollar y optimizar pipelines de datos para recopilación, almacenamiento, procesamiento y análisis de grandes volúmenes de datos.\n- Implementar modelos de datos y algoritmos para resolver problemas de negocio y proveer insights accionables.\n- Trabajar en estrecha colaboración con equipos de data scientists y analistas para apoyar sus requisitos de datos y facilitar el análisis de datos.\n- Asegurar la integridad, disponibilidad y confidencialidad de los datos a través de las mejores prácticas de seguridad y gobernanza de datos.\n- Mantenerse al día con las últimas tecnologías y tendencias en el campo de la ingeniería de datos.\nRequisitos\n- Experiencia mínima de 3 años en roles de ingeniería de datos.\n- Fuerte dominio de lenguajes de programación como Python, Java o Scala.\n- Experiencia trabajando con grandes volúmenes de datos y herramientas de procesamiento de datos (como Hadoop, Spark).\n- Conocimientos en bases de datos SQL y NoSQL, así como en soluciones de almacenamiento de datos en la nube (AWS, Google Cloud, Azure).\n- Capacidad para trabajar en entornos ágiles y multidisciplinarios.\n- Inglés intermedio (deseable).\n¿Por qué sumarte a nuestra comunidad de Seeders?\nElegí tus proyectos.\nTrabajá desde donde vos quieras.\nEventos de networking.\nAsesoramiento personalizado.\nSeeds Academy: Potencia tu desarrollo profesional adquiriendo nuevas skills (upskilling & reskilling), participando de webinars, Bootcamps y otras acciones exclusivas para la comunidad.\nNo dejes de sumarte a nuestra comunidad de Seeds y aplicar a oportunidades de empresas lideres de la región. ¡Te esperamos!        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Consulting",
         "Technology, Information and Media",
         "https://www.linkedin.com/jobs/view/4189755534/",
         "High",
         "Senior",
         "No",
         "No",
         "3",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Collaboration with data scientists or analysts' 'Data governance'\n 'Spark knowledge']"
        ],
        [
         "21",
         "Data Engineer",
         "Tsoft",
         "Santiago Metropolitan Region, Chile",
         "2025-04-15 00:00:00",
         "https://www.linkedin.com/jobs/view/4191440590/",
         "En Tsoft estamos en la búsqueda de un \nData Engineer\n con al menos \n3 años de experiencia\n para diseñar, desarrollar y gestionar sistemas de procesamiento de datos. Será responsable de garantizar que los datos sean accesibles, organizados y seguros, optimizando su disponibilidad y precisión.\nResponsabilidades:\n- Creación y optimización de \nqueries\n para el procesamiento de datos.\n- Implementación de flujos de trabajo en \nAirflow\n.\n- Obtención, depuración, filtrado y preparación de datos para su análisis.\n- Diseño y desarrollo de sistemas de procesamiento de datos eficientes.\n- Asegurar la seguridad e integridad de los datos almacenados y procesados.\n- Garantizar que los datos estén organizados y sean de fácil acceso para los equipos de negocio y analítica.\nRequisitos Excluyentes:\n- Experiencia y manejo avanzado en \nGCP BigQuery\n.\n- Conocimiento en \nAirflow DBT\n.\n- Experiencia en el uso de \nGitHub\n para control de versiones.\nConocimientos Deseables:\n- Manejo de herramientas de gestión de proyectos como \nJira\n.\n- Documentación y gestión del conocimiento en \nConfluence\n.\nSi cumples con los requisitos y estás interesado/a en formar parte de un equipo dinámico y en crecimiento, ¡te invitamos a postular!        \n            \n                            \n            ",
         "Associate",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4191440590/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "3",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "22",
         "Data Engineer",
         "Xepelin",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4205474021/",
         "Somos una FinTech que busca democratizar los servicios financieros para todo tipo de empresas. Nos apalancamos en la mejor tecnología para crear soluciones ágiles, personalizadas y transparentes. Nuestro objetivo es ser la FinTech B2B más grande de Latam y convertirnos en el CFO digital de todas las empresas en la región.\r\nXepelin nace en Chile en 2019 y, desde entonces, hemos levantado más de USD145 millones en equity y USD300 millones en asset-backed facilities para potenciar el crecimiento en toda la región, especialmente en los países donde hoy operamos, Chile y México. La última ronda de equity fue de USD111 millones, record en Chile y una de las más grandes en América Latina para una FinTech.\r\n¿Por qué trabajar en Xepelin?\r\n💪 Desafío\r\nEstamos sacudiendo una de las industrias más poderosas y competitivas, eliminando fricciones para darle a las Pymes acceso a capital y apalancado en la última tecnología disponible. Todo esto poniendo siempre a nuestras Pymes en el centro.\r\nConstruir un banco digital desde cero es un gran proyecto; el producto es complejo y no se puede romper, existen regulaciones estrictas y tendremos que ser mejores que algunas de las corporaciones más grandes y consolidadas del mundo. Pero superar estos desafíos significa que habremos construido algo duradero.\r\n💥 Impacto\r\nTrabajar en un equipo de clase mundial y automotivado significa autonomía, amplia experiencia en todos los proyectos y ver que tus contribuciones afectan directamente al producto e impactan a nuestras Pymes.\r\nTrabajarás con todos nuestros productos, tendrás que tomar decisiones fundamentales. El correcto posicionamiento de ellos marcará su futuro.\r\n✔️ Calidad\r\nNuestro posicionamiento de marca y productos es algo diferenciador frente al resto del mercado por lo que invertimos mucho en crear productos de calidad que nuestras Pymes respeten y valoren.\r\nNos damos el tiempo para pensar fuera de la caja y volver con propuestas innovadoras. Estamos aquí para cambiar la industria!\r\n¿Qué estamos buscando? \r\nEn Xepelin estamos buscando personas creativas y visionarias que piensen fuera de la caja para sumarse a nuestro equipo. Si te apasiona resolver desafíos interesantes de alto impacto y quieres ser parte de un entorno dinámico que está transformando la industria financiera, ¡Esta oportunidad es para ti!\r\nEl rol se integrará a nuestro equipo de \r\nData Platform\r\n. Si te motiva el desafío de construir soluciones innovadoras en un entorno de rápido cambio, queremos conocerte.\r\nUnete a nosotros, crezcamos juntos!\r\nPrincipales responsabilidades...\r\n-  Diseñar, crear y mantener pipelines de datos\r\n-  Mantener y optimizar la infraestructura de datos necesaria para una extracción precisa, transformación y carga de datos de una amplia variedad de fuentes de datos\r\n-  Automatizar los flujos de trabajo de datos, como la ingesta de datos, la agregación y el procesamiento de ETL o ELT\r\n-  Preparar datos sin procesar en almacenes de datos en un conjunto de datos consumibles para fines técnicos y partes interesadas no técnicas\r\n-  Crear, mantener e implementar productos de datos para equipos de análisis y ciencia de datos en Plataformas en la nube, preferentemente en GCP, y/o AWS\r\n-  Desarrollar sistemas y arquitectura que soporten las diferentes etapas del flujo de Machine Learning\r\n¿Qué necesitas para brillar?\r\n- Conocimientos en alguna Nube, preferentemente GCP o AWS (en ese orden de preferencia)\r\n- Conocimientos intermedio/avanzado en Python\r\n- Conocimiento intermedio/avanzado de SQL\r\n- Experiencia trabajando almacenamiento en la nube como GCS o AWS S3\r\n- Experiencia desplegando aplicaciones en ambientes serverless como Cloud Functions o AWS Lambda\r\n- Conocimientos administrando y desplegando algún orquestador, por ejemplo: Dagster, Apache Airflow, Prefect, etc\r\n- Excelentes habilidades para trabajar en equipo. Ser humilde y saber colaborar, un Team Player!\r\n- Saber escuchar a tus stakeholders y poder traducir eso en requerimientos y ejecutarlos con tu equipo\r\n- Trabajo proactivo y responsable\r\n- Conocimientos en DBT\r\n- Conocimientos y manejo de lenguajes de programación y/o frameworks, NodeJS, Golang, por ejemplo\r\n- Experiencia en MLOps\r\n- No tener miedo a tomar decisiones y liderar proyectos\r\n- Foco en impacto e historia consistente entregando resultados para usuarios y el negocio\r\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\r\n- Te sientes cómodo cuestionando el status-quo de los servicios financieros, adaptándose rápidamente a los cambios, y presentando claramente tus ideas y conceptos para debatirlos en equipo\r\nNuestros Beneficios:\r\n🌴 Xepelin Balance\r\nVacaciones:\r\n 15 días hábiles. Por cada año que cumplas en Xepelin, te damos un día extra de vacaciones.\r\nBalance days:\r\n 10 días libres adicionales al año, para disfrutar como quieras.\r\nTrabajo híbrido y flexibilidad horaria según el rol. Trabajamos por objetivos.\r\nBeneficios Flexibles: \r\nPuntos flexibles en tu moneda local al mes para gastar en lo que quieras.\r\nXepelin Fun:\r\n Actividades de encuentro financiadas por Xepelin para divertirnos juntos.\r\n🚀 Xepelin Performance & Career\r\nPlataformas de capacitación:\r\n Convenios con las mejores plataformas, como Reforge, Udemy y DataCamp.\r\nKit de Bienvenida: \r\ntodo lo que necesitas para comenzar tu viaje en Xepelin 😊\r\n🤝 Xepelin Cares\r\nCobertura de salud:\r\n contamos con convenios de salud con proveedores de calidad o reembolsos según el país donde te encuentres.\r\nPost Natal:\r\n te damos una semana extra de licencia post natal. ¡Nos interesa que estés con tu familia y seres queridos!\r\nMatrimonio plus:\r\n Lleva tus planes al siguiente nivel, con una gift card y extendiendo tu permiso legal por matrimonio con dos días de regalo por Xepelin.        \r\n            \r\n                            \r\n            ",
         "Not Applicable",
         "Full-time",
         "Information Technology",
         "Software Development, IT Services and IT Consulting, and Biotechnology Research",
         "https://www.linkedin.com/jobs/view/4205474021/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "23",
         "Data Engineer",
         "Xepelin",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4224174809/",
         "Somos una FinTech que busca democratizar los servicios financieros para todo tipo de empresas. Nos apalancamos en la mejor tecnología para crear soluciones ágiles, personalizadas y transparentes. Nuestro objetivo es ser la FinTech B2B más grande de Latam y convertirnos en el CFO digital de todas las empresas en la región.\nXepelin nace en Chile en 2019 y, desde entonces, hemos levantado más de USD145 millones en equity y USD300 millones en asset-backed facilities para potenciar el crecimiento en toda la región, especialmente en los países donde hoy operamos, Chile y México. La última ronda de equity fue de USD111 millones, record en Chile y una de las más grandes en América Latina para una FinTech.\n¿Por qué trabajar en Xepelin?\n💪 Desafío\nEstamos sacudiendo una de las industrias más poderosas y competitivas, eliminando fricciones para darle a las Pymes acceso a capital y apalancado en la última tecnología disponible. Todo esto poniendo siempre a nuestras Pymes en el centro.\nConstruir un banco digital desde cero es un gran proyecto; el producto es complejo y no se puede romper, existen regulaciones estrictas y tendremos que ser mejores que algunas de las corporaciones más grandes y consolidadas del mundo. Pero superar estos desafíos significa que habremos construido algo duradero.\n💥 Impacto\nTrabajar en un equipo de clase mundial y automotivado significa autonomía, amplia experiencia en todos los proyectos y ver que tus contribuciones afectan directamente al producto e impactan a nuestras Pymes.\nTrabajarás con todos nuestros productos, tendrás que tomar decisiones fundamentales. El correcto posicionamiento de ellos marcará su futuro.\n✔️ Calidad\nNuestro posicionamiento de marca y productos es algo diferenciador frente al resto del mercado por lo que invertimos mucho en crear productos de calidad que nuestras Pymes respeten y valoren.\nNos damos el tiempo para pensar fuera de la caja y volver con propuestas innovadoras. Estamos aquí para cambiar la industria!\n¿Qué estamos buscando? \nEn Xepelin estamos buscando personas creativas y visionarias que piensen fuera de la caja para sumarse a nuestro equipo. Si te apasiona resolver desafíos interesantes de alto impacto y quieres ser parte de un entorno dinámico que está transformando la industria financiera, ¡Esta oportunidad es para ti!\nEl rol se integrará a nuestro equipo de \nData Platform\n. Si te motiva el desafío de construir soluciones innovadoras en un entorno de rápido cambio, queremos conocerte.\nUnete a nosotros, crezcamos juntos!\nPrincipales responsabilidades...\n-  Diseñar, crear y mantener pipelines de datos\n-  Mantener y optimizar la infraestructura de datos necesaria para una extracción precisa, transformación y carga de datos de una amplia variedad de fuentes de datos\n-  Automatizar los flujos de trabajo de datos, como la ingesta de datos, la agregación y el procesamiento de ETL o ELT\n-  Preparar datos sin procesar en almacenes de datos en un conjunto de datos consumibles para fines técnicos y partes interesadas no técnicas\n-  Crear, mantener e implementar productos de datos para equipos de análisis y ciencia de datos en Plataformas en la nube de GCP\n-  Desarrollo de sistemas y arquitectura que soporte las diferentes etapas del flujo de Machine Learning\n-  Conocimientos en herramientas de infraestructura como código (Terraform preferentemente)\n¿Qué necesitas para brillar?\n- Conocimientos en GCP\n- Conocimientos intermedio/avanzado en Python\n- Conocimiento intermedio/avanzado de SQL\n- Experiencia desplegando aplicaciones en ambientes serverless como Cloud Functions o AWS Lambda\n- Conocimientos administrando y desplegando algún orquestador, por ejemplo: Apache Airflow, Dagster, etc\n- Excelentes habilidades para trabajar en equipo. Ser humilde y saber colaborar. un Team Player!\n- Saber escuchar a tus stakeholders y poder traducir eso en requerimientos y ejecutarlos con tu equipo\n- Conocimientos y manejo de lenguajes de programación y/o frameworks, NodeJS, Golang, por ejemplo\n- Conocimientos y/o experiencia en MLOps\n- Experiencia en la industria Fintech B2B\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- No tener miedo a tomar decisiones y liderar proyectos\n- Foco en impacto e historia consistente entregando resultados para usuarios y el negocio\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- Te sientes cómodo cuestionando el status-quo de los servicios financieros, adaptándose rápidamente a los cambios, y presentando claramente tus ideas y conceptos para debatirlos en equipo\nNuestros Beneficios:\n🌴 Xepelin Balance\nVacaciones:\n 15 días hábiles. Por cada año que cumplas en Xepelin, te damos un día extra de vacaciones.\nBalance days:\n 10 días libres adicionales al año, para disfrutar como quieras.\nTrabajo híbrido y flexibilidad horaria según el rol. Trabajamos por objetivos.\nBeneficios Flexibles: \nPuntos flexibles en tu moneda local al mes para gastar en lo que quieras.\nXepelin Fun:\n Actividades de encuentro financiadas por Xepelin para divertirnos juntos.\n🚀 Xepelin Performance & Career\nPlataformas de capacitación:\n Convenios con las mejores plataformas, como Reforge, Udemy y DataCamp.\nKit de Bienvenida: \ntodo lo que necesitas para comenzar tu viaje en Xepelin 😊\n🤝 Xepelin Cares\nCobertura de salud:\n contamos con convenios de salud con proveedores de calidad o reembolsos según el país donde te encuentres.\nPost Natal:\n te damos una semana extra de licencia post natal. ¡Nos interesa que estés con tu familia y seres queridos!\nMatrimonio plus:\n Lleva tus planes al siguiente nivel, con una gift card y extendiendo tu permiso legal por matrimonio con dos días de regalo por Xepelin.        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "hospitality,_food_and_beverage_services,_and_retail",
         "https://www.linkedin.com/jobs/view/4224174809/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "24",
         "Data Engineer ( Azure Cloud & Python & Databricks)",
         "Option",
         "Santiago Metropolitan Area",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4219824203/",
         "¿Quiénes somos?\nEn Option, creemos en un mundo donde las soluciones tecnológicas no tienen límites. Nuestra misión es transformar los desafíos en oportunidades mediante la creación de soluciones innovadoras que potencien la Aceleración Digital. Nuestro equipo es dinámico, colaborativo y apasionado por la tecnología. Únete a una organización que está redefiniendo cómo el mundo utiliza los datos y la tecnología para resolver problemas complejos.\n¿Qué buscamos?\nEstamos en la búsqueda de un/a Ingeniero/a de Datos para unirse al equipo de Analítica Avanzada. Este rol será clave en el desarrollo de soluciones de analítica avanzada y disponibilización de los resultados de los modelos para consumo de soluciones y clientes.\n¡Te estamos buscando!\n¿Qué te ofrece este puesto?\n- Participación en un proceso estratégico de migración a la nube.\n- Un entorno de trabajo colaborativo y con líderes técnicos accesibles.\n- Uso de tecnologías modernas como Cloud Azure, Databricks, entre otros.\n- Trabajo conjunto con equipos de analítica, desarrollo y operaciones.\n¿Cuáles serán tus principales responsabilidades?\n- Desarrollo de capas de negocio, conceptuales y físicas, a nivel de datos. \n- Asegurar que la solución desarrollada sea escalable a nivel de datos, en términos de reutilización de información y validación de indicadores de cara a la operación.\n¿Qué necesitas para ser nuestro próximo Ingeniero de Datos?\nHabilidades Técnicas Excluyentes\n- Azure Cloud\n- Conocimiento en gestión de proyectos bajo framework ágiles.\n- Conocimiento WKC y otra plataforma equivalente.\n- Python: Numpy, Pandas, Sklearn, Pyspark.\n- SQL\n- Spark\n- Databricks: Intermedio/Avanzado.\nHabilidades Técnicas Deseables\n- Certificaciones en Azure Cloud\n- Ubicación: LATAM Modalidad de trabajo: 100% Remoto\n¡Únete a nuestro equipo y transforma el futuro con nosotros!\nhttps://www.option.tech\n                \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4219824203/",
         "Medium",
         "Mid-Senior",
         "No",
         "Yes",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Spark knowledge' 'Collaboration with data scientists or analysts'\n 'Migration']"
        ],
        [
         "25",
         "Data Engineer (GCP & DataFlow & Bigquery)",
         "Option",
         "Santiago Metropolitan Area",
         "2025-04-12 00:00:00",
         "https://www.linkedin.com/jobs/view/4208792219/",
         "¿Quiénes somos?\nEn \nOption\n, creemos en un mundo donde las soluciones tecnológicas no tienen límites. Nuestra misión es transformar los desafíos en oportunidades mediante la creación de soluciones innovadoras que potencien la Aceleración Digital. Nuestro equipo es dinámico, colaborativo y apasionado por la tecnología. Únete a una organización que está redefiniendo cómo el mundo utiliza los datos y la tecnología para resolver problemas complejos.\n¿Qué buscamos?\nEstamos en la búsqueda de un/a \nIngeniero/a de Datos\n para unirse al equipo de Data Services. Este rol será clave en el levantamiento, análisis y migración de procesos ETL desde un Data Lake mal gobernado hacia una arquitectura moderna sobre Google Cloud Platform (GCP). ¡Te estamos buscando!\n¿Qué te ofrece este puesto?\n- Participación en un proceso estratégico de migración a la nube.\n- Un entorno de trabajo colaborativo y con líderes técnicos accesibles.\n- Uso de tecnologías modernas como GCP, Dataflow y BigQuery.\n- Trabajo conjunto con equipos de analítica, desarrollo y operación.\n¿Cuáles serán tus principales responsabilidades?\n- Levantar y documentar los ETLs actuales en Data Services.\n- Analizar el ambiente de datos y planificar su migración a GCP.\n- Tomar iniciativa en la migración de ETLs críticos.\n- Resolver incidencias relacionadas a ETLs mediante la mesa de ayuda.\n- Participar en el diseño del plan de migración.\n- Colaborar con los líderes técnicos y equipos multidisciplinarios.\n¿Qué necesitas para ser nuestro próximo Ingeniero de Datos?\nHabilidades Técnicas Excluyentes\n- Oracle\n- Python\n- Dataflow (GCP)\n- Dataform (GCP)\n- GitLab\n- BigQuery (GCP)\n- Data Modeling\n- Composer (GCP)\nHabilidades Técnicas Deseables\n- Oracle Data Integrator (ODI)\nUbicación: LATAM\nModalidad de trabajo:\n 100% Remoto\n¡Únete a nuestro equipo y transforma el futuro con nosotros!\nhttps://www.option.tech        \n            \n                            \n            ",
         "Entry level",
         "Full-time",
         "Information Technology",
         "Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4208792219/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "26",
         "Data Engineer - AWS Ssr",
         "axity",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4204874848/",
         "Company Description: axity\r\nJob Description: Axity una compañía con más de 35 años de trayectoria nuestro portafolio de servicios es uno de los más grandes en la industria: Estrategia Digital, Desarrollo de Software, Bussiness Intelligence, Big Data, Analítica Avanzada, Seguridad, IOT.\r\nResponsabilidades Principales\r\n-  Diseño eficiente de almacenamiento de datos: utilizando servicios como Amazon S3, DynamoDB, RDS, entre otros, optimizando costos, rendimiento y accesibilidad.\r\n-  Optimización de consultas: aplicando índices, claves de partición y patrones de acceso eficientes (Query, GetItem, etc.).\r\n-  Integración y procesamiento de datos: a través de herramientas como AWS Glue, Amazon Kinesis y/o Firehose para ingesta, transformación y orquestación.\r\n-  Uso de formatos optimizados: como Parquet, ORC, entre otros, para mejorar la compresión y velocidad de lectura/escritura.\r\n-  Infraestructura como Código (IaC): despliegue de infraestructura en AWS mediante CloudFormation, AWS CDK o Terraform.\r\n________________________________________\r\n️ Conocimientos Técnicos Clave\r\n-  Amplio manejo de servicios AWS: S3, DynamoDB, RDS, AWS Glue, Kinesis/Firehose.\r\n-  Dominio de bases de datos SQL/NoSQL: diseño de esquemas, indexación y tuning.\r\n-  Experiencia en optimización y particionamiento de grandes volúmenes de datos.\r\n-  Familiaridad con automatización y orquestación de pipelines: scripting, Jenkins, Shell, entre otros.\r\n-  Modalidad: Híbrido\r\n-  Área: Tecnología – Cloud\r\n-  Horario: Lunes a viernes de 9:00 a 18:00 hrs\r\n-  Tipo de Contrato: Plazo fijo, con posibilidad de pasar a indefinido\r\n________________________________________\r\n¿Por qué postular?\r\n-  Serás parte de una empresa con enfoque en la innovación y la transformación digital.\r\n-  Trabajarás con tecnologías de vanguardia en un entorno colaborativo.\r\n-  Tendrás oportunidades reales de crecimiento profesional.        \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4204874848/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Data quality'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)' 'Migration']"
        ],
        [
         "27",
         "Data Engineer - Databricks - Mid Level",
         "Lumenalta",
         "Chile",
         "2025-04-13 00:00:00",
         "https://www.linkedin.com/jobs/view/4209427076/",
         "Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n- 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\n- You are comfortable manipulating large data sets and handling raw SQL\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\n- Experience creating ETL Pipeline from scratch\n- E-commerce and Financial Services industry experience preferred\n- English fluency, verbal and written\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\n- Be a part of a team of talented and friendly senior-level developers.\n- Work on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nWhat's it like to work at Lumenalta?        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Engineering, Information Technology, and Consulting",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4209427076/",
         "Medium",
         "Senior",
         "No",
         "No",
         "3",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge']"
        ],
        [
         "28",
         "Data Engineer - Databricks - Mid Level",
         "Lumenalta",
         "Chile",
         "2025-05-11 00:00:00",
         "https://www.linkedin.com/jobs/view/4228057809/",
         "Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n- 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\n- You are comfortable manipulating large data sets and handling raw SQL\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\n- Experience creating ETL Pipeline from scratch\n- E-commerce and Financial Services industry experience preferred\n- English fluency, verbal and written\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\n- Be a part of a team of talented and friendly senior-level developers.\n- Work on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nWhat's it like to work at Lumenalta?        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_consulting",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4228057809/",
         "Medium",
         "Senior",
         "No",
         "No",
         "3",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "29",
         "Data Engineer - Databricks - Senior",
         "Lumenalta",
         "Chile",
         "2025-04-13 00:00:00",
         "https://www.linkedin.com/jobs/view/4209434250/",
         "Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n- 7+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\n- You are comfortable manipulating large data sets and handling raw SQL\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\n- Experience creating ETL Pipeline from scratch\n- E-commerce and Financial Services industry experience preferred\n- English fluency, verbal and written\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\n- Be a part of a team of talented and friendly senior-level developers.\n- Work on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nWhat's it like to work at Lumenalta?        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Engineering, Information Technology, and Consulting",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4209434250/",
         "Medium",
         "Senior",
         "No",
         "No",
         "7+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge'\n 'Version control (GIT or similar)']"
        ],
        [
         "30",
         "Data Engineer - GCP Ssr",
         "axity",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4204875774/",
         "Company Description: axity\r\nJob Description: Axity una compañía con más de 35 años de trayectoria nuestro portafolio de servicios es uno de los más grandes en la industria: Estrategia Digital, Desarrollo de Software, Bussiness Intelligence, Big Data, Analítica Avanzada, Seguridad, IOT.\r\nBuscamos Data Analyst / Data Engineer – Nivel Medio/Avanzado\r\n¿Te apasiona convertir datos en información valiosa y accionable?\r\nEstamos en búsqueda de un profesional con experiencia en desarrollo de productos de complejidad media a avanzada, capaz de entregar soluciones de calidad dentro de los plazos establecidos.\r\nResponsabilidades\r\nDesarrollar productos analíticos cumpliendo con estándares de calidad y tiempos definidos.\r\nTraducir datos en información útil para la toma de decisiones.\r\nTrabajar en conjunto con equipos de analítica para profundizar en el análisis y la síntesis de datos.\r\nSeleccionar y aplicar técnicas analíticas adecuadas a cada requerimiento.\r\nMantenerse actualizado sobre herramientas analíticas y productos de manipulación de datos.\r\nRealizar procesos de recolección, clasificación, limpieza e interpretación de grandes volúmenes de datos.\r\nAplicar prácticas de gobernanza y seguridad de los datos.\r\nParticipar en iniciativas que involucren integración de datos para procesos como planeación de compras, demanda y gestión de inventarios.\r\nRequisitos\r\nExperiencia en GCP (Google Cloud Platform): BigQuery, Cloud Storage, Dataflow, Cloud Functions, Composer, Pub/Sub.\r\nConocimientos básicos en Kubernetes, contenedores y consumo de APIs.\r\nExperiencia en manejo de grandes volúmenes de datos.\r\nConocimiento de sistemas legados, flujos de compras y demand forecasting.\r\nCapacidad de interacción continua con áreas de negocio.\r\nOfrecemos\r\nHorario: Lunes a viernes de 9:00 a 18:30\r\nContrato a plazo fijo luego a indefinido\r\nOportunidad de trabajar en proyectos desafiantes con impacto directo en el negocio\r\nAmbiente colaborativo e innovador\r\nSi te motiva trabajar con datos, impactar decisiones estratégicas y enfrentarte a desafíos técnicos, ¡postula con nosotros!\r\n                \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4204875774/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Data governance' 'APIs' 'Collaboration with data scientists or analysts']"
        ],
        [
         "31",
         "Data Engineer - Inglés Avanzado",
         "Deloitte",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4221350232/",
         "¿Buscas generar impactos significativos con tu trabajo? ¿Te interesa la innovación y desarrollar tus habilidades de liderazgo?\nEn Deloitte, encontrarás un espacio intelectualmente desafiante, diverso, inclusivo, y comprometido con su entorno, que potenciará el camino profesional que hoy desees emprender.\nCon un equipo multidisciplinario y una red global de profesionales expertos que comparten las mejores prácticas y experiencias del mercado, Deloitte se destaca por la entrega de un servicio de excelencia, conectado con las últimas tendencias, que ofrece a sus colaboradores/as un espacio de aprendizaje único donde se incentiva el liderazgo en todo nivel.\nNos encontramos en búsqueda de las y los mejores profesionales que quieran integrar nuestra práctica de \nArtificial Intelligence & Data (AI & Data).\nNuestro equipo de AI & Data utiliza el valor de los datos, la analítica, la robótica, la ciencia de datos y las tecnologías cognitivas para generar conocimiento y apoyar organizaciones en su jornada de evolución a AI & Insight Driven.\nDentro de las principales funciones como data engineer se destacan:\n- Responsable de la ingesta y la curación de datos.\n- Creación de ETL y canalizaciones asociadas aprovechando Azure Data Factory y Databricks.\n- Trabajar con Synapse y Python durante todo el proceso.\n- Tener la posibilidad de participar de proyectos nacionales e internacionales con equipos globales inglés-parlantes.\nHabilidades requeridas:\n- Fuerte comprensión de conceptos de bases de datos y modelado de datos.\n- Excelencia y Calidad.\n- Orientación al cliente interno/externo.\n- Trabajo en equipo.\n- Iniciativa y Proactividad.\n- Aprendizaje continuo\nExperiencia requerida:\n- Profesional titulado área informática, industrial, estadística o matemática.\n- Al menos 3 o más años de experiencia práctica en los requisitos expuestos.\n- Inglés nivel avanzado (Excluyente).\n- Experiencia comprobada en Databricks, Azure Data Factory, Python y PySpark (Excluyente)\n- Deseable Synapse\n- Deseable Certificaciones de Data Engineer en algunas de las principales nubes (GCP, AWS, Azure), Snowflake, Databricks, entre otros.\nAlgunos de nuestros beneficios…\n- Jornada de trabajo hibrida.\n- Dia de cumpleaños libre.\n- Dia libre por mudanza.\n- Actividades de voluntariado.\n- Permiso sin goce de sueldo “Cumple tus sueños” (año sabático).\n- Deloitte Days (5 días libres al año).\n- Cuida de los tuyos, licencias para el cuidado de tus familiares.\n- Equipos deportivos\n¿Qué impacto quieres hacer? \nNuestras ofertas laborales están abiertas a todos quienes, dentro del marco de la Ley de Inclusión, quieran formar parte de esta gran Firma, aportando con sus distintas capacidades y fortalezas, tanto humanas como profesionales.\n*Deloitte no efectúa ningún tipo de cobro por participar en los procesos de reclutamiento y selección y todas nuestras comunicaciones siempre son realizadas desde correos con dominio Deloitte”.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "consulting,_information_technology,_and_project_management",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4221350232/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "Yes",
         "3",
         "Yes",
         "Azure",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Spark knowledge']"
        ],
        [
         "32",
         "Data Engineer - LATAM (100% Remoto)",
         "Imagemaker",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4219332744/",
         "Company Description: Imagemaker\nJob Description: ¿Estás en búsqueda de nuevos desafíos? Únete a Imagemaker y sigue creciendo con nosotros como Senior Data Engineer en uno de nuestros grandes clientes.\nSerá responsable de diseñar, construir, optimizar y mantener la infraestructura de datos, asegurando el flujo eficiente de información desde diversas fuentes internas y externas.\nTrabajará con grandes volúmenes de datos, integrando y procesando información crítica para apoyar la toma de decisiones y mejorar la eficiencia operativa y la experiencia del cliente.\nLa vacante es en modalidad remota, probablemente se pida ir una vez al mes a reunión pero, en su mayoría es remota.\nBuscamos seniority tanto Semi Senior como Senior. No te pierdas esta gran oportunidad\nKey Responsibilities\nDiseño y construcción de tuberías de datos.\nOptimización de la infraestructura de datos.\nGestión de datos en tiempo real.\nProveer acceso a datos de calidad para apoyar iniciativas de análisis predictivo, inteligencia artificial y machine learning.\nImplementar mecanismos de seguridad, calidad y trazabilidad de datos.\nSkills, knowledge & expertise\nExperiencia en GCP (excluyente)\nExperiencia en Python\nExperiencia en POO\nExperiencia en SQL (optimización y consultas avanzadas)\nExperiencia en modelamiento, integración y procesamiento de datos.\nExperiencia en BigQuery\nExperiencia en ETL\nExperiencia en Apache Airflow\nExperiencia en Dataflow\nExperiencia en Git\nConocimientos en Terraform\nConocimientos en Jenkins\nConocimientos en CI/CD\nJob benefits\nSer maker es cool: tenemos muy buenos beneficios y muchas actividades para divertirnos!\n️ Don’t worry, be happy: 3 días libres al año adicionales a tus vacaciones.\nPermiso sin goce de sueldo para cumplir tus sueños.\nPrograma de bienestar enfocado a equilibrar el trabajo y la vida personal.\nSeguro Complementario 100% gratuito para makers.\n¡Programas de formación, clases de inglés y mucho más!\nDía libre para tu cumpleaños y medio día para los cumpleaños de tus hijos.\nBonificaciones que dan respiros: fiestas patrias, navidad, matrimonio/AUC, nacimiento/adopción de hijos, etc.\nConvenios y precios preferenciales con bancos.\n3 Días adicionales para padres por nacimiento o adopción de hijo/a.\nConvenio de seguro para tus mascotas!\nNuestra cultura es horizontal, de innovación, desafiante y sobre todo, se respira mucha buena onda!\nEn el marco de nuestro compromiso con la inclusión, la siguiente vacante está abierta en el marco de la ley 21.015, te instamos a postular enviando tu CV\nLink de postulación: https://imagemaker.pinpointhq.com/postings/ef52a091-247a-4b18-8e7c-72568cda3892\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4219332744/",
         "High",
         "Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Data quality'\n 'CI/CD' 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "33",
         "Data Engineer - Snowflake - Mid Level",
         "Lumenalta",
         "Chile",
         "2025-04-14 00:00:00",
         "https://www.linkedin.com/jobs/view/4209447525/",
         "Experience Remote done Right. With over 20 years of remote experience, our 500+ team members are 100% remote, and we continue to cultivate vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions, and data points. The challenges we solve daily are real and require creativity, grit, and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of the problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media, or other complex multifactor industries.\nRequirements\n- 3+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises\n- You are comfortable manipulating large data sets and handling raw SQL\n- Experience using technologies such as Snowflake, AWS, and ETL pipelines is essential.\n- Have extensive experience with data warehousing and working with scalability of large volumes of structured data\n- Financial Services industry experience preferred\n- English fluency, verbal and written\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\n- Be a part of a team of talented and friendly senior-level developers.\n- Work on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nWhat's it like to work at Lumenalta?        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Engineering, Information Technology, and Consulting",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4209447525/",
         "Medium",
         "Senior",
         "No",
         "No",
         "3",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Databricks or snowflake']"
        ],
        [
         "34",
         "Data Engineer - Trainee",
         "Soluciones - Data & Analytics Consulting",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226385320/",
         "✔️\n¿Quiénes Somos?\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente.\nA través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros\n✔️ Responsabilidades del Cargo:\n- Mantener y generar nuevos Desarrollos sobre SQL Server y Pentaho.\n✔️ Requisitos:\n- Ingeniero (a) en informática o afín\n- Haber realizado tu practica en el área de datos \n- Disponibilidad para trabajar en modalidad Hibrida ( 3 días presencial 2 remoto)\n- Lugar de trabajo Las Condes.\n✔️ Conocimientos Obligatorios:\n- SQL\n- PL/SQL\n- ETL\n✔️ Conocimientos Deseables:\n- Pentaho\n✔️ Ofrecemos\n- Seguros complementario de salud\n- Rutas de estudios\n- Día libre cumpleaños\n- Reajuste salarial anual según variación del IPC        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology_and_consulting",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4226385320/",
         "Medium",
         "Junior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes']"
        ],
        [
         "35",
         "Data Engineer AWS",
         "BC Tecnología",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205294910/",
         "En \r\nBC Tecnología\r\n, nos especializamos en la consultoría de TI, ofreciendo un amplio rango de servicios para adaptarnos a las necesidades específicas de nuestros clientes, principalmente en finanzas, seguros, retail y gobierno. Nuestro equipo trabaja mediante metodologías ágiles, lo que nos permite diseñar e implementar soluciones tecnológicas efectivas y dirigidas al cliente. Actualmente, estamos buscando un Data Engineer con experiencia en AWS para sumarse a nuestro equipo y contribuir a proyectos innovadores en la gestión y explotación de datos.\r\n Responsabilidades del Cargo\r\n- Desarrollar y gestionar procesos de ETL, asegurando la calidad y fiabilidad de los datos.\r\n- Optimizar la explotación de datos a través de técnicas de Tuning.\r\n- Implementar soluciones utilizando herramientas de AWS, incluyendo Glue, Lambda, S3, Redshift y DynamoDB.\r\n- Colaborar con los equipos de desarrollo de software para asegurar la integración de datos eficiente.\r\n- Realizar análisis y visualización de datos para apoyar en la toma de decisiones.\r\n- Mantener un enfoque en la innovación y la mejora continua de los procesos y herramientas utilizadas.\r\n Descripción del Cargo\r\nBuscamos Un Data Engineer AWS Con Un Mínimo De 3 Años De Experiencia En El Manejo De Datos. El Candidato Ideal Tendrá Conocimientos Sólidos En\r\n- Explotación de datos y Tuning.\r\n- Diseño e implementación de procesos ETL.\r\n- Desarrollo de consultas efectivas en SQL.\r\n- Programación en Python.\r\n- Uso de herramientas de orquestación como Apache Airflow (deseable).\r\nValoramos habilidades como el trabajo en equipo, la proactividad y la capacidad para adaptarse a nuevas tecnologías. La combinación de habilidades técnicas y soft skills es esencial para unirse a nuestro equipo dinámico.\r\n Habilidades Deseables\r\nAdemás de los requisitos mencionados, sería beneficioso contar con experiencia en:\r\n- Integraciones y gestión de datos en múltiples fuentes.\r\n- Implementación de soluciones en la nube de AWS.\r\n- Conocimientos en herramientas de visualización de datos.\r\nEstas habilidades ayudarán al candidato a integrarse de manera efectiva en nuestros equipos de trabajo y contribuir a proyectos futuros.\r\n Beneficios de Trabajar con Nosotros\r\nEn \r\nBC Tecnología\r\n, valoramos a nuestro equipo y ofrecemos un entorno flexible y beneficios atractivos:\r\n- Contrato indefinido.\r\n- Modalidad híbrida, combinando trabajo remoto y en oficina.\r\n- Paquete de beneficios corporativos que incluye salud prepaga de primer nivel para el empleado y su familia.\r\n- Un día de home office a la semana, junto con desayunos y un comedor en la planta.\r\n- Acceso a un Sport Club y asistencia de un nutricionista.\r\n¡Únete a nosotros y marca una diferencia en el mundo de la tecnología! 🎉\r\n                \r\n            \r\n                            \r\n            ",
         "Not Applicable",
         "Full-time",
         "Information Technology",
         "Technology, Information and Internet and Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4205294910/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "3",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data quality']"
        ],
        [
         "36",
         "Data Engineer AWS",
         "Soluciones - Data & Analytics Consulting",
         "Santiago Metropolitan Region, Chile",
         "2025-04-11 00:00:00",
         "https://www.linkedin.com/jobs/view/4207848749/",
         "✔️\r\n¿Quiénes Somos?\r\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente. A través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros.\r\n✔️\r\n ¿Qué harás?\r\n- Responsabilidades del cargo: Diseñar, construir y mantener procesos de ingesta de datos que permiten recolectar, almacenar, procesar y acceder a grandes volúmenes de datos de manera eficiente y segura. Su trabajo asegura que los datos estén disponibles, limpios y organizados para su análisis adhiriéndose a los estándares definidos por el cliente.\r\n✔️\r\n ¿Qué se requiere?\r\n- Experiencia de al menos 3 años en el rol\r\n- Conocimientos técnicos excluyentes: S3, Lambdas, Glue Jobs, DynamoDB, Redshift, StepFunctions, Python, SQS.\r\n- Conocimientos técnicos deseables: API Gateway, Transfer Family.\r\n✔️ ¿Qué Ofrecemos ?\r\n- Seguros complementario de salud\r\n- Rutas de estudios\r\n- Día libre cumpleaños\r\n- Reajuste salarial anual según variación del IPC        \r\n            \r\n                            \r\n            ",
         "Not Applicable",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4207848749/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "3",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'APIs']"
        ],
        [
         "37",
         "Data Engineer AWS Senior",
         "Soluciones - Data & Analytics Consulting",
         "Santiago Metropolitan Region, Chile",
         "2025-05-16 00:00:00",
         "https://www.linkedin.com/jobs/view/4231766325/",
         "✔️\n¿Quiénes Somos?\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente. A través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros.\n✔️\n ¿Qué harás?\n- Responsabilidades del cargo: Diseñar, construir y mantener procesos de ingesta de datos que permiten recolectar, almacenar, procesar y acceder a grandes volúmenes de datos de manera eficiente y segura. Su trabajo asegura que los datos estén disponibles, limpios y organizados para su análisis adhiriéndose a los estándares definidos por el cliente.\n✔️\n ¿Qué se requiere?\n- Experiencia de al menos 4 años en el rol\n- Conocimientos técnicos excluyentes: S3, Lambdas, Glue (Spark ETLs, Crawlers y Data Catalog), DynamoDB, Python, Terraform, Apache Spark, Athena, Step Function.\n- Conocimientos técnicos deseables: API Gateway, Kinesis (Stream y Firehose, tener nociones al menos), SQS(tener nociones), SNS, CloudWatch (deseable monitoreo y logs insights), AWS Load.\n✔️ ¿Qué Ofrecemos ?\n- Seguros complementario de salud\n- Rutas de estudios\n- Día libre cumpleaños\n- Reajuste salarial anual según variación del IPC        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4231766325/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "4",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge'\n 'Data monitoring']"
        ],
        [
         "38",
         "Data Engineer AWS – Contrato Indefinido.",
         "BC Tecnología",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4204878459/",
         "Company Description: BC Tecnología\r\nJob Description: Experiencia de 3 años en:\r\n-  Explotación de datos (Tunnig)\r\n-  ETL\r\n-  SQL\r\n-  Python\r\n-  Apache Airflow (Deseable)\r\n-  AWS (Glue, Lambda, S3, Redshift, Dynamodb\r\n-  Trabajo Hibrido\r\nInteresados o referidos favor enviar cv actualizado a [email] indicando en asunto de e-mail cargo al cual postula (Data Engineer AWS)\r\n                \r\n            \r\n                            \r\n            ",
         "Mid-Senior level",
         "Full-time",
         "Information Technology",
         "Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4204878459/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "3",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'None of the above mentioned skills']"
        ],
        [
         "39",
         "Data Engineer Azure",
         "",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4206654598/",
         "Resumen del Cargo:\r\nBusco Ingeniero de Datos  para liderar y ejecutar proyectos de Data & Analytics. \r\nSerá responsable del diseño, desarrollo e implementación de soluciones de datos usando Azure Databricks, Data Factory, Python  y otros servicios en la nube, asegurando que la infraestructura de datos sea escalable, segura y optimizada para la toma de decisiones. \r\nAdemás, deberá interactuar directamente con áreas de negocio para levantar requerimientos y traducirlos en soluciones técnicas eficientes.\r\n \r\n \r\nResponsabilidades:\r\n        •       Diseñar y desarrollar soluciones de ingestión, transformación y modelado de datos en Azure DataFactory, Databricks y otras tecnologías relacionadas.\r\n        •       Optimizar procesos de ETL/ELT, asegurando calidad, integridad y eficiencia en el procesamiento de datos.\r\n        •       Colaborar con equipos de negocio para entender necesidades, identificar oportunidades y proponer soluciones basadas en datos.\r\n        •       Implementar arquitecturas de datos escalables que soporten analítica avanzada, machine learning e inteligencia de negocio.\r\n        •       Garantizar la seguridad y el cumplimiento normativo en el manejo de datos, siguiendo estándares bancarios y regulatorios.\r\n        •       Desarrollar y optimizar pipelines de datos para la explotación en entornos analíticos y de reportes.\r\n        •       Documentar procesos, arquitecturas y modelos de datos, asegurando buenas prácticas de gobernanza.\r\n        •       Trabajar en conjunto con equipos de Data Science, BI y Tecnología para garantizar la disponibilidad y calidad de los datos.\r\n         \r\n            \r\n                            \r\n            ",
         "Entry level",
         "Full-time",
         "Information Technology",
         null,
         "https://www.linkedin.com/jobs/view/4206654598/",
         "High",
         "Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "Azure",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Data governance'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "40",
         "Data Engineer Azure DataBricks",
         "Accenture Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4157214213/",
         "WORK AT THE HEART OF THE CHANGE\nEmprende una Carrera que te inspire, súmate a nuestra área de \nAccenture Technology \ny sé parte del cambio!\n“En Accenture vivimos el futuro hoy, porque no solo aplicamos tecnología, la creamos. Aquí construimos el futuro”. \nCombinemos tu ingenio con las últimas tecnologías y sé parte del impacto positivo que puedes generar.\nTe invitamos a abrir las puertas del mundo digital donde podrás desarrollar tu talento para crecer, aprender y certificarte en las tecnologías más avanzadas, proporcionar innovación continua, ágil y participar de nuevos negocios.\nCada día, en todo el mundo, nuestros equipos innovan para crear un cambio significativo. Sé parte de Accenture!\nNos encontramos en búsqueda de profesionales para el role de \nData Engineer.\nSé parte de nuestro equipo dinámico mientras nos embarcamos en un viaje para convertirnos en expertos en Ingeniería de Datos. Ayudarás a darle forma al futuro colaborando, gestionando e interactuando con varios equipos para proporcionar soluciones innovadoras y contribuir a las decisiones clave.\n ¡Vamos a crear juntos un mundo basado en datos!\n¿Qué te hará tener éxito? 🚀\nEntre 1 a 3 años de experiencia en Data Engineering.\nExperiencia en DataBricks, DataLake, Data Factory, Azure DevOps.\nLenguajes de programación y frameworks: Python, SQL.\nSoftskills: habilidades de comunicación para trabajar de cara a cliente.\n¿Por qué elegir Accenture? \nUn lugar de trabajo único, descubre algunos de los beneficios que tenemos para ti\n:\n💪 Desarrollo de carrera\n💯 Jornadas Flex\n📚 +40 mil capacitaciones y cursos disponibles (online y presencial)\n📒Bibliotecas, libros y podcasts\n🗣️Programa de idiomas\n⭐ ¡Certificaciones gratuitas mediante nuestros partners! +900 certificados en Chile\n🤖GenAI Academy, con programas exclusivos para Accenture.\n⭐⭐Reconocidos por Great Place To Work Chile 2023 en el puesto #10 entre las mejores compañías para trabajar de más de 1.000 colaboradores. ¡Sí, estamos en el Top 10 de Chile!\n🎉 ¡Experiencia de onboarding global! +6.000 personas recibidas en el Metaverso a nivel LATAM\n🏆 Bonos y aguinaldos\n👩‍⚕️ Seguro complementario de salud (sin deducible ni copago)\n🎂 Día de cumpleaños libre\n👨‍👩Licencias de Paternidad & Maternidad Extendida\n🌎 Red global de conocimiento\n🌈Elegida la compañía más diversa e inclusiva del mundo, según el Índice de Diversidad e Inclusión de Refinitiv\n♻️Sostenibilidad, un motor de cambio, conoce nuestro compromiso\nSobre nosotros: \n 733K colaboradores a nivel global.\n 9K clientes en 120 países\n + de 1.900 Talentos en Chile.\n Proyectos en diversas industrias        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4157214213/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "1",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes'\n 'Version control (GIT or similar)']"
        ],
        [
         "41",
         "Data Engineer Databricks",
         "Tata Consultancy Services",
         "Santiago Metropolitan Region, Chile",
         "2025-04-16 00:00:00",
         "https://www.linkedin.com/jobs/view/4211760013/",
         "En \nTata Consultancy Services\n (www.tcs.com) estamos en búsqueda de talentos como tú, somos una empresa global con presencia en más de 55 países y 600.000+ asociados. En Latinoamérica estamos en 9 países (Chile, Argentina, México, Uruguay, Colombia, Guatemala, Perú, Ecuador, Brasil) y ya somos más de 26.000 asociados que hacen parte de esta gran familia.\nHoy en \nTCSChile\n queremos invitarte a que te unas a nosotros y transformemos el mundo a través de la tecnología, siendo parte de nuestro equipo como\n Data Engineer\n, especialista \nDatabricks \ncon experiencia en migraciones desde Cloudera a Azure, para brindar servicios a cliente multinacional, líder global en el mercado de Servicios financieros y así ser parte del equipo responsable del análisis, implementación de migraciones, desarrollo de modelos y base de datos.\nExperiencia\n:\n• Experiencia en proyecto de Migraciones de Base de Datos y Modelos.\n• Mínimo 4 años en desarrollo y programación en ámbitos de Data Engineering | Data Science.\n• Experiencia en gestión y/o desarrollo de proyectos en sector de Banca, Seguros, y/o Servicios Financieros.\n• Contar con conocimientos en metodologías Agile y Tradicional. (Scrum, Kanban, Waterfall).\n• Especialista en : Azure Databricks, Datafactory, Python, PySpark, Cloudera y SQL.\n• Conocimientos: Spark, Scala, Kafka, MongoDB, Cloudera, Streaming, SAS, SAP ERP, Jira, Confluence, Event Hubs, Control M, Git, Jenkins.\n• Altamente deseable que cuentes con alguna certificación.\nModalidad híbrida en Santiago, Chile.\nSomos Certificados cómo Top Employer en Chile y Latinoamérica🥇. Como #\nTCSERS \ncontamos con beneficios como seguro complementario de salud y vida con cobertura en atención Psicológica, Kinesiología, Psicopedagogía y Fonoaudiología 🐍, convenios ópticos y dentales🍭, asesoría nutricional 🥗, deportiva 🏃‍♀️ y del sueño 🌛, portales de capacitación, desarrollo de plan de carrera y reembolso en certificaciones para potenciar tu desarrollo profesional 🎓, día libre por cumpleaños 🍰, excelentes herramientas de trabajo ¡y mucho más!\nSi estás interesado envía tu CV actualizado, con disponibilidad y pretensiones de renta al correo \nf.pinoserey@tcs.com\n“Tata Consultancy Services es un empleador que ofrece igualdad de oportunidades, nuestro compromiso con la diversidad y la inclusión impulsa nuestros esfuerzos para brindar igualdad de oportunidades a todos los candidatos que satisfacen nuestras necesidades de conocimientos y competencias requeridas, independientemente de cualquier origen socioeconómico, raza, color, origen nacional, religión, sexo, identidad de género, edad, estado civil, discapacidad, orientación sexual o cualesquiera otros.\n \nEn particular, TCS Chile promueve la contratación de personas con discapacidad de acuerdo con la Ley N°20.422, contamos con un proceso de reclutamiento acorde y alentamos a cualquier persona interesada en construir una carrera en TCS a participar en nuestros procesos de reclutamiento y selección”.        \n            \n                            \n            ",
         "Mid-Senior level",
         "Full-time",
         "Project Management and Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4211760013/",
         "Medium",
         "Mid-Senior",
         "No",
         "Yes",
         "4",
         "No",
         "Azure",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Version control (GIT or similar)' 'Migration'\n 'Spark knowledge']"
        ],
        [
         "42",
         "Data Engineer GCP",
         "Soluciones - Data & Analytics Consulting",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4206574777/",
         "✔️\r\n¿Quiénes Somos?\r\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente. A través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros.\r\n✔️\r\n ¿Qué harás?\r\n- Integración de productos de datos.\r\n- Se trabajará con información para conocer a clientes y segmentarlos, para innovar en productos.\r\n- Trabajará los procesos ETL de inicio a fin (ingesta, transformación, disponibilización).\r\n- Conocimientos Full GCP (airflow, bigquery, cloudstorage como herramientas principales)\r\n- Deberá generar el flujo completo del dato desde la ingesta, transformación y disponibilización.\r\n✔️\r\n ¿Qué se requiere?\r\n- Experiencia en el rol o cargo de Ingeniero de Datos Google Cloud Platform (GCP)\r\n✔️\r\nConocimientos técnicos excluyentes:\r\n- Experiencia en datos y modelo de datos\r\n- Metodología agile\r\n- Procesos ETL\r\n- Conocimientos en Suit GCP\r\n✔️ ¿Qué Ofrecemos ?\r\n- Seguros complementario de salud\r\n- Rutas de estudios\r\n- Día libre cumpleaños\r\n- Reajuste salarial anual según variación del IPC        \r\n            \r\n                            \r\n            ",
         "Associate",
         "Full-time",
         "Information Technology",
         "IT Services and IT Consulting",
         "https://www.linkedin.com/jobs/view/4206574777/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)']"
        ],
        [
         "43",
         "Data Engineer GCP",
         "Soluciones - Data & Analytics Consulting",
         "Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4219126754/",
         ".✔️\n¿Quiénes Somos?\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente. A través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros.\n \n✔️¿Qué harás ?\n- Migración de artefactos (procedure, view) SQL Netezza a GCP BigQuery\n- Migración de datos históricos\n- Creación de Flujos con GCP Dataform\n- Pruebas unitarias, Pruebas funcionales, cuadraturas de datos origen v/s destino\n- Versionamiento de artefactos\n- Despliegue productivo (CI/CD)\n- Análisis de datos\n- Optimización de queries\n \n✔️¿Qué se requiere ?\n- Conocer la consola de GCP y sus servicios (GCP BigQuery y DataForm).\n- Entendimiento de Modelos de Base de Datos Relacional y Dimensional\n- Integración de Datos y Analítica de Datos\n- Ejecución de pruebas unitarias y funcionales.\n- Resolución de problemas de aplicaciones, bases de datos y rendimiento del sistema.\n- Capacidad para realizar evaluaciones comparativas del rendimiento de la base de datos, análisis de conducta y análisis de causa raíz.\n- Capacidad para analizar planes de ejecución y realizar mejoras a objetos de base de datos.\n- Conocimientos de desarrollo PL/SQL (DDL/DML)\n- Versionamiento con Git\n- Python\n✔️ Se valora tu experiencia en \n- Base de datos Netezza o Teradata\n- Cloud Composer\n- Ctrl-M\n- Metodologías de Desarrollo de Proyectos Agiles        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting_and_information_services",
         "https://www.linkedin.com/jobs/view/4219126754/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'CI/CD' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "44",
         "Data Engineer Jr",
         "SII Group Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-05 00:00:00",
         "https://www.linkedin.com/jobs/view/4223983495/",
         "🚀 ¡En SII Group estamos en búsqueda de un/a Data Engineer! 💻✨\nMisión🎯\nBuscamos un Data Engineer con experiencia en proyectos de Business Intelligence, Big Data y Cloud Computing, para desarrollar y gestionar soluciones de datos en la nube.\n¿Cuáles son los requisitos?\n 🛠️\n- Experiencia en Python, SQL y herramientas de Cloud Computing (Azure)\n- Conocimientos en Linux, Shell Scripting, Git, GitLab y CI/CD.\n- Dominio de herramientas de procesamiento de datos: Pandas, SQL, Spark, Apache Beam, Hadoop, DataFlow, Big Query.\n- Experiencia en orquestación de procesos con Airflow.\n- Conocimientos en IaC (Terraform) y herramientas de BI como Tableau o Looker Studio.\n¿Qué harás?\n 💡\n- Desarrollo e implementación de soluciones para procesamiento y análisis de datos.\n- Gestión de datos en plataformas en la nube.\n- Optimización de procesos y flujos de trabajo.\n- Creación de informes y dashboards de BI.\nModalidad\n- Hibrido, 2 veces presencial en las oficinas ubicadas en Las Condes.\nAcerca de SII Group\nSII Group es un proveedor global de servicios TI con más de 16,000 profesionales. Nuestra misión es acompañar a nuestros clientes en su viaje digital, con un enfoque en la innovación y el desarrollo profesional. ¡Somos una de las mejores empresas para trabajar! 🌟\n“Está vacante está disponible para personas con discapacidad de acuerdo a la Ley 21.015, por lo que, si presentas una discapacidad y estás interesado/a en la posición, te agradeceremos nos indiques las adecuaciones necesarias para que tu proceso de entrevista sea óptimo”.\nEn SII Group Chile\n⏰ Tu tiempo nos importa: días administrativos libres.\n🏆 Queremos que te desarrolles: Programas de capacitaciones a tu medida.\n🚀 Ser #fungineer es fun: ¡FunFridays y eventos varios!\n💡Anímate a formar parte del Team enviándonos tu cv con pretensiones de renta a: \nnatalia.lara@siigroup.cl / seleccion@siigroup.cl        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4223983495/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)' 'Spark knowledge']"
        ],
        [
         "45",
         "Data Engineer Junior",
         "Isapre Consalud",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226383768/",
         "Trabajamos para brindar tranquilidad y seguridad en el acceso a la salud a más de 500 mil personas cada día ✨✨\nPara lograrlo contamos con el respaldo de la Cámara Chilena de la Construcción y un gran equipo de personas comprometidas con el bienestar de Chile. Tenemos la convicción de que es posible acceder a una experiencia de salud y bienestar superior que se articula de manera integral en torno a las necesidades.\nNos desafiamos a transformar la relación de las personas con el sistema a través de un vínculo colaborativo simple y transparente, que entrega soluciones efectivas y oportunas.\nNuestra Subgerencia Inteligencia de Negocios está en búsqueda de un profesional para el cargo de \nData Engineer Junior, \ndonde serás responsable de definir, configurar y construir sistemas, arquitecturas y plataformas de datos para asegurar la confiabilidad y calidad de grandes volúmenes de datos.\nEntre sus principales funciones se encuentran:\n- Implementar proyectos de Gestión de la información y garantizar que cumplan los lineamientos establecidos a partir de la estrategia de los datos.\n- Diseño de sistemas de Procesamiento de Datos, usando métodos y herramientas apropiadas para la gestión de la información y los modelamientos de datos para soluciones de procesamiento de datos.\n- Contribuir a la política de selección de los componentes de la arquitectura analítica.\n- Evaluar y realizar el análisis de impacto en las principales opciones de diseño y gestionar los riesgos asociados\nLos principales requisitos se encuentran:\n- Titulo profesional de Ingeniero en Computación y/o Informática (Ejecución o Civil); Otras carreras afines.\n- Conocimientos en modelamiento de Datos, Data Warehouse, SQL, ETL (ODI), Inteligencia de Negocios, OBI, Tableau, AWS, Python.\n- Deseable: Machine Learning.\n- 1 año de experiencia en cargos similares\n✨En Consalud promovemos el valor de ser tú mismo, por lo que impulsamos la inclusión laboral y diversidad bajo la Ley 21.015✨ Nos enfocamos en el equilibrio de tu vida personal y laboral, por lo que trabajamos 40 horas semanales.\n🚀🚀Súmate a nuestro equipo y sigamos protagonizando el ecosistema de salud de nuestro país desde adentro 💼\n❗️Toda la información recopilada durante el proceso de postulación será utilizada solamente para fines de selección❗️        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology",
         "banking_and_health_and_human_services",
         "https://www.linkedin.com/jobs/view/4226383768/",
         "Medium",
         "Junior",
         "Yes",
         "No",
         "1",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "46",
         "Data Engineer Junior",
         "LISIT",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4205294902/",
         "En Lisit, nos dedicamos a crear, desarrollar e implementar herramientas y servicios de software que automatizan y optimizan procesos, siempre con un fuerte enfoque en la innovación y los desafíos que se presentan. Nuestro objetivo es fomentar la eficacia operativa de nuestros clientes, ayudándoles a alcanzar sus metas de transformación mediante un acompañamiento consultivo integral. Actualmente, estamos en búsqueda de un Data Engineer Junior que se una a nuestro equipo apasionado por la tecnología y el aprendizaje continuo.\r\n Funciones del Rol\r\nComo Data Engineer Junior, Serás Parte Esencial Del Equipo Encargado De Manejar y Optimizar El Flujo De Datos De La Organización. Tus Principales Responsabilidades Incluirán\r\n- Colaborar en la recopilación y procesamiento de datos relacionales y no relacionales.\r\n- Trabajar con lenguajes de programación, especialmente Python, para crear soluciones de datos efectivas.\r\n- Implementar y mantener los procesos de integración en ambientes cloud como GCP o Azure.\r\n- Realizar consultas y manipulación de bases de datos utilizando SQL.\r\n- Aprender y adaptarte a nuevas tecnologías y herramientas en el entorno de la nube.\r\n Descripción del Perfil\r\nBuscamos Un Perfil Proactivo, Con Conocimientos Intermedios En Python y Disposición Para Aprender Sobre Nuevas Tecnologías. El Candidato Ideal Deberá Tener\r\n- Experiencia básica a intermedia en programación Python.\r\n- Habilidades en el uso y tratamiento de datos en ambientes tanto relacionales como no relacionales.\r\n- Conocimientos fundamentales en tecnologías de nube, incluyendo GCP o Azure.\r\n- Experiencia en el uso del lenguaje SQL.\r\n- Bajo es requisito pero se valorará el conocimiento en Power BI.\r\n Habilidades Deseables\r\nSería excelente contar con conocimientos adicionales en herramientas de visualización de datos como Power BI. Además, habilidad para trabajar en equipo y una mentalidad orientada al aprendizaje continuo son altamente valoradas.\r\n Beneficios de Trabajar con Nosotros\r\nEn Lisit, Promovemos Un Ambiente De Trabajo Excepcional\r\n- Acceso a oportunidades de desarrollo profesional continuo en tecnologías emergentes.\r\n- Un equipo apasionado por la innovación y el aprendizaje, donde tu entusiasmo será bienvenido.        \r\n            \r\n                            \r\n            ",
         "Entry level",
         "Full-time",
         "Information Technology",
         "Technology, Information and Internet and Information Technology & Services",
         "https://www.linkedin.com/jobs/view/4205294902/",
         "Medium",
         "Junior",
         "No",
         "No",
         "Not Specified",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Version control (GIT or similar)' 'None of the above mentioned skills']"
        ],
        [
         "47",
         "Data Engineer MACHBANK",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-04-28 00:00:00",
         "https://www.linkedin.com/jobs/view/4219005225/",
         "El rol de Senior Data Engineer en el equipo de Data Risk tiene como misión habilitar y escalar la oferta de crédito de MACH mediante el diseño e implementación de servicios de datos que soporten políticas de riesgo y decisiones crediticias. Buscamos un perfil con experiencia técnica sólida para liderar la definición y evolución de pipelines de datos robustos y eficientes, transformando requerimientos de negocio en soluciones arquitectónicas innovadoras y sostenibles. Además, el rol requiere autonomía y capacidad de articulación con stakeholders técnicos y no técnicos, con el objetivo de crear soluciones integrales que mejoren la experiencia y acceso al crédito de los usuarios.\nEn este rol tendrás la oportunidad de:\nArquitectura y desarrollo de soluciones de datos:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nBuenas prácticas y calidad técnica:\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nColaboración y alineamiento con el negocio:\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de riesgo del banco, alineando objetivos técnicos con las metas de negocio.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los elementos clave del riesgo de crédito, para orientar los desarrollos hacia soluciones que permitan una gestión adecuada del riesgo.\nComprender los principales desafíos relacionados al riesgo no financiero, con especial foco en prevención de fraude, para aportar en el diseño de soluciones que mitiguen dichos riesgos.\nLiderazgo técnico y mentoring:\nMentorear a perfiles con menos seniority del equipo, compartiendo conocimientos, buenas prácticas y promoviendo una cultura de aprendizaje continuo.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nExperiencia profesional:\nAl menos 3 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 2 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como ingenieros/arquitectos de datos y desarrolladores de software.\nExperiencia en el sector bancario o financiero, con entendimiento de conceptos clave del negocio.\nConocimientos técnicos:\nConocimientos sólidos de arquitectura de software, especialmente en el diseño y construcción de servicios de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos y optimización de consultas.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o Bitbucket.\nFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nEs aún mejor si tienes:\nConocimiento en riesgo de crédito, tanto a nivel de modelos como en las políticas del proceso de otorgamiento de crédito.\nConocimiento de riesgo no financiero, con foco en fraude y cumplimiento normativo.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4219005225/",
         "High",
         "Senior",
         "No",
         "No",
         "3",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "48",
         "Data Engineer MACHBANK",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226352405/",
         "El rol de Senior Data Engineer en el equipo de Data Risk tiene como misión habilitar y escalar la oferta de crédito de MACH mediante el diseño e implementación de servicios de datos que soporten políticas de riesgo y decisiones crediticias. Buscamos un perfil con experiencia técnica sólida para liderar la definición y evolución de pipelines de datos robustos y eficientes, transformando requerimientos de negocio en soluciones arquitectónicas innovadoras y sostenibles. Además, el rol requiere autonomía y capacidad de articulación con stakeholders técnicos y no técnicos, con el objetivo de crear soluciones integrales que mejoren la experiencia y acceso al crédito de los usuarios.\nEn este rol tendrás la oportunidad de:\nArquitectura y desarrollo de soluciones de datos:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nBuenas prácticas y calidad técnica:\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nColaboración y alineamiento con el negocio:\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de riesgo del banco, alineando objetivos técnicos con las metas de negocio.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los elementos clave del riesgo de crédito, para orientar los desarrollos hacia soluciones que permitan una gestión adecuada del riesgo.\nComprender los principales desafíos relacionados al riesgo no financiero, con especial foco en prevención de fraude, para aportar en el diseño de soluciones que mitiguen dichos riesgos.\nLiderazgo técnico y mentoring:\nMentorear a perfiles con menos seniority del equipo, compartiendo conocimientos, buenas prácticas y promoviendo una cultura de aprendizaje continuo.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nExperiencia profesional:\nAl menos 3 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 2 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como ingenieros/arquitectos de datos y desarrolladores de software.\nExperiencia en el sector bancario o financiero, con entendimiento de conceptos clave del negocio.\nConocimientos técnicos:\nConocimientos sólidos de arquitectura de software, especialmente en el diseño y construcción de servicios de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos y optimización de consultas.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o Bitbucket.\nFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nEs aún mejor si tienes:\nConocimiento en riesgo de crédito, tanto a nivel de modelos como en las políticas del proceso de otorgamiento de crédito.\nConocimiento de riesgo no financiero, con foco en fraude y cumplimiento normativo.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4226352405/",
         "High",
         "Senior",
         "No",
         "No",
         "3",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "49",
         "Data Engineer MACHBANK",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-17 00:00:00",
         "https://www.linkedin.com/jobs/view/4228584318/",
         "El rol de Data Engineer en el equipo de Data Experience tiene como misión principal habilitar experiencias WOW basadas en datos e inteligencia artificial en MACHBANK, colaborando con los equipos de Customer Success, Customer Experience y Product. Buscamos un perfil con experiencia técnica, capaz de liderar la definición, construcción y evolución de procesos de datos robustos, eficientes y alineados con estándares de calidad y escalabilidad. Esta persona será clave en la transformación de requerimientos de negocio en soluciones de arquitectura de datos y machine learning, permitiendo potenciar la experiencia que viven nuestros clientes en la App y en nuestros canales asistidos.\nEn este rol tendrás la oportunidad de:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de MACHBANK.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los dolores de nuestros clientes para apoyar la búsqueda de nuevos casos de usos de experiencia basada en datos.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nAl menos 2 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 1 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como devops, ingenieros de software, data scientists y machine learning engineers.\nConocimientos sólidos de arquitectura e ingeniería de software, especialmente en el diseño y construcción de servicios escalables basados en alto volumen y/o velocidad de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos, optimización de consultas y análisis de datos.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o BitbucketFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nConocimiento de las técnicas y metodologías de Machine Learning.\nEs aún mejor si tienes:\nConocimiento en LLM (Large Language Models) e Inteligencia Artificial, incluyendo patrones de diseños y prompting.\nConocimiento en frameworks de procesamiento distribuido como Spark.\nExperiencia en el desarrollo de experiencias potenciada con datos e inteligencia artificial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4228584318/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "2",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Knowledge of Machine Learning or MLOps' 'CI/CD'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 110
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>job_url</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>job_url_1</th>\n",
       "      <th>task_clarity</th>\n",
       "      <th>seniority_level_ai</th>\n",
       "      <th>requires_degree_it</th>\n",
       "      <th>mentions_certifications</th>\n",
       "      <th>years_of_experience</th>\n",
       "      <th>is_in_english</th>\n",
       "      <th>cloud_preference</th>\n",
       "      <th>skills_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analista Data Engineer AzureDataBricks</td>\n",
       "      <td>Accenture Chile</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212693987/</td>\n",
       "      <td>WORK AT THE HEART OF THE CHANGE\\nEmprende una ...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212693987/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Junior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212167524/</td>\n",
       "      <td>We are seeking an experienced Chief Data Engin...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Engineering, Information Technology, and Busin...</td>\n",
       "      <td>Software Development, IT Services and IT Consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212167524/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chief Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4220611480/</td>\n",
       "      <td>We are seeking an experienced \\nChief Data Eng...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering,_information_technology,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4220611480/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consultor Data Engineer</td>\n",
       "      <td>MAS Analytics</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223641021/</td>\n",
       "      <td>MAS Analytics\\n es una consultora de datos e i...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet_and_infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223641021/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>2Brains</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205294882/</td>\n",
       "      <td>2Brains es una empresa dedicada a construir y ...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Technology, Information and Internet and Infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205294882/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Ottomatik.io</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-12</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4207190424/</td>\n",
       "      <td>Hi there! We are South and our client is looki...</td>\n",
       "      <td>Executive</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Business Development, Consulting, and Engineering</td>\n",
       "      <td>IT System Data Services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4207190424/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sparq</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210998431/</td>\n",
       "      <td>Team Sparq is committed to creating high-quali...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210998431/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Senior Data Engineer Python</td>\n",
       "      <td>23people</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205294906/</td>\n",
       "      <td>Únete a Equifax Chile como Senior Data Enginee...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Technology, Information and Internet and Infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205294906/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Sr. Data Engineer (@Remote, Chile and Colombia)</td>\n",
       "      <td>OfferUp</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4230935527/</td>\n",
       "      <td>About The Role\\nWe are looking for an experien...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4230935527/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Spark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Sr. Data Engineer (Snowflake/dbt)</td>\n",
       "      <td>Sparq</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210996594/</td>\n",
       "      <td>Team Sparq is committed to creating high-quali...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210996594/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Mention</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          company   \n",
       "0             Analista Data Engineer AzureDataBricks  Accenture Chile  \\\n",
       "1                                Chief Data Engineer     EPAM Systems   \n",
       "2                                Chief Data Engineer     EPAM Systems   \n",
       "3                            Consultor Data Engineer    MAS Analytics   \n",
       "4                                      Data Engineer          2Brains   \n",
       "..                                               ...              ...   \n",
       "105                             Senior Data Engineer     Ottomatik.io   \n",
       "106                             Senior Data Engineer            Sparq   \n",
       "107                      Senior Data Engineer Python         23people   \n",
       "108  Sr. Data Engineer (@Remote, Chile and Colombia)          OfferUp   \n",
       "109                Sr. Data Engineer (Snowflake/dbt)            Sparq   \n",
       "\n",
       "                                          location       date   \n",
       "0    Santiago, Santiago Metropolitan Region, Chile 2025-05-13  \\\n",
       "1                                            Chile 2025-04-17   \n",
       "2                                            Chile 2025-05-01   \n",
       "3    Santiago, Santiago Metropolitan Region, Chile 2025-05-07   \n",
       "4                                            Chile 2025-04-10   \n",
       "..                                             ...        ...   \n",
       "105  Santiago, Santiago Metropolitan Region, Chile 2025-04-12   \n",
       "106                                          Chile 2025-04-15   \n",
       "107                                          Chile 2025-04-10   \n",
       "108                                          Chile 2025-05-15   \n",
       "109                                          Chile 2025-04-15   \n",
       "\n",
       "                                            job_url   \n",
       "0    https://www.linkedin.com/jobs/view/4212693987/  \\\n",
       "1    https://www.linkedin.com/jobs/view/4212167524/   \n",
       "2    https://www.linkedin.com/jobs/view/4220611480/   \n",
       "3    https://www.linkedin.com/jobs/view/4223641021/   \n",
       "4    https://www.linkedin.com/jobs/view/4205294882/   \n",
       "..                                              ...   \n",
       "105  https://www.linkedin.com/jobs/view/4207190424/   \n",
       "106  https://www.linkedin.com/jobs/view/4210998431/   \n",
       "107  https://www.linkedin.com/jobs/view/4205294906/   \n",
       "108  https://www.linkedin.com/jobs/view/4230935527/   \n",
       "109  https://www.linkedin.com/jobs/view/4210996594/   \n",
       "\n",
       "                                       job_description   seniority_level   \n",
       "0    WORK AT THE HEART OF THE CHANGE\\nEmprende una ...       entry_level  \\\n",
       "1    We are seeking an experienced Chief Data Engin...  Mid-Senior level   \n",
       "2    We are seeking an experienced \\nChief Data Eng...  mid-senior_level   \n",
       "3    MAS Analytics\\n es una consultora de datos e i...    not_applicable   \n",
       "4    2Brains es una empresa dedicada a construir y ...  Mid-Senior level   \n",
       "..                                                 ...               ...   \n",
       "105  Hi there! We are South and our client is looki...         Executive   \n",
       "106  Team Sparq is committed to creating high-quali...  Mid-Senior level   \n",
       "107  Únete a Equifax Chile como Senior Data Enginee...  Mid-Senior level   \n",
       "108  About The Role\\nWe are looking for an experien...  mid-senior_level   \n",
       "109  Team Sparq is committed to creating high-quali...  Mid-Senior level   \n",
       "\n",
       "    employment_type                                       job_function   \n",
       "0         full-time                             information_technology  \\\n",
       "1         Full-time  Engineering, Information Technology, and Busin...   \n",
       "2         full-time  engineering,_information_technology,_and_busin...   \n",
       "3         full-time                             information_technology   \n",
       "4         Full-time                             Information Technology   \n",
       "..              ...                                                ...   \n",
       "105       Full-time  Business Development, Consulting, and Engineering   \n",
       "106       Full-time                             Information Technology   \n",
       "107       Full-time                             Information Technology   \n",
       "108       full-time                             information_technology   \n",
       "109       Full-time                             Information Technology   \n",
       "\n",
       "                                            industries   \n",
       "0                     business_consulting_and_services  \\\n",
       "1    Software Development, IT Services and IT Consu...   \n",
       "2    software_development,_it_services_and_it_consu...   \n",
       "3    technology,_information_and_internet_and_infor...   \n",
       "4    Technology, Information and Internet and Infor...   \n",
       "..                                                 ...   \n",
       "105                            IT System Data Services   \n",
       "106                               Software Development   \n",
       "107  Technology, Information and Internet and Infor...   \n",
       "108               technology,_information_and_internet   \n",
       "109                               Software Development   \n",
       "\n",
       "                                          job_url_1 task_clarity   \n",
       "0    https://www.linkedin.com/jobs/view/4212693987/       Medium  \\\n",
       "1    https://www.linkedin.com/jobs/view/4212167524/         High   \n",
       "2    https://www.linkedin.com/jobs/view/4220611480/         High   \n",
       "3    https://www.linkedin.com/jobs/view/4223641021/       Medium   \n",
       "4    https://www.linkedin.com/jobs/view/4205294882/         High   \n",
       "..                                              ...          ...   \n",
       "105  https://www.linkedin.com/jobs/view/4207190424/       Medium   \n",
       "106  https://www.linkedin.com/jobs/view/4210998431/         High   \n",
       "107  https://www.linkedin.com/jobs/view/4205294906/       Medium   \n",
       "108  https://www.linkedin.com/jobs/view/4230935527/         High   \n",
       "109  https://www.linkedin.com/jobs/view/4210996594/         High   \n",
       "\n",
       "    seniority_level_ai requires_degree_it mentions_certifications   \n",
       "0               Junior                 No                     Yes  \\\n",
       "1      Lead or greater                Yes                      No   \n",
       "2      Lead or greater                Yes                      No   \n",
       "3               Junior                Yes                      No   \n",
       "4           Mid-Senior                 No                      No   \n",
       "..                 ...                ...                     ...   \n",
       "105             Senior                Yes                      No   \n",
       "106             Senior                 No                      No   \n",
       "107             Senior                 No                      No   \n",
       "108    Lead or greater                Yes                      No   \n",
       "109             Senior                 No                      No   \n",
       "\n",
       "    years_of_experience is_in_english cloud_preference   \n",
       "0                     1            No            Azure  \\\n",
       "1                    7+           Yes              AWS   \n",
       "2                    7+           Yes              AWS   \n",
       "3         Not Specified            No  Multiple Clouds   \n",
       "4         Not Specified            No              AWS   \n",
       "..                  ...           ...              ...   \n",
       "105                   4           Yes              AWS   \n",
       "106                   5           Yes            Azure   \n",
       "107                   5            No              GCP   \n",
       "108                   5           Yes              AWS   \n",
       "109                   3           Yes       No Mention   \n",
       "\n",
       "                                      skills_mentioned  \n",
       "0    [Databricks or snowflake, Develop pipelines or...  \n",
       "1    [Databricks or snowflake, Develop pipelines or...  \n",
       "2    [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "3    [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "4    [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "..                                                 ...  \n",
       "105  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "106  [Databricks or snowflake, Develop pipelines or...  \n",
       "107  [Develop pipelines or ETL/ELT processes, Versi...  \n",
       "108  [Develop pipelines or ETL/ELT processes, Spark...  \n",
       "109  [Databricks or snowflake, Develop pipelines or...  \n",
       "\n",
       "[110 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"\n",
    "        SELECT t1.*, t2.*\n",
    "        FROM test_table t1\n",
    "        JOIN genai_table t2\n",
    "        ON t1.job_url = t2.job_url\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY t1.title, t1.company, t1.date ORDER BY t1.date DESC) = 1\n",
    "        ORDER BY t1.title, t1.company, t1.date\n",
    "        \"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dee7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"\"\"\n",
    "ATTACH 'output/my_project.duckdb' AS output_db;\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1e08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "job_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seniority_level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_function",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_url_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_clarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seniority_level_ai",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "requires_degree_it",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mentions_certifications",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "years_of_experience",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_in_english",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cloud_preference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "skills_mentioned",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "68c3d421-8a62-4a6f-9e53-8009acf0ebf1",
       "rows": [
        [
         "0",
         "Chief Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4220611480/",
         "We are seeking an experienced \nChief Data Engineer\n to join our remote team and lead the delivery of innovative and scalable data solutions.\nThis role involves managing technical teams, designing system architecture, and contributing to meaningful projects that solve complex data challenges. The ideal candidate brings a strong technical background, leadership qualities, and a dedication to mentoring and innovation.\nResponsibilities\n- Take the lead in designing and implementing scalable and efficient data architectures\n- Build and maintain data pipelines and workflows to ensure data quality and accessibility\n- Collaborate with stakeholders to define requirements and deliver data-driven solutions\n- Mentor engineering teams and offer technical guidance\n- Improve system performance, scalability, and reliability\n- Enforce technical best practices and ensure the delivery of high-quality code\n- Provide feedback through code reviews and participate in technical discussions\n- Support client engagements, pre-sales activities, and SWAT team efforts\n- Foster knowledge sharing within the team and participate in meetups and technical talks\n- Keep up with emerging technologies to incorporate innovative solutions\nRequirements\n- A degree in Engineering, Computer Science, or a related field\n- At least 7 years of experience in a Data Engineering role\n- Minimum of 2 years' experience in leadership roles, including team and project management\n- Participation in at least 2 full-cycle projects or multiple phases of development life cycles\n- Expertise in coding and solving complex technical challenges\n- Proficiency in Python for data processing, analysis, and automation\n- Deep knowledge of Amazon Web Services (AWS) for cloud-based data solutions\n- Experience with Databricks for big data analytics and machine learning workflows\n- Strong skills in system design and architecture with the ability to align vision and implementation\n- Achievements delivering significant impact on projects and organizational goals\n- Broad cross-disciplinary knowledge covering various technical domains and stacks\n- Commitment to mentoring and sharing expertise within teams and beyond\n- Capacity to drive innovation by adopting new tools, frameworks, or methodologies\n- Flexibility to work with multiple programming languages, stacks, and domains as required\n- Fluent communication in English at a B2 level or higher\nNice to have\n- Background in real-time data processing and streaming technologies\n- Knowledge of advanced machine learning and AI frameworks\n- Familiarity with serverless computing solutions for data workflows\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4220611480/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "7+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Spark knowledge']"
        ],
        [
         "1",
         "Chief Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4212167524/",
         "We are seeking an experienced Chief Data Engineer to join our remote team and take charge of delivering innovative and scalable data solutions. In this role, you will lead technical teams, drive system architecture design, and contribute to impactful projects that solve complex data challenges. The ideal candidate will have a strong technical background, leadership skills, and a passion for mentoring and innovation.\nResponsibilities\n- Lead the design and implementation of scalable and efficient data architectures\n- Develop and maintain data pipelines and workflows to ensure data quality and accessibility\n- Collaborate with stakeholders to gather requirements and deliver data-driven solutions\n- Provide technical guidance and mentorship to engineering teams\n- Optimize system performance, scalability, and reliability\n- Drive technical best practices and ensure high-quality code delivery\n- Conduct code reviews and contribute to technical discussions\n- Support pre-sales activities, client engagements, and SWAT team initiatives\n- Promote knowledge sharing and participate in meetups and technical talks\n- Stay informed on emerging technologies and integrate innovative solutions\nRequirements\n- A degree in Engineering, Computer Science, or a related field\n- At least 7 years of experience as a Data Engineer\n- Minimum of 2 years of leadership experience, including team management and project direction\n- Experience participating in at least 2 full-cycle projects or multiple phases of development lifecycles\n- Proven hands-on expertise in coding and solving complex technical problems\n- Proficiency in Python for data processing, analysis, and automation\n- Deep knowledge of Amazon Web Services (AWS) for cloud-based data solutions\n- Experience with Databricks for big data analytics and machine learning workflows\n- Strong skills in system design and architecture, with the ability to balance high-level vision and detailed implementation\n- A track record of delivering significant technical impact within projects and across organizations\n- Broad cross-disciplinary knowledge of multiple technical domains and stacks\n- A mindset for mentoring and sharing expertise within teams and the broader community\n- Demonstrated ability to innovate by implementing new tools, frameworks, or methodologies\n- Adaptability to switch between programming languages, technical stacks, and domains as needed\n- Fluent communication skills in English at a B2 level or higher\nNice to have\n- Experience with real-time data processing and streaming technologies\n- Knowledge of advanced machine learning and AI frameworks\n- Familiarity with serverless computing solutions for data workflows\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4212167524/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "7+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Knowledge of Machine Learning or MLOps'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Spark knowledge']"
        ],
        [
         "2",
         "Consultor Data Engineer",
         "MAS Analytics",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4223641021/",
         "MAS Analytics\n es una consultora de datos e inteligencia artificail que nos dedicamos a desarrollar proyectos de Data Science (DS), Inteligencia Artificial, Cloud Architecture y Business Intelligence (BI). Hoy en día tenemos muchos proyectos con clientes de muchos rubros distintos y necesitamos ingeniero/as apasionados que estén buscando aprender y desafiarse trabajando en equipo.\nSi estás buscando profundizar y trabajar aplicando tus conocimientos día a día, dentro de un equipo de mucho talento, y alto rendimiento, este puesto es lo que estás buscando.\n Funciones del cargo\nEstamos buscando ingenier@s que sepan trabajar con datos, mover datos, interpretar datos y buscar datos. El cargo está orientado hacia la ingeniería de datos utilizando herramienta de ETL, Bases de datos y herramientas de reportería.\nSi no manejas alguno de estos conceptos, no te preocupes, nos importa que tengas las bases para poder aprender y desafiarte día a día.\nTendrás Que Realizar Las Siguientes Tareas (entre Otras)\n- Reuniones de negocio con clientes\n- Creación de queries SQL\n- Creación de pipelines ETL (on premise y cloud)\n- Creación de visualizaciones en herramientas de BI\nEl éxito de tu trabajo se evaluará en el nivel de satisfacción de los modelos entregados a los clientes y en el cumplimiento de plazos.\nTrabajarás dentro de los equipos de desarrollo de MAS Analytics teniendo reuniones de avance semanal con los clientes y otros miembros del equipo de MAS Analytics.\n Requerimientos del cargo\nAl ser consultoría en tecnología, necesitamos que tengas habilidades técnicas y habilidades blandas. Se privilegiarán perfiles de ingeniería civil TI o Computación.\nEn cuanto a las habilidades técnicas, se espera que puedas trabajar con las siguientes herramientas:\n- Queries SQL (CRUD)\n- Procesamiento: Dataflow (GCP, Azure), Lambda, Glue (AWS)\n- Visualizaciones: Qlik, Tableau, Power BI\n- Modelamiento de datos: modelos dimensionales OLAP\nCon respecto a las habilidades blandas, necesitamos que estés preparada/o para:\n- Llevar comunicación con los clientes\n- Levantar requerimientos\n- Trabajo en equipo\n- Proactividad\n- Pensamiento analítico\n Opcionales\nConocimientos en herramientas Cloud (GCP, AWS, Azure)\n Algunos beneficios\n- Horario flexible\n- Financiamiento de cursos y certificaciones 👨‍🎓👩‍🎓\n- Ambiente laboral joven\n- Viernes medio día\n- Vacaciones extra 🌞🏝\n- Celebraciones de cumpleaños 🎁🎊\n- Actividades de empresa 🍻⚽\n- Y muchos más que podrás conocer…        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4223641021/",
         "High",
         "Junior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Data modeling']"
        ],
        [
         "3",
         "Data Engineer",
         "2Brains",
         "Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4216874808/",
         "2Brains es una empresa dedicada a construir y desarrollar el Futuro Digital de nuestros clientes, con una visión excepcional que radica en la integración sinérgica de estrategia, diseño y tecnología, un tríptico poderoso que impulsa el crecimiento de empresas y disruptores tecnológicos.\nContamos con un nutrido equipo de más de 200 profesionales, verdaderos artífices de la innovación digital. En el corazón de nuestra labor, destacamos como líderes indiscutibles, canalizando años de experiencia hacia la creación de plataformas tecnológicas adaptables y productos digitales de clase mundial.\nEn 2Brains, no solo somos consultores, somos arquitectos de experiencias digitales. Aspiramos a ir más allá de las expectativas, estableciendo nuevos estándares en la industria. Descubre cómo damos vida a la innovación, cómo convertimos ideas en resultados tangibles y cómo, junto a nosotros, puedes forjar un futuro digital brillante.\n El/la Data Engineer de 2Brains debe\nParticipar en el diseño y desarrollo de los nuevos modelos de información de gestión y las mantenciones evolutivas de los existentes. Participar en las iniciativas de Analítica avanzada del área, apoyando las exploración de modelos de información internos y externos (Data Discovery). Obtener datos históricos desde múltiples fuentes de información interna para apoyar las iniciativas de analítica avanzada del equipo.\n Qué conocimientos buscamos en/la Data Engineer\n- Desarrollo de pipelines de datos para extracción, transformación y carga (ETL/ELT).\n- Dominio de herramientas de orquestación como Apache Airflow o Luigi.\n- Automatización y programación de flujos de datos complejos.\n- Gestión de bases de datos relacionales (SQL).\n- Manejo de bases de datos NoSQL (MongoDB, Cassandra, etc.).\n- Experiencia en Cloud Databases (BigQuery, Redshift, Snowflake, etc.).\n- Diseño y construcción de data warehouses, data lakes y lakehouses.\n- Modelado de datos para entornos analíticos y operacionales.\n- Uso de herramientas y tecnologías para procesamiento distribuido (ej. Spark, Hadoop, Kafka).\n- Experiencia en servicios de datos en la nube: Microsoft Azure, Google Cloud Platform, AWS.\n- Conocimientos en integración de flujos de datos con modelos de machine learning (deseable).\n- Manejo de prácticas DevOps para automatización, CI/CD y monitoreo en pipelines de datos.\n- +4 años de experiencia en roles de Ingeniería de Datos.\n- Certificaciones como Professional Data Engineer en Microsoft Azure o Google Cloud Platform.\n- Deseable experiencia desarrollando soluciones de análisis de datos o BI (Business Intelligence).\n Que competencias buscamos en/la Data Engineer \n- Empatía\n- Buena capacidad de comunicación.\n- Colaboración y trabajo en equipo.\n- Proactividad.\n- Autonomía.\n- Foco en los objetivos de proyectos.\n Te ofrecemos\n- Trabajar con un equipo de alto rendimiento, aprendemos y nos desarrollamos juntos\n- Acceso a grandes clientes y proyectos desafiantes\n- Aprendizaje y crecimiento permanente, organizamos meetups, capacitaciones y actividades culturales\n- Un entorno de trabajo flexible y dinámico\n- Día libre para tu cumpleaños.\n- 4 semanas de vacaciones al año.\n- Cursos de especialización.\n- Espacio para charlas internas.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4216874808/",
         "High",
         "Senior",
         "No",
         "Yes",
         "5+",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Knowledge of Machine Learning or MLOps' 'CI/CD' 'Spark knowledge']"
        ],
        [
         "4",
         "Data Engineer",
         "ARKHO",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4223640231/",
         "ARKHO es una consultora experta en tecnologías de la información, que ofrece servicios expertos de TI en el marco de modernización de aplicaciones, analítica de datos y migración a la nube. Nuestro trabajo facilita y acelera la adopción de la cloud en múltiples industrias.\nNos destacamos por ser Partner Advanced de Amazon Web Services con foco estratégico en la generación de soluciones usando tecnología en la nube, somos obsesionados por lograr los objetivos propuestos y tenemos especial énfasis en el grupo humano que compone ARKHO (nuestros Archers), reconociendo a las personas y equipo como un componente vital para el logro de los resultados.\n¿Te motivas? ¡Te esperamos!\n Funciones\nEstamos en busca de un o una profesional con experiencia en el desarrollo de pipelines de analytics, con una visión en la arquitectura de datos End to End, que logre aplicar sus conocimientos técnicos en la ejecución de proyectos tecnológicos. La vacante considera que el o la aspirante sea un referente orientando a la mejora continua y el cumplimiento de los objetivos y la creación de productos tecnológicos de alta calidad. Para ello, la experiencia en el uso de diversas herramientas, lenguajes de programación, infraestructura y otros, son la base para apoyar desde el conocimiento al equipo de desarrollo, participando en el desarrollo de entregables de alto impacto y calidad.\n- Trabajar de forma integrada y colaborativa con el líder de equipo, y el equipo de implementación proporcionando un contexto claro del estado de las iniciativas desde el ámbito tecnológico.\n- Apoyar al equipo en la definición, estimación y planificación de tareas/actividades para el desarrollo de productos de analítica en el marco ágil del desarrollo.\n- Gobernar técnicamente la solución con el cliente, participando activamente de decisiones claves y proporcionando información relevante en el aspecto técnico.\n- Trabajar con los integrantes del equipo en el diseño e implementación de arquitecturas de soluciones de analítica de datos en la nube y para la solución completa.\n- Involucramiento en el modelado de los datos, generando modelos analíticos corporativos.\n- Profundización en aspectos tecnológicos desconocidos y que se requieren para el logro de los objetivos.\n Perfil del archer\nEn Nuestra Compañía Valoramos a Las Personas Auto-gestionadas, Proactivas e Innovadoras. Debes Ser Capaz De Organizar Tu Propia Carga De Trabajo, Tomar Decisiones, Cumpliendo Tus Plazos y Objetivos. Buscamos Personas Con\n- Experiencia relevante en Python con datos.\n- Dominio de SQL y bases de datos no relacionales.\n- Conocimiento en tecnologías de ETL.\n- Capacidades de documentación de negocio y tecnológica.\n- Desarrollo de APIs.\n- Experiencia en AWS cloud.\n- Experiencia en proyectos BI y modelando bases de datos para Data Warehouse (Estrella, Copo de nieve).\n- Habilidades de priorización de requerimientos y resultados de negocio.\n Habilidades opcionales\nSi posees experiencia en algunas de las tecnologías a continuación es un plus:\n- Conocimiento de Machine Learning\n- Nivel de inglés Intermedio - Avanzado\n- Experiencia en metodologías ágiles como Scrum\n- Git\n Beneficios del Archer\n- Día administrativo semestral hasta los 12 meses \n- Week off: 5 días de vacaciones extra\n- ¡Celebra tu cumpleaños!\n- Path de entrenamiento\n- Certificaciones AWS\n- Aguinaldos Fiestas Patrias y Navidad\n- Bono de salud\n- Flexibilidad (trabajo híbrido o remoto)\n- Regalo por casamiento + 5 días hábiles libres\n- Regalo por nacimiento de hijos\n- Kit escolar\n- Beneficio paternidad\n- Bonda        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4223640231/",
         "Medium",
         "Mid-Senior",
         "No",
         "Yes",
         "Not Specified",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'APIs'\n 'Version control (GIT or similar)']"
        ],
        [
         "5",
         "Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225594752/",
         "We are seeking a highly skilled and motivated \nData Engineer\n to join our team.\nThe ideal candidate will be responsible for maintaining existing data systems, enhancing our current Data Platform, and implementing future changes as our clients' consumption needs and ingestion evolve. This role requires expertise in handling complex datasets and collaborating with senior team members on high-impact projects.\nResponsibilities\n- Maintain and enhance existing data systems\n- Develop enhancements for the existing Data Platform\n- Implement future changes to accommodate growing consumption needs and ingestion\n- Work with complex datasets, including the addition of new datasets from scratch\n- Collaborate with senior team members on complex projects, particularly when adding new large datasets\n- Grow with the technological landscape, adapting to new tools and methodologies as they emerge\nRequirements\n- 2+ experience in Data Software Engineering\n- Proficiency in PySpark\n- Experience with Azure Databricks, Azure DevOps, and Azure Event Hubs\n- Knowledge of the Azure cloud stack\n- Experience in working with large and complex datasets\n- Ability to work in a team and collaborate with various stakeholders\n- Familiarity with Databricks Unity Catalog and Terraform\nNice to have\n- Experience in Azure Analytics Engineering\n- Familiarity with DevOps practices\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4225594752/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "2+",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge' 'CI/CD'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "6",
         "Data Engineer",
         "Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4207045620/",
         "Descripción Empresa\nSomos más de 80 mil personas que cada día trabajamos por el firme Propósito - Simplificar y Disfrutar más la Vida. Estamos presentes en 9 países y compuestos por grandes marcas posicionadas de diversas industrias. Falabella Retail, Sodimac, Banco Falabella, Tottus, Mallplaza, Falabella.com, Falabella Inmobiliario. Cada una de éstas nos hace ser quienes somos, y es entre todos, como Un Solo Equipo, que buscamos diariamente reinventarnos y superar las experiencias de nuestros clientes.\nSi eres trabajador de Falabella, revisa todos los cursos disponibles en la Academia Falabella, que te ayudarán a seguir impulsando tu desarrollo y preparar tu próxima aventura con nosotros!\nSOMOS UNA EMPRESA QUE APOYA LA LEY 21015, APOYAMOS LA DIVERSIDAD Y LA INCLUSIÓN EN TODAS SUS FORMAS, SIN IMPORTAR RELIGIÓN, RAZA, GÉNERO, SITUACIÓN DE DISCAPACIDAD, NACIONALIDAD.\nFunciones Del Cargo\n¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti!\nEn Falabella Retail buscamos a nuestro/a próximo/a Data Engineer, con base en Santiago, Chile.\nSomos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\n¿Cuál es el principal objetivo del cargo?\nLiderar la construcción y mantención de estructuras de datos, así como la arquitectura tecnológica requerida para el procesamiento de apps.\n¿Qué harás en el día a día?\n-  Desarrollo, implementación de procesos ETL.\n-  Levantamiento de requerimientos funcionales y técnicos relacionados con los clientes internos.\n-  Implementar modelos de datos automatizados para transformar datos de acuerdo a los requisitos del negocio.\n-  Migración de datos desde entornos on-premise a entornos Cloud.\n-  Trabajar con tecnologías Google Cloud Platform (Big Query).\n¿Qué necesitas para postular?\n-  Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n-  Conocimiento en SQL (excluyente)\n-  Sólidos conocimientos en Google Cloud Platform (excluyente)\n-  Conocimiento avanzado en Python (excluyente)\n-  Conocimiento y experiencia trabajando en GIT (excluyente)\n-  Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nEn Nuestro Equipo Encontrarás\n-  Espacios para crear e innovar.\n-  Serás parte de un lugar lleno de oportunidades de desarrollo.\n-  Tener un trabajo con sentido y donde se promueve la calidad de vida.\n-  Participar en voluntariados.\n-  ¡Pertenecer a una empresa llena de energía!\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\nSomos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\nRequisitos\n- Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n- Conocimiento en SQL (excluyente)\n- Sólidos conocimientos en Google Cloud Platform (excluyente)\n- Conocimiento avanzado en Python (excluyente)\n- Conocimiento y experiencia trabajando en GIT (excluyente)\n- Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nCondiciones Oferta\nDescripción proceso de selección:\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\nPara Postular Solo Necesitas\n-  Postular a la oferta\n-  Revisar tu email\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "retail",
         "https://www.linkedin.com/jobs/view/4207045620/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "7",
         "Data Engineer",
         "Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225586690/",
         "Descripción Empresa\nSomos más de 80 mil personas que cada día trabajamos por el firme Propósito - Simplificar y Disfrutar más la Vida. Estamos presentes en 9 países y compuestos por grandes marcas posicionadas de diversas industrias. Falabella Retail, Sodimac, Banco Falabella, Tottus, Mallplaza, Falabella.com, Falabella Inmobiliario. Cada una de éstas nos hace ser quienes somos, y es entre todos, como Un Solo Equipo, que buscamos diariamente reinventarnos y superar las experiencias de nuestros clientes.\nSi eres trabajador de Falabella, revisa todos los cursos disponibles en la Academia Falabella, que te ayudarán a seguir impulsando tu desarrollo y preparar tu próxima aventura con nosotros!\nSOMOS UNA EMPRESA QUE APOYA LA LEY 21015, APOYAMOS LA DIVERSIDAD Y LA INCLUSIÓN EN TODAS SUS FORMAS, SIN IMPORTAR RELIGIÓN, RAZA, GÉNERO, SITUACIÓN DE DISCAPACIDAD, NACIONALIDAD.\nFunciones Del Cargo\n¡Si tienes una mente inquieta y te gusta soñar en grande, este llamado es para ti!\nEn Falabella Retail buscamos a nuestro/a próximo/a Data Engineer, con base en Santiago, Chile.\nSomos Falabella, UN equipo diverso con más de 100 mil colaboradores compuesto por grandes marcas: Falabella Retail, Sodimac, Banco Falabella, Seguros Falabella, Tottus, Mallplaza, Open Plaza y Linio. Hoy tenemos presencia en 7 países de América Latina, además de oficinas en China e India.\n¿Cuál es el principal objetivo del cargo?\nLiderar la construcción y mantención de estructuras de datos, así como la arquitectura tecnológica requerida para el procesamiento de apps.\n¿Qué harás en el día a día?\n-  Desarrollo, implementación de procesos ETL.\n-  Levantamiento de requerimientos funcionales y técnicos relacionados con los clientes internos.\n-  Implementar modelos de datos automatizados para transformar datos de acuerdo a los requisitos del negocio.\n-  Migración de datos desde entornos on-premise a entornos Cloud.\n-  Trabajar con tecnologías Google Cloud Platform (Big Query).\n¿Qué necesitas para postular?\n-  Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n-  Conocimiento en SQL (excluyente)\n-  Sólidos conocimientos en Google Cloud Platform (excluyente)\n-  Conocimiento avanzado en Python (excluyente)\n-  Conocimiento y experiencia trabajando en GIT (excluyente)\n-  Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nEn Nuestro Equipo Encontrarás\n-  Espacios para crear e innovar.\n-  Serás parte de un lugar lleno de oportunidades de desarrollo.\n-  Tener un trabajo con sentido y donde se promueve la calidad de vida.\n-  Participar en voluntariados.\n-  ¡Pertenecer a una empresa llena de energía!\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros!\nSomos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\nRequisitos\n- Profesional: Ingeniería Civil en Computación, Informática, Sistemas o carrera afín.\n- Conocimiento en SQL (excluyente)\n- Sólidos conocimientos en Google Cloud Platform (excluyente)\n- Conocimiento avanzado en Python (excluyente)\n- Conocimiento y experiencia trabajando en GIT (excluyente)\n- Disponibilidad para ir a la oficina al menos 2 veces por semana (Las Condes) (excluyente)\nCondiciones Oferta\nDescripción proceso de selección:\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\nPara Postular Solo Necesitas\n-  Postular a la oferta\n-  Revisar tu email\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "retail",
         "https://www.linkedin.com/jobs/view/4225586690/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "8",
         "Data Engineer",
         "Genesys",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4218817757/",
         "Somos una empresa con más de 34 años de experiencia en el mercado, especializada en fábrica de desarrollo de software y Outsourcing de profesionales TI. Nuestra casa matriz está en Concepción y también operamos en Santiago. Actualmente, buscamos incorporar a nuestro equipo un/a Data Engineer\nCondiciones de contratación\n- Ubicación: híbrido en Providencia, Santiago;\n- Modalidad: híbrida;\n- Contrato: por proyecto (6 meses);\n- Jornada: completa (44 horas semanales).\nResponsabilidades\n- Diseñar, desarrollar y mantener pipelines de datos eficientes y escalables (ETL/ELT) para integrar diversas fuentes de datos, respondiendo a las necesidades de los equipos de ciencia de datos y análisis\n- Implementar y administrar arquitecturas de almacenamiento de datos (Data Warehouses, Data Lakes, etc.).\n- Garantizar la calidad, integridad y seguridad de los datos procesados.\n- Colaborar con equipos de analítica y ciencia de datos para proporcionar datos limpios y procesables.\n- Optimizar el rendimiento y la escalabilidad de las infraestructuras de datos.\n- Implementar soluciones de monitoreo y diagnóstico para los sistemas de datos.\n- Documentar arquitecturas, procesos y flujos de trabajo relacionados con los datos.\nRequisitos\n- Desde 3 años a 5 años de experiencia;\n- Experiencia en la creación de pipelines de datos utilizando herramientas como Apache Airflow, Apache Nifi, Step Functions o similares.\n- Conocimientos avanzados de bases de datos relacionales y no relacionales (SQL, PostgreSQL, MongoDB, etc.). Dominio de PL-SQL.\n- Dominio de lenguajes de programación como Python o Scala.\n- Experiencia trabajando con plataformas de Big Data como Hadoop o Spark.\n- Familiaridad con arquitecturas en la nube (Idealmente sobre AWS) y herramientas relacionadas (S3, EMR, Athena, Glue, Redshift, etc.).\n- Experiencia en la implementación de procesos de control de calidad de datos y depuración..\n¡Postula!        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "software_development_and_financial_services",
         "https://www.linkedin.com/jobs/view/4218817757/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "3+",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Spark knowledge']"
        ],
        [
         "9",
         "Data Engineer",
         "NeuralWorks",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4216875700/",
         "NeuralWorks es una compañía de alto crecimiento fundada hace 3 años. Estamos trabajando a toda máquina en cosas que darán que hablar.\nSomos un equipo donde se unen la creatividad, curiosidad y la pasión por hacer las cosas bien. Nos arriesgamos a explorar fronteras donde otros no llegan: un modelo predictor basado en monte carlo, una red convolucional para detección de caras, un sensor de posición bluetooth, la recreación de un espacio acústico usando finite impulse response.\nEstos son solo algunos de los desafíos, donde aprendemos, exploramos y nos complementamos como equipo para lograr cosas impensadas.\nTrabajamos en proyectos propios y apoyamos a corporaciones en partnerships donde codo a codo combinamos conocimiento con creatividad, donde imaginamos, diseñamos y creamos productos digitales capaces de cautivar y crear impacto.\n👉 Conoce más sobre nosotros\n Descripción del trabajo\nEl equipo de Data y Analytics trabaja en diferentes proyectos que combinan volúmenes de datos enormes e IA, como detectar y predecir fallas antes que ocurran, optimizar pricing, personalizar la experiencia del cliente, optimizar uso de combustible, detectar caras y objetos usando visión por computador.\nDentro del equipo multidisciplinario con Data Scientist, Translators, DevOps, Data Architect, tu rol será clave en construir y proveer los sistemas e infraestructura que permiten el desarrollo de estos servicios, formando los cimientos sobre los cuales se construyen los modelos que permiten generar impacto, con servicios que deben escalar, con altísima disponibilidad y tolerantes a fallas, en otras palabras, que funcionen. Además, mantendrás tu mirada en los indicadores de capacidad y performance de los sistemas.\nEn cualquier proyecto que trabajes, esperamos que tengas un gran espíritu de colaboración, pasión por la innovación y el código y una mentalidad de automatización antes que procesos manuales.\nComo Data Engineer, Tu Trabajo Consistirá En\n- Participar activamente durante el ciclo de vida del software, desde inception, diseño, deploy, operación y mejora.\n- Apoyar a los equipos de desarrollo en actividades de diseño y consultoría, desarrollando software, frameworks y capacity planning.\n- Desarrollar y mantener arquitecturas de datos, pipelines, templates y estándares.\n- Conectarse a través de API a otros sistemas (Python)\n- Manejar y monitorear el desempeño de infraestructura y aplicaciones.\n- Asegurar la escalabilidad y resiliencia.\n Calificaciones clave\n- Estudios de Ingeniería Civil en Computación o similar.\n- Experiencia práctica de al menos 3 años en entornos de trabajo como Data Engineer, Software Engineer entre otros.\n- Experiencia con Python. Entendimiento de estructuras de datos con habilidades analíticas relacionadas con el trabajo con conjuntos de datos no estructurados, conocimiento avanzado de SQL, incluida optimización de consultas.\n- Pasión en problemáticas de procesamiento de datos.\n- Experiencia con servidores cloud (GCP, AWS o Azure), especialmente el conjunto de servicios de procesamiento de datos.\n- Buen manejo de inglés, sobre todo en lectura donde debes ser capaz de leer un paper, artículos o documentación de forma constante.\n- Habilidades de comunicación y trabajo colaborativo.\n¡En NeuralWorks nos importa la diversidad! Creemos firmemente en la creación de un ambiente laboral inclusivo, diverso y equitativo. Reconocemos y celebramos la diversidad en todas sus formas y estamos comprometidos a ofrecer igualdad de oportunidades para todos los candidatos.\n“Los hombres postulan a un cargo cuando cumplen el 60% de las calificaciones, pero las mujeres sólo si cumplen el 100%.” D. Gaucher , J. Friesen and A. C. Kay, Journal of Personality and Social Psychology, 2011.\nTe invitamos a postular aunque no cumplas con todos los requisitos.\n Nice to have\n- Agilidad para visualizar posibles mejoras, problemas y soluciones en Arquitecturas.\n- Experiencia en Infrastructure as code, observabilidad y monitoreo.\n- Experiencia en la construcción y optimización de data pipelines, colas de mensajes y arquitecturas big data altamente escalables.\n- Experiencia en procesamiento distribuido utilizando servicios cloud.\n Beneficios\n- MacBook Air M2 o similar (con opción de compra hiper conveniente)\n- Bono por desempeño\n- Bono de almuerzo mensual y almuerzo de equipo los viernes\n- Seguro complementario de salud y dental\n- Horario flexible\n- Flexibilidad entre oficina y home office\n- Medio día libre el día de tu cumpleaños\n- Financiamiento de certificaciones\n- Inscripción en Coursera con plan de entrenamiento a medida\n- Estacionamiento de bicicletas\n- Vestimenta informal\n- Programa de referidos\n- Salida de “teambuilding” mensual        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "technology,_information_and_internet_and_information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4216875700/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "3+",
         "Yes",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'APIs'\n 'Version control (GIT or similar)'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)']"
        ],
        [
         "10",
         "Data Engineer",
         "Seeds",
         "Santiago Metropolitan Region, Chile",
         "2025-04-28 00:00:00",
         "https://www.linkedin.com/jobs/view/4189755534/",
         "¿Sos \nData Engineer\n? Entonces… ¿Qué estás esperando para sumarte a nuestra comunidad de Seeders? ¡Aplica a nuestra comunidad y accede a trabajo on-demand en las empresas líderes, sumate al Present of Work!\n¿Quiénes somos?\nSomos una \ncomunidad\n que reúne al mejor talento on-demand de Latinoamérica, y lo conecta con las empresas líderes de la región. Gestionamos el match perfecto entre las necesidades de las empresas y el talento con las competencias y la experiencia buscada, fomentando flexibilidad y el desarrollo profesional de nuestra comunidad.\nNo somos una plataforma más de freelancers, Seeds lidera un dream team de profesionales altamente calificados que eligen dónde, cómo y para quién trabajar, disfrutando así de contribuir a una misión más grande, definiendo y moldeando la forma en que trabajamos.\nEstamos buscando sumar a nuestro Talent Pool roles de \nData Engineer\n para nuestra comunidad de Seeders.\nEstas son algunas de las responsabilidades usuales del rol:\n- Diseñar, construir y mantener arquitecturas de datos robustas y escalables.\n- Desarrollar y optimizar pipelines de datos para recopilación, almacenamiento, procesamiento y análisis de grandes volúmenes de datos.\n- Implementar modelos de datos y algoritmos para resolver problemas de negocio y proveer insights accionables.\n- Trabajar en estrecha colaboración con equipos de data scientists y analistas para apoyar sus requisitos de datos y facilitar el análisis de datos.\n- Asegurar la integridad, disponibilidad y confidencialidad de los datos a través de las mejores prácticas de seguridad y gobernanza de datos.\n- Mantenerse al día con las últimas tecnologías y tendencias en el campo de la ingeniería de datos.\nRequisitos\n- Experiencia mínima de 3 años en roles de ingeniería de datos.\n- Fuerte dominio de lenguajes de programación como Python, Java o Scala.\n- Experiencia trabajando con grandes volúmenes de datos y herramientas de procesamiento de datos (como Hadoop, Spark).\n- Conocimientos en bases de datos SQL y NoSQL, así como en soluciones de almacenamiento de datos en la nube (AWS, Google Cloud, Azure).\n- Capacidad para trabajar en entornos ágiles y multidisciplinarios.\n- Inglés intermedio (deseable).\n¿Por qué sumarte a nuestra comunidad de Seeders?\nElegí tus proyectos.\nTrabajá desde donde vos quieras.\nEventos de networking.\nAsesoramiento personalizado.\nSeeds Academy: Potencia tu desarrollo profesional adquiriendo nuevas skills (upskilling & reskilling), participando de webinars, Bootcamps y otras acciones exclusivas para la comunidad.\nNo dejes de sumarte a nuestra comunidad de Seeds y aplicar a oportunidades de empresas lideres de la región. ¡Te esperamos!        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "consulting",
         "technology,_information_and_media",
         "https://www.linkedin.com/jobs/view/4189755534/",
         "Medium",
         "Senior",
         "No",
         "No",
         "3+",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Collaboration with data scientists or analysts' 'Data governance'\n 'Spark knowledge']"
        ],
        [
         "11",
         "Data Engineer",
         "Xepelin",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4205474021/",
         "Somos una FinTech que busca democratizar los servicios financieros para todo tipo de empresas. Nos apalancamos en la mejor tecnología para crear soluciones ágiles, personalizadas y transparentes. Nuestro objetivo es ser la FinTech B2B más grande de Latam y convertirnos en el CFO digital de todas las empresas en la región.\nXepelin nace en Chile en 2019 y, desde entonces, hemos levantado más de USD145 millones en equity y USD300 millones en asset-backed facilities para potenciar el crecimiento en toda la región, especialmente en los países donde hoy operamos, Chile y México. La última ronda de equity fue de USD111 millones, record en Chile y una de las más grandes en América Latina para una FinTech.\n¿Por qué trabajar en Xepelin?\n💪 Desafío\nEstamos sacudiendo una de las industrias más poderosas y competitivas, eliminando fricciones para darle a las Pymes acceso a capital y apalancado en la última tecnología disponible. Todo esto poniendo siempre a nuestras Pymes en el centro.\nConstruir un banco digital desde cero es un gran proyecto; el producto es complejo y no se puede romper, existen regulaciones estrictas y tendremos que ser mejores que algunas de las corporaciones más grandes y consolidadas del mundo. Pero superar estos desafíos significa que habremos construido algo duradero.\n💥 Impacto\nTrabajar en un equipo de clase mundial y automotivado significa autonomía, amplia experiencia en todos los proyectos y ver que tus contribuciones afectan directamente al producto e impactan a nuestras Pymes.\nTrabajarás con todos nuestros productos, tendrás que tomar decisiones fundamentales. El correcto posicionamiento de ellos marcará su futuro.\n✔️ Calidad\nNuestro posicionamiento de marca y productos es algo diferenciador frente al resto del mercado por lo que invertimos mucho en crear productos de calidad que nuestras Pymes respeten y valoren.\nNos damos el tiempo para pensar fuera de la caja y volver con propuestas innovadoras. Estamos aquí para cambiar la industria!\n¿Qué estamos buscando? \nEn Xepelin estamos buscando personas creativas y visionarias que piensen fuera de la caja para sumarse a nuestro equipo. Si te apasiona resolver desafíos interesantes de alto impacto y quieres ser parte de un entorno dinámico que está transformando la industria financiera, ¡Esta oportunidad es para ti!\nEl rol se integrará a nuestro equipo de \nData Platform\n. Si te motiva el desafío de construir soluciones innovadoras en un entorno de rápido cambio, queremos conocerte.\nUnete a nosotros, crezcamos juntos!\nPrincipales responsabilidades...\n-  Diseñar, crear y mantener pipelines de datos\n-  Mantener y optimizar la infraestructura de datos necesaria para una extracción precisa, transformación y carga de datos de una amplia variedad de fuentes de datos\n-  Automatizar los flujos de trabajo de datos, como la ingesta de datos, la agregación y el procesamiento de ETL o ELT\n-  Preparar datos sin procesar en almacenes de datos en un conjunto de datos consumibles para fines técnicos y partes interesadas no técnicas\n-  Crear, mantener e implementar productos de datos para equipos de análisis y ciencia de datos en Plataformas en la nube de GCP\n-  Desarrollo de sistemas y arquitectura que soporte las diferentes etapas del flujo de Machine Learning\n-  Conocimientos en herramientas de infraestructura como código (Terraform preferentemente)\n¿Qué necesitas para brillar?\n- Conocimientos en GCP\n- Conocimientos intermedio/avanzado en Python\n- Conocimiento intermedio/avanzado de SQL\n- Experiencia desplegando aplicaciones en ambientes serverless como Cloud Functions o AWS Lambda\n- Conocimientos administrando y desplegando algún orquestador, por ejemplo: Apache Airflow, Dagster, etc\n- Excelentes habilidades para trabajar en equipo. Ser humilde y saber colaborar. un Team Player!\n- Saber escuchar a tus stakeholders y poder traducir eso en requerimientos y ejecutarlos con tu equipo\n- Conocimientos y manejo de lenguajes de programación y/o frameworks, NodeJS, Golang, por ejemplo\n- Conocimientos y/o experiencia en MLOps\n- Experiencia en la industria Fintech B2B\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- No tener miedo a tomar decisiones y liderar proyectos\n- Foco en impacto e historia consistente entregando resultados para usuarios y el negocio\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- Te sientes cómodo cuestionando el status-quo de los servicios financieros, adaptándose rápidamente a los cambios, y presentando claramente tus ideas y conceptos para debatirlos en equipo\nNuestros Beneficios:\n🌴 Xepelin Balance\nVacaciones:\n 15 días hábiles. Por cada año que cumplas en Xepelin, te damos un día extra de vacaciones.\nBalance days:\n 10 días libres adicionales al año, para disfrutar como quieras.\nTrabajo híbrido y flexibilidad horaria según el rol. Trabajamos por objetivos.\nBeneficios Flexibles: \nPuntos flexibles en tu moneda local al mes para gastar en lo que quieras.\nXepelin Fun:\n Actividades de encuentro financiadas por Xepelin para divertirnos juntos.\n🚀 Xepelin Performance & Career\nPlataformas de capacitación:\n Convenios con las mejores plataformas, como Reforge, Udemy y DataCamp.\nKit de Bienvenida: \ntodo lo que necesitas para comenzar tu viaje en Xepelin 😊\n🤝 Xepelin Cares\nCobertura de salud:\n contamos con convenios de salud con proveedores de calidad o reembolsos según el país donde te encuentres.\nPost Natal:\n te damos una semana extra de licencia post natal. ¡Nos interesa que estés con tu familia y seres queridos!\nMatrimonio plus:\n Lleva tus planes al siguiente nivel, con una gift card y extendiendo tu permiso legal por matrimonio con dos días de regalo por Xepelin.        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "hospitality,_food_and_beverage_services,_and_retail",
         "https://www.linkedin.com/jobs/view/4205474021/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Knowledge of Machine Learning or MLOps'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "12",
         "Data Engineer",
         "Xepelin",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4224174809/",
         "Somos una FinTech que busca democratizar los servicios financieros para todo tipo de empresas. Nos apalancamos en la mejor tecnología para crear soluciones ágiles, personalizadas y transparentes. Nuestro objetivo es ser la FinTech B2B más grande de Latam y convertirnos en el CFO digital de todas las empresas en la región.\nXepelin nace en Chile en 2019 y, desde entonces, hemos levantado más de USD145 millones en equity y USD300 millones en asset-backed facilities para potenciar el crecimiento en toda la región, especialmente en los países donde hoy operamos, Chile y México. La última ronda de equity fue de USD111 millones, record en Chile y una de las más grandes en América Latina para una FinTech.\n¿Por qué trabajar en Xepelin?\n💪 Desafío\nEstamos sacudiendo una de las industrias más poderosas y competitivas, eliminando fricciones para darle a las Pymes acceso a capital y apalancado en la última tecnología disponible. Todo esto poniendo siempre a nuestras Pymes en el centro.\nConstruir un banco digital desde cero es un gran proyecto; el producto es complejo y no se puede romper, existen regulaciones estrictas y tendremos que ser mejores que algunas de las corporaciones más grandes y consolidadas del mundo. Pero superar estos desafíos significa que habremos construido algo duradero.\n💥 Impacto\nTrabajar en un equipo de clase mundial y automotivado significa autonomía, amplia experiencia en todos los proyectos y ver que tus contribuciones afectan directamente al producto e impactan a nuestras Pymes.\nTrabajarás con todos nuestros productos, tendrás que tomar decisiones fundamentales. El correcto posicionamiento de ellos marcará su futuro.\n✔️ Calidad\nNuestro posicionamiento de marca y productos es algo diferenciador frente al resto del mercado por lo que invertimos mucho en crear productos de calidad que nuestras Pymes respeten y valoren.\nNos damos el tiempo para pensar fuera de la caja y volver con propuestas innovadoras. Estamos aquí para cambiar la industria!\n¿Qué estamos buscando? \nEn Xepelin estamos buscando personas creativas y visionarias que piensen fuera de la caja para sumarse a nuestro equipo. Si te apasiona resolver desafíos interesantes de alto impacto y quieres ser parte de un entorno dinámico que está transformando la industria financiera, ¡Esta oportunidad es para ti!\nEl rol se integrará a nuestro equipo de \nData Platform\n. Si te motiva el desafío de construir soluciones innovadoras en un entorno de rápido cambio, queremos conocerte.\nUnete a nosotros, crezcamos juntos!\nPrincipales responsabilidades...\n-  Diseñar, crear y mantener pipelines de datos\n-  Mantener y optimizar la infraestructura de datos necesaria para una extracción precisa, transformación y carga de datos de una amplia variedad de fuentes de datos\n-  Automatizar los flujos de trabajo de datos, como la ingesta de datos, la agregación y el procesamiento de ETL o ELT\n-  Preparar datos sin procesar en almacenes de datos en un conjunto de datos consumibles para fines técnicos y partes interesadas no técnicas\n-  Crear, mantener e implementar productos de datos para equipos de análisis y ciencia de datos en Plataformas en la nube de GCP\n-  Desarrollo de sistemas y arquitectura que soporte las diferentes etapas del flujo de Machine Learning\n-  Conocimientos en herramientas de infraestructura como código (Terraform preferentemente)\n¿Qué necesitas para brillar?\n- Conocimientos en GCP\n- Conocimientos intermedio/avanzado en Python\n- Conocimiento intermedio/avanzado de SQL\n- Experiencia desplegando aplicaciones en ambientes serverless como Cloud Functions o AWS Lambda\n- Conocimientos administrando y desplegando algún orquestador, por ejemplo: Apache Airflow, Dagster, etc\n- Excelentes habilidades para trabajar en equipo. Ser humilde y saber colaborar. un Team Player!\n- Saber escuchar a tus stakeholders y poder traducir eso en requerimientos y ejecutarlos con tu equipo\n- Conocimientos y manejo de lenguajes de programación y/o frameworks, NodeJS, Golang, por ejemplo\n- Conocimientos y/o experiencia en MLOps\n- Experiencia en la industria Fintech B2B\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- No tener miedo a tomar decisiones y liderar proyectos\n- Foco en impacto e historia consistente entregando resultados para usuarios y el negocio\n- Capacidad para pensar en grande y desarrollar iniciativas con impacto real y medible\n- Te sientes cómodo cuestionando el status-quo de los servicios financieros, adaptándose rápidamente a los cambios, y presentando claramente tus ideas y conceptos para debatirlos en equipo\nNuestros Beneficios:\n🌴 Xepelin Balance\nVacaciones:\n 15 días hábiles. Por cada año que cumplas en Xepelin, te damos un día extra de vacaciones.\nBalance days:\n 10 días libres adicionales al año, para disfrutar como quieras.\nTrabajo híbrido y flexibilidad horaria según el rol. Trabajamos por objetivos.\nBeneficios Flexibles: \nPuntos flexibles en tu moneda local al mes para gastar en lo que quieras.\nXepelin Fun:\n Actividades de encuentro financiadas por Xepelin para divertirnos juntos.\n🚀 Xepelin Performance & Career\nPlataformas de capacitación:\n Convenios con las mejores plataformas, como Reforge, Udemy y DataCamp.\nKit de Bienvenida: \ntodo lo que necesitas para comenzar tu viaje en Xepelin 😊\n🤝 Xepelin Cares\nCobertura de salud:\n contamos con convenios de salud con proveedores de calidad o reembolsos según el país donde te encuentres.\nPost Natal:\n te damos una semana extra de licencia post natal. ¡Nos interesa que estés con tu familia y seres queridos!\nMatrimonio plus:\n Lleva tus planes al siguiente nivel, con una gift card y extendiendo tu permiso legal por matrimonio con dos días de regalo por Xepelin.        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "hospitality,_food_and_beverage_services,_and_retail",
         "https://www.linkedin.com/jobs/view/4224174809/",
         "Medium",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "13",
         "Data Engineer ( Azure Cloud & Python & Databricks)",
         "Option",
         "Santiago Metropolitan Area",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4219824203/",
         "¿Quiénes somos?\nEn Option, creemos en un mundo donde las soluciones tecnológicas no tienen límites. Nuestra misión es transformar los desafíos en oportunidades mediante la creación de soluciones innovadoras que potencien la Aceleración Digital. Nuestro equipo es dinámico, colaborativo y apasionado por la tecnología. Únete a una organización que está redefiniendo cómo el mundo utiliza los datos y la tecnología para resolver problemas complejos.\n¿Qué buscamos?\nEstamos en la búsqueda de un/a Ingeniero/a de Datos para unirse al equipo de Analítica Avanzada. Este rol será clave en el desarrollo de soluciones de analítica avanzada y disponibilización de los resultados de los modelos para consumo de soluciones y clientes.\n¡Te estamos buscando!\n¿Qué te ofrece este puesto?\n- Participación en un proceso estratégico de migración a la nube.\n- Un entorno de trabajo colaborativo y con líderes técnicos accesibles.\n- Uso de tecnologías modernas como Cloud Azure, Databricks, entre otros.\n- Trabajo conjunto con equipos de analítica, desarrollo y operaciones.\n¿Cuáles serán tus principales responsabilidades?\n- Desarrollo de capas de negocio, conceptuales y físicas, a nivel de datos. \n- Asegurar que la solución desarrollada sea escalable a nivel de datos, en términos de reutilización de información y validación de indicadores de cara a la operación.\n¿Qué necesitas para ser nuestro próximo Ingeniero de Datos?\nHabilidades Técnicas Excluyentes\n- Azure Cloud\n- Conocimiento en gestión de proyectos bajo framework ágiles.\n- Conocimiento WKC y otra plataforma equivalente.\n- Python: Numpy, Pandas, Sklearn, Pyspark.\n- SQL\n- Spark\n- Databricks: Intermedio/Avanzado.\nHabilidades Técnicas Deseables\n- Certificaciones en Azure Cloud\n- Ubicación: LATAM Modalidad de trabajo: 100% Remoto\n¡Únete a nuestro equipo y transforma el futuro con nosotros!\nhttps://www.option.tech\n                \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4219824203/",
         "Medium",
         "Mid-Senior",
         "No",
         "Yes",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge'\n 'Collaboration with data scientists or analysts' 'Migration'\n 'Databricks or snowflake' 'Data modeling']"
        ],
        [
         "14",
         "Data Engineer - Databricks - Mid Level",
         "Lumenalta",
         "Chile",
         "2025-05-11 00:00:00",
         "https://www.linkedin.com/jobs/view/4228057809/",
         "Experience Remote done Right. With over 20 years of remote experience, all 500+ staff are 100% remote, and we still grow vibrant relationships and provide exceptional opportunities for career growth while working with stellar clients on ambitious projects.\nWhat we're working on:\nEnterprise companies turn to us to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. The problems we solve daily are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. To grasp the scale of problems we face, ideally, you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.\nRequirements\n- 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises\n- You are comfortable manipulating large data sets and handling raw SQL\n- Experience using technologies such as Pyspark/AWS/Databricks is essential\n- Experience creating ETL Pipeline from scratch\n- E-commerce and Financial Services industry experience preferred\n- English fluency, verbal and written\n- Personality traits: Professional, problem solver, proactive, passionate, team player.\nWhy Lumenalta is an amazing place to work at\nAt Lumenalta, you can expect that you will:\n- Be 100% dedicated to one project at a time so that you can innovate and grow.\n- Be a part of a team of talented and friendly senior-level developers.\n- Work on projects that allow you to use leading tech.\nThe result? We produce meaningful outcomes for our clients that break barriers in their industries.\nThe job is 100% Remote; please ensure you have a comfortable office set at your desired work location.\nLumenalta is committed to hiring exceptional talent from a wide variety of diverse backgrounds. If you share our values and enthusiasm for digital transformation, we encourage you to apply\nWhat's it like to work at Lumenalta?        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_consulting",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4228057809/",
         "Medium",
         "Senior",
         "No",
         "No",
         "3+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Spark knowledge']"
        ],
        [
         "15",
         "Data Engineer - Inglés Avanzado",
         "Deloitte",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4221350232/",
         "¿Buscas generar impactos significativos con tu trabajo? ¿Te interesa la innovación y desarrollar tus habilidades de liderazgo?\nEn Deloitte, encontrarás un espacio intelectualmente desafiante, diverso, inclusivo, y comprometido con su entorno, que potenciará el camino profesional que hoy desees emprender.\nCon un equipo multidisciplinario y una red global de profesionales expertos que comparten las mejores prácticas y experiencias del mercado, Deloitte se destaca por la entrega de un servicio de excelencia, conectado con las últimas tendencias, que ofrece a sus colaboradores/as un espacio de aprendizaje único donde se incentiva el liderazgo en todo nivel.\nNos encontramos en búsqueda de las y los mejores profesionales que quieran integrar nuestra práctica de \nArtificial Intelligence & Data (AI & Data).\nNuestro equipo de AI & Data utiliza el valor de los datos, la analítica, la robótica, la ciencia de datos y las tecnologías cognitivas para generar conocimiento y apoyar organizaciones en su jornada de evolución a AI & Insight Driven.\nDentro de las principales funciones como data engineer se destacan:\n- Responsable de la ingesta y la curación de datos.\n- Creación de ETL y canalizaciones asociadas aprovechando Azure Data Factory y Databricks.\n- Trabajar con Synapse y Python durante todo el proceso.\n- Tener la posibilidad de participar de proyectos nacionales e internacionales con equipos globales inglés-parlantes.\nHabilidades requeridas:\n- Fuerte comprensión de conceptos de bases de datos y modelado de datos.\n- Excelencia y Calidad.\n- Orientación al cliente interno/externo.\n- Trabajo en equipo.\n- Iniciativa y Proactividad.\n- Aprendizaje continuo\nExperiencia requerida:\n- Profesional titulado área informática, industrial, estadística o matemática.\n- Al menos 3 o más años de experiencia práctica en los requisitos expuestos.\n- Inglés nivel avanzado (Excluyente).\n- Experiencia comprobada en Databricks, Azure Data Factory, Python y PySpark (Excluyente)\n- Deseable Synapse\n- Deseable Certificaciones de Data Engineer en algunas de las principales nubes (GCP, AWS, Azure), Snowflake, Databricks, entre otros.\nAlgunos de nuestros beneficios…\n- Jornada de trabajo hibrida.\n- Dia de cumpleaños libre.\n- Dia libre por mudanza.\n- Actividades de voluntariado.\n- Permiso sin goce de sueldo “Cumple tus sueños” (año sabático).\n- Deloitte Days (5 días libres al año).\n- Cuida de los tuyos, licencias para el cuidado de tus familiares.\n- Equipos deportivos\n¿Qué impacto quieres hacer? \nNuestras ofertas laborales están abiertas a todos quienes, dentro del marco de la Ley de Inclusión, quieran formar parte de esta gran Firma, aportando con sus distintas capacidades y fortalezas, tanto humanas como profesionales.\n*Deloitte no efectúa ningún tipo de cobro por participar en los procesos de reclutamiento y selección y todas nuestras comunicaciones siempre son realizadas desde correos con dominio Deloitte”.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "consulting,_information_technology,_and_project_management",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4221350232/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "Yes",
         "3+",
         "Yes",
         "Azure",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Spark knowledge']"
        ],
        [
         "16",
         "Data Engineer - LATAM (100% Remoto)",
         "Imagemaker",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4219332744/",
         "Company Description: Imagemaker\nJob Description: ¿Estás en búsqueda de nuevos desafíos? Únete a Imagemaker y sigue creciendo con nosotros como Senior Data Engineer en uno de nuestros grandes clientes.\nSerá responsable de diseñar, construir, optimizar y mantener la infraestructura de datos, asegurando el flujo eficiente de información desde diversas fuentes internas y externas.\nTrabajará con grandes volúmenes de datos, integrando y procesando información crítica para apoyar la toma de decisiones y mejorar la eficiencia operativa y la experiencia del cliente.\nLa vacante es en modalidad remota, probablemente se pida ir una vez al mes a reunión pero, en su mayoría es remota.\nBuscamos seniority tanto Semi Senior como Senior. No te pierdas esta gran oportunidad\nKey Responsibilities\nDiseño y construcción de tuberías de datos.\nOptimización de la infraestructura de datos.\nGestión de datos en tiempo real.\nProveer acceso a datos de calidad para apoyar iniciativas de análisis predictivo, inteligencia artificial y machine learning.\nImplementar mecanismos de seguridad, calidad y trazabilidad de datos.\nSkills, knowledge & expertise\nExperiencia en GCP (excluyente)\nExperiencia en Python\nExperiencia en POO\nExperiencia en SQL (optimización y consultas avanzadas)\nExperiencia en modelamiento, integración y procesamiento de datos.\nExperiencia en BigQuery\nExperiencia en ETL\nExperiencia en Apache Airflow\nExperiencia en Dataflow\nExperiencia en Git\nConocimientos en Terraform\nConocimientos en Jenkins\nConocimientos en CI/CD\nJob benefits\nSer maker es cool: tenemos muy buenos beneficios y muchas actividades para divertirnos!\n️ Don’t worry, be happy: 3 días libres al año adicionales a tus vacaciones.\nPermiso sin goce de sueldo para cumplir tus sueños.\nPrograma de bienestar enfocado a equilibrar el trabajo y la vida personal.\nSeguro Complementario 100% gratuito para makers.\n¡Programas de formación, clases de inglés y mucho más!\nDía libre para tu cumpleaños y medio día para los cumpleaños de tus hijos.\nBonificaciones que dan respiros: fiestas patrias, navidad, matrimonio/AUC, nacimiento/adopción de hijos, etc.\nConvenios y precios preferenciales con bancos.\n3 Días adicionales para padres por nacimiento o adopción de hijo/a.\nConvenio de seguro para tus mascotas!\nNuestra cultura es horizontal, de innovación, desafiante y sobre todo, se respira mucha buena onda!\nEn el marco de nuestro compromiso con la inclusión, la siguiente vacante está abierta en el marco de la ley 21.015, te instamos a postular enviando tu CV\nLink de postulación: https://imagemaker.pinpointhq.com/postings/ef52a091-247a-4b18-8e7c-72568cda3892\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4219332744/",
         "High",
         "Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "17",
         "Data Engineer - Trainee",
         "Soluciones - Data & Analytics Consulting",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226385320/",
         "✔️\n¿Quiénes Somos?\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente.\nA través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros\n✔️ Responsabilidades del Cargo:\n- Mantener y generar nuevos Desarrollos sobre SQL Server y Pentaho.\n✔️ Requisitos:\n- Ingeniero (a) en informática o afín\n- Haber realizado tu practica en el área de datos \n- Disponibilidad para trabajar en modalidad Hibrida ( 3 días presencial 2 remoto)\n- Lugar de trabajo Las Condes.\n✔️ Conocimientos Obligatorios:\n- SQL\n- PL/SQL\n- ETL\n✔️ Conocimientos Deseables:\n- Pentaho\n✔️ Ofrecemos\n- Seguros complementario de salud\n- Rutas de estudios\n- Día libre cumpleaños\n- Reajuste salarial anual según variación del IPC        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology_and_consulting",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4226385320/",
         "Medium",
         "Junior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes']"
        ],
        [
         "18",
         "Data Engineer Azure DataBricks",
         "Accenture Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4157214213/",
         "WORK AT THE HEART OF THE CHANGE\nEmprende una Carrera que te inspire, súmate a nuestra área de \nAccenture Technology \ny sé parte del cambio!\n“En Accenture vivimos el futuro hoy, porque no solo aplicamos tecnología, la creamos. Aquí construimos el futuro”. \nCombinemos tu ingenio con las últimas tecnologías y sé parte del impacto positivo que puedes generar.\nTe invitamos a abrir las puertas del mundo digital donde podrás desarrollar tu talento para crecer, aprender y certificarte en las tecnologías más avanzadas, proporcionar innovación continua, ágil y participar de nuevos negocios.\nCada día, en todo el mundo, nuestros equipos innovan para crear un cambio significativo. Sé parte de Accenture!\nNos encontramos en búsqueda de profesionales para el role de \nData Engineer.\nSé parte de nuestro equipo dinámico mientras nos embarcamos en un viaje para convertirnos en expertos en Ingeniería de Datos. Ayudarás a darle forma al futuro colaborando, gestionando e interactuando con varios equipos para proporcionar soluciones innovadoras y contribuir a las decisiones clave.\n ¡Vamos a crear juntos un mundo basado en datos!\n¿Qué te hará tener éxito? 🚀\nEntre 1 a 3 años de experiencia en Data Engineering.\nExperiencia en DataBricks, DataLake, Data Factory, Azure DevOps.\nLenguajes de programación y frameworks: Python, SQL.\nSoftskills: habilidades de comunicación para trabajar de cara a cliente.\n¿Por qué elegir Accenture? \nUn lugar de trabajo único, descubre algunos de los beneficios que tenemos para ti\n:\n💪 Desarrollo de carrera\n💯 Jornadas Flex\n📚 +40 mil capacitaciones y cursos disponibles (online y presencial)\n📒Bibliotecas, libros y podcasts\n🗣️Programa de idiomas\n⭐ ¡Certificaciones gratuitas mediante nuestros partners! +900 certificados en Chile\n🤖GenAI Academy, con programas exclusivos para Accenture.\n⭐⭐Reconocidos por Great Place To Work Chile 2023 en el puesto #10 entre las mejores compañías para trabajar de más de 1.000 colaboradores. ¡Sí, estamos en el Top 10 de Chile!\n🎉 ¡Experiencia de onboarding global! +6.000 personas recibidas en el Metaverso a nivel LATAM\n🏆 Bonos y aguinaldos\n👩‍⚕️ Seguro complementario de salud (sin deducible ni copago)\n🎂 Día de cumpleaños libre\n👨‍👩Licencias de Paternidad & Maternidad Extendida\n🌎 Red global de conocimiento\n🌈Elegida la compañía más diversa e inclusiva del mundo, según el Índice de Diversidad e Inclusión de Refinitiv\n♻️Sostenibilidad, un motor de cambio, conoce nuestro compromiso\nSobre nosotros: \n 733K colaboradores a nivel global.\n 9K clientes en 120 países\n + de 1.900 Talentos en Chile.\n Proyectos en diversas industrias        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4157214213/",
         "Medium",
         "Mid-Senior",
         "No",
         "Yes",
         "1+",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes'\n 'Version control (GIT or similar)' 'Spark knowledge']"
        ],
        [
         "19",
         "Data Engineer GCP",
         "Soluciones - Data & Analytics Consulting",
         "Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4219126754/",
         ".✔️\n¿Quiénes Somos?\nSomos una consultora enfocada en Data & Analytics y contamos con más de 20 años de experiencia y exitosa participación en implementación de proyectos de pequeña, mediana y gran envergadura. Nuestro equipo, compuesto por consultores multidisciplinarios expertos y certificados, ha entregado soluciones tecnológicas de calidad que exceden las expectativas de cada cliente. A través de una metodología flexible y adaptable, logramos entregar soluciones adecuadas a la realidad de cada organización, satisfaciendo los estándares de cada una de las empresas que confían en nosotros.\n \n✔️¿Qué harás ?\n- Migración de artefactos (procedure, view) SQL Netezza a GCP BigQuery\n- Migración de datos históricos\n- Creación de Flujos con GCP Dataform\n- Pruebas unitarias, Pruebas funcionales, cuadraturas de datos origen v/s destino\n- Versionamiento de artefactos\n- Despliegue productivo (CI/CD)\n- Análisis de datos\n- Optimización de queries\n \n✔️¿Qué se requiere ?\n- Conocer la consola de GCP y sus servicios (GCP BigQuery y DataForm).\n- Entendimiento de Modelos de Base de Datos Relacional y Dimensional\n- Integración de Datos y Analítica de Datos\n- Ejecución de pruebas unitarias y funcionales.\n- Resolución de problemas de aplicaciones, bases de datos y rendimiento del sistema.\n- Capacidad para realizar evaluaciones comparativas del rendimiento de la base de datos, análisis de conducta y análisis de causa raíz.\n- Capacidad para analizar planes de ejecución y realizar mejoras a objetos de base de datos.\n- Conocimientos de desarrollo PL/SQL (DDL/DML)\n- Versionamiento con Git\n- Python\n✔️ Se valora tu experiencia en \n- Base de datos Netezza o Teradata\n- Cloud Composer\n- Ctrl-M\n- Metodologías de Desarrollo de Proyectos Agiles        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting_and_information_services",
         "https://www.linkedin.com/jobs/view/4219126754/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'CI/CD' 'Migration'\n 'Version control (GIT or similar)']"
        ],
        [
         "20",
         "Data Engineer Jr",
         "SII Group Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-05 00:00:00",
         "https://www.linkedin.com/jobs/view/4223983495/",
         "🚀 ¡En SII Group estamos en búsqueda de un/a Data Engineer! 💻✨\nMisión🎯\nBuscamos un Data Engineer con experiencia en proyectos de Business Intelligence, Big Data y Cloud Computing, para desarrollar y gestionar soluciones de datos en la nube.\n¿Cuáles son los requisitos?\n 🛠️\n- Experiencia en Python, SQL y herramientas de Cloud Computing (Azure)\n- Conocimientos en Linux, Shell Scripting, Git, GitLab y CI/CD.\n- Dominio de herramientas de procesamiento de datos: Pandas, SQL, Spark, Apache Beam, Hadoop, DataFlow, Big Query.\n- Experiencia en orquestación de procesos con Airflow.\n- Conocimientos en IaC (Terraform) y herramientas de BI como Tableau o Looker Studio.\n¿Qué harás?\n 💡\n- Desarrollo e implementación de soluciones para procesamiento y análisis de datos.\n- Gestión de datos en plataformas en la nube.\n- Optimización de procesos y flujos de trabajo.\n- Creación de informes y dashboards de BI.\nModalidad\n- Hibrido, 2 veces presencial en las oficinas ubicadas en Las Condes.\nAcerca de SII Group\nSII Group es un proveedor global de servicios TI con más de 16,000 profesionales. Nuestra misión es acompañar a nuestros clientes en su viaje digital, con un enfoque en la innovación y el desarrollo profesional. ¡Somos una de las mejores empresas para trabajar! 🌟\n“Está vacante está disponible para personas con discapacidad de acuerdo a la Ley 21.015, por lo que, si presentas una discapacidad y estás interesado/a en la posición, te agradeceremos nos indiques las adecuaciones necesarias para que tu proceso de entrevista sea óptimo”.\nEn SII Group Chile\n⏰ Tu tiempo nos importa: días administrativos libres.\n🏆 Queremos que te desarrolles: Programas de capacitaciones a tu medida.\n🚀 Ser #fungineer es fun: ¡FunFridays y eventos varios!\n💡Anímate a formar parte del Team enviándonos tu cv con pretensiones de renta a: \nnatalia.lara@siigroup.cl / seleccion@siigroup.cl        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4223983495/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'CI/CD'\n 'Data analysis or visualization'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)' 'Spark knowledge']"
        ],
        [
         "21",
         "Data Engineer Junior",
         "Isapre Consalud",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226383768/",
         "Trabajamos para brindar tranquilidad y seguridad en el acceso a la salud a más de 500 mil personas cada día ✨✨\nPara lograrlo contamos con el respaldo de la Cámara Chilena de la Construcción y un gran equipo de personas comprometidas con el bienestar de Chile. Tenemos la convicción de que es posible acceder a una experiencia de salud y bienestar superior que se articula de manera integral en torno a las necesidades.\nNos desafiamos a transformar la relación de las personas con el sistema a través de un vínculo colaborativo simple y transparente, que entrega soluciones efectivas y oportunas.\nNuestra Subgerencia Inteligencia de Negocios está en búsqueda de un profesional para el cargo de \nData Engineer Junior, \ndonde serás responsable de definir, configurar y construir sistemas, arquitecturas y plataformas de datos para asegurar la confiabilidad y calidad de grandes volúmenes de datos.\nEntre sus principales funciones se encuentran:\n- Implementar proyectos de Gestión de la información y garantizar que cumplan los lineamientos establecidos a partir de la estrategia de los datos.\n- Diseño de sistemas de Procesamiento de Datos, usando métodos y herramientas apropiadas para la gestión de la información y los modelamientos de datos para soluciones de procesamiento de datos.\n- Contribuir a la política de selección de los componentes de la arquitectura analítica.\n- Evaluar y realizar el análisis de impacto en las principales opciones de diseño y gestionar los riesgos asociados\nLos principales requisitos se encuentran:\n- Titulo profesional de Ingeniero en Computación y/o Informática (Ejecución o Civil); Otras carreras afines.\n- Conocimientos en modelamiento de Datos, Data Warehouse, SQL, ETL (ODI), Inteligencia de Negocios, OBI, Tableau, AWS, Python.\n- Deseable: Machine Learning.\n- 1 año de experiencia en cargos similares\n✨En Consalud promovemos el valor de ser tú mismo, por lo que impulsamos la inclusión laboral y diversidad bajo la Ley 21.015✨ Nos enfocamos en el equilibrio de tu vida personal y laboral, por lo que trabajamos 40 horas semanales.\n🚀🚀Súmate a nuestro equipo y sigamos protagonizando el ecosistema de salud de nuestro país desde adentro 💼\n❗️Toda la información recopilada durante el proceso de postulación será utilizada solamente para fines de selección❗️        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology",
         "banking_and_health_and_human_services",
         "https://www.linkedin.com/jobs/view/4226383768/",
         "Medium",
         "Junior",
         "Yes",
         "No",
         "1+",
         "No",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Knowledge of Machine Learning or MLOps']"
        ],
        [
         "22",
         "Data Engineer MACHBANK",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-04-28 00:00:00",
         "https://www.linkedin.com/jobs/view/4219005225/",
         "El rol de Senior Data Engineer en el equipo de Data Risk tiene como misión habilitar y escalar la oferta de crédito de MACH mediante el diseño e implementación de servicios de datos que soporten políticas de riesgo y decisiones crediticias. Buscamos un perfil con experiencia técnica sólida para liderar la definición y evolución de pipelines de datos robustos y eficientes, transformando requerimientos de negocio en soluciones arquitectónicas innovadoras y sostenibles. Además, el rol requiere autonomía y capacidad de articulación con stakeholders técnicos y no técnicos, con el objetivo de crear soluciones integrales que mejoren la experiencia y acceso al crédito de los usuarios.\nEn este rol tendrás la oportunidad de:\nArquitectura y desarrollo de soluciones de datos:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nBuenas prácticas y calidad técnica:\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nColaboración y alineamiento con el negocio:\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de riesgo del banco, alineando objetivos técnicos con las metas de negocio.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los elementos clave del riesgo de crédito, para orientar los desarrollos hacia soluciones que permitan una gestión adecuada del riesgo.\nComprender los principales desafíos relacionados al riesgo no financiero, con especial foco en prevención de fraude, para aportar en el diseño de soluciones que mitiguen dichos riesgos.\nLiderazgo técnico y mentoring:\nMentorear a perfiles con menos seniority del equipo, compartiendo conocimientos, buenas prácticas y promoviendo una cultura de aprendizaje continuo.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nExperiencia profesional:\nAl menos 3 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 2 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como ingenieros/arquitectos de datos y desarrolladores de software.\nExperiencia en el sector bancario o financiero, con entendimiento de conceptos clave del negocio.\nConocimientos técnicos:\nConocimientos sólidos de arquitectura de software, especialmente en el diseño y construcción de servicios de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos y optimización de consultas.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o Bitbucket.\nFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nEs aún mejor si tienes:\nConocimiento en riesgo de crédito, tanto a nivel de modelos como en las políticas del proceso de otorgamiento de crédito.\nConocimiento de riesgo no financiero, con foco en fraude y cumplimiento normativo.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4219005225/",
         "High",
         "Senior",
         "No",
         "No",
         "3+",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)' 'APIs']"
        ],
        [
         "23",
         "Data Engineer MACHBANK",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-08 00:00:00",
         "https://www.linkedin.com/jobs/view/4226352405/",
         "El rol de Senior Data Engineer en el equipo de Data Risk tiene como misión habilitar y escalar la oferta de crédito de MACH mediante el diseño e implementación de servicios de datos que soporten políticas de riesgo y decisiones crediticias. Buscamos un perfil con experiencia técnica sólida para liderar la definición y evolución de pipelines de datos robustos y eficientes, transformando requerimientos de negocio en soluciones arquitectónicas innovadoras y sostenibles. Además, el rol requiere autonomía y capacidad de articulación con stakeholders técnicos y no técnicos, con el objetivo de crear soluciones integrales que mejoren la experiencia y acceso al crédito de los usuarios.\nEn este rol tendrás la oportunidad de:\nArquitectura y desarrollo de soluciones de datos:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nBuenas prácticas y calidad técnica:\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nColaboración y alineamiento con el negocio:\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de riesgo del banco, alineando objetivos técnicos con las metas de negocio.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los elementos clave del riesgo de crédito, para orientar los desarrollos hacia soluciones que permitan una gestión adecuada del riesgo.\nComprender los principales desafíos relacionados al riesgo no financiero, con especial foco en prevención de fraude, para aportar en el diseño de soluciones que mitiguen dichos riesgos.\nLiderazgo técnico y mentoring:\nMentorear a perfiles con menos seniority del equipo, compartiendo conocimientos, buenas prácticas y promoviendo una cultura de aprendizaje continuo.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nExperiencia profesional:\nAl menos 3 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 2 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como ingenieros/arquitectos de datos y desarrolladores de software.\nExperiencia en el sector bancario o financiero, con entendimiento de conceptos clave del negocio.\nConocimientos técnicos:\nConocimientos sólidos de arquitectura de software, especialmente en el diseño y construcción de servicios de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos y optimización de consultas.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o Bitbucket.\nFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nEs aún mejor si tienes:\nConocimiento en riesgo de crédito, tanto a nivel de modelos como en las políticas del proceso de otorgamiento de crédito.\nConocimiento de riesgo no financiero, con foco en fraude y cumplimiento normativo.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4226352405/",
         "High",
         "Senior",
         "No",
         "No",
         "3+",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)' 'APIs']"
        ],
        [
         "24",
         "Data Engineer MACHBANK (Plazo Fijo)",
         "MACHBANK",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4221378934/",
         "El rol de Data Engineer en el equipo de Data Experience tiene como misión principal habilitar experiencias WOW basadas en datos e inteligencia artificial en MACHBANK, colaborando con los equipos de Customer Success, Customer Experience y Product. Buscamos un perfil con experiencia técnica, capaz de liderar la definición, construcción y evolución de procesos de datos robustos, eficientes y alineados con estándares de calidad y escalabilidad. Esta persona será clave en la transformación de requerimientos de negocio en soluciones de arquitectura de datos y machine learning, permitiendo potenciar la experiencia que viven nuestros clientes en la App y en nuestros canales asistidos.\nEn este rol tendrás la oportunidad de:\nDiseñar e implementar arquitecturas de datos y software robustas, escalables y alineadas a las necesidades del negocio.?\nDiseñar e implementar servicios de datos que habiliten políticas de riesgo y decisiones crediticias.\nDesarrollar arquitecturas de servicios de datos en tiempo real que permitan decisiones ágiles y seguras en procesos clave del negocio.\nParticipar activamente en el diseño y evolución de modelos de datos.\nDesarrollar pipelines de datos eficientes, mantenibles y de alto rendimiento.\nOptimizar procesos de ingesta, transformación y exposición de datos, considerando performance, escalabilidad y costos operacionales.\nDefinir y promover patrones de desarrollo, estándares técnicos y lineamientos de arquitectura dentro del equipo.\nImplementar y velar por buenas prácticas de ingeniería: control de versiones, testing automatizado, revisión de código, integración y despliegue continuo (CI/CD).\nParticipar en revisiones de código para asegurar la calidad y consistencia técnica de las soluciones.\nGenerar y mantener documentación técnica clara y actualizada sobre los desarrollos y procesos clave.\nTrabajar de forma colaborativa con equipos de producto, analítica, backend y áreas de MACHBANK.\nApoyar la toma de decisiones estratégicas a partir del análisis y uso eficiente de los datos.\nEntender los dolores de nuestros clientes para apoyar la búsqueda de nuevos casos de usos de experiencia basada en datos.\nModalidad de trabajo del cargo:\nMixto, 4 días remotos semanales\nPara tener éxito en esta posición necesitas:\nAl menos 2 años de experiencia trabajando en equipos de desarrollo de soluciones basadas en datos.\nAl menos 1 años de experiencia desarrollando soluciones de datos en infraestructura cloud (AWS, GCP o Azure).\nExperiencia previa en proyectos implementados en la nube, colaborando con perfiles como devops, ingenieros de software, data scientists y machine learning engineers.\nConocimientos sólidos de arquitectura e ingeniería de software, especialmente en el diseño y construcción de servicios escalables basados en alto volumen y/o velocidad de datos.\nExperiencia con infraestructura cloud y servicios asociados a la construcción de pipelines y almacenamiento de datos (por ejemplo: S3, Lambda, DynamoDB, etc.).\nDominio de SQL avanzado, incluyendo modelado de datos, optimización de consultas y análisis de datos.\nManejo fluido de Python (o lenguaje similar) para procesamiento de datos y automatización de tests.\nConocimiento en diseño y operación de soluciones de procesamiento batch y en tiempo real, y su persistencia en entornos cloud.\nExperiencia con orquestador de flujos de trabajos (por ejemplo: Apache Airflow).\nExperiencia utilizando sistemas de control de versiones como GitHub o BitbucketFamiliaridad con automatización de pruebas y flujos de desarrollo basados en buenas prácticas (CI/CD, testing, code review).\nConocimiento de las técnicas y metodologías de Machine Learning.\nEs aún mejor si tienes:\nConocimiento en LLM (Large Language Models) e Inteligencia Artificial, incluyendo patrones de diseños y prompting.\nConocimiento en frameworks de procesamiento distribuido como Spark.\nExperiencia en el desarrollo de experiencias potenciada con datos e inteligencia artificial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4221378934/",
         "High",
         "Mid-Senior",
         "No",
         "No",
         "2+",
         "No",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'Data quality'\n 'Knowledge of Machine Learning or MLOps' 'CI/CD'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)' 'APIs' 'Spark knowledge']"
        ],
        [
         "25",
         "Data Engineer Senior",
         "Equifax",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4144902157/",
         "Como Data Engineer, estarás a cargo de integrar, consolidar y estructurar los datos, apoyándote en un las mejores prácticas para manejar, mantener y mejorar nuestras soluciones.\n \n \n ¿Qué vas a hacer? \n \n \n-  Evaluar, gestionar y resolver incidentes. \n-  Realizar cálculos en linea utilizando Python. \n-  Ejecución de pruebas. \n \n \n ¿Qué experiencia necesita? \n \n \n-  + 4 años de experiencia trabajando en Python. \n-  + 4 años de experiencia BigQuery. \n-  + 4 años de experiencia trabajando con Apache Beam y GitHub. \n \n \n ¿Qué podría diferenciarte? \n \n \n-  Poseer alguna certificación de Google en Data Engineer. \n-  Experiencia trabajando con Airflow. \n-  Experiencia Trabajando con Jmeter. \n-  Experiencia trabajando con Unit Test. \n-  Experiencia trabajando con PySpark. \n-  Experiencia trabajando con CI/DF. \n-  Experiencia trabajando con Postman.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4144902157/",
         "Medium",
         "Senior",
         "No",
         "Yes",
         "1+",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)' 'CI/CD'\n 'Version control (GIT or similar)' 'APIs' 'Spark knowledge']"
        ],
        [
         "26",
         "Data Engineer with GCP Senior-Level or Expert-Level",
         "Globant",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4222832762/",
         "At Globant, we are working to make the world a better place, one step at a time. We enhance business development and enterprise solutions to prepare them for a digital future. With a diverse and talented team present in more than 30 countries, we are strategic partners to leading global companies in their business process transformation.\nWe seek a \nData Engineer with GCP Senior-Level or Expert-Level\n who shares our passion for innovation and change. This role is critical to helping our business partners evolve and adapt to consumers' personalized expectations in this new technological era.\nWhat will help you succeed:\n- Fluent English (B2 - Upper Intermediate)\n- Spark\n- Cloud\n- GCP is a plus\nThis job can be filled in \nlocation \nCreate with us digital products that people love. We will bring businesses and consumers together through AI technology and creativity, driving digital transformation to positively impact the world.\nAt Globant, we believe in fostering a diverse and inclusive workplace where everyone feels valued and respected. We are an Equal Opportunity Employer committed to creating a thriving and inclusive environment for all employees and candidates, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, veteran status, or any other legally protected characteristic. If you need any assistance or accommodations due to a disability, please let us know by applying through our Career Site or contacting your assigned recruiter.\nWe may use AI and machine learning technologies in our recruitment process. Compensation is determined based on skills, qualifications, experience, and location. In addition to competitive salaries, we offer a comprehensive benefits package. Learn more about our commitment to diversity and inclusion and \nGlobant’s Benefits.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4222832762/",
         "Low",
         "Senior",
         "No",
         "No",
         "Not Specified",
         "Yes",
         "GCP",
         "['Spark knowledge' 'None of the above mentioned skills']"
        ],
        [
         "27",
         "Data Engineer – Amazon Web Services (AWS)",
         "Deloitte",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4218749215/",
         "¿Buscas generar impactos significativos con tu trabajo? ¿Te interesa la innovación y desarrollar tus habilidades de liderazgo?\nEn Deloitte, encontrarás un espacio intelectualmente desafiante, diverso, inclusivo, y comprometido con su entorno, que potenciará el camino profesional que hoy desees emprender.\nCon un equipo multidisciplinario y una red global de profesionales expertos que comparten las mejores prácticas y experiencias del mercado, Deloitte se destaca por la entrega de un servicio de excelencia, conectado con las últimas tendencias, que ofrece a sus colaboradores/as un espacio de aprendizaje único donde se incentiva el liderazgo en todo nivel.\nNos encontramos en búsqueda de un Data Engineer especializado en Amazon Web Services (AWS) con experiencia en Data Mesh para integrarse a nuestra línea de Data Engineering.\nNuestro equipo de Data Engineering utiliza las herramientas y servicios más avanzados para diseñar soluciones escalables y eficientes en la nube de AWS, con un enfoque en arquitecturas modernas como Data Mesh.\nDentro de las principales funciones se destacan:\n- Serás responsable de entregar soluciones eficientes y escalables para el procesamiento de datos utilizando AWS y arquitecturas de Data Mesh. Tu rol implicará:\n- Diseñar, construir y mantener sistemas de procesamiento de datos escalables y de alto rendimiento en AWS, alineados con los principios de Data Mesh.\n- Desarrollar y mantener pipelines de datos para la extracción, transformación y carga (ETL) de datos desde diversas fuentes hacia AWS, en un entorno de Data Mesh.\n- Implementar soluciones de almacenamiento y procesamiento de datos utilizando servicios de AWS, asegurando la integración y la interoperabilidad entre dominios de datos en un enfoque de Data Mesh.\n- Colaborar con equipos multidisciplinarios para entender los requisitos de negocio y diseñar soluciones de datos adecuadas, asegurando la adopción de Data Mesh.\n- Optimizar el rendimiento de los sistemas de procesamiento de datos y garantizar la integridad de los datos en AWS bajo los principios de Data Mesh.\nHabilidades requeridas:\n- Sólida comprensión de conceptos de bases de datos, modelado de datos y arquitecturas de datos distribuidas.\n- Experiencia en la implementación de arquitecturas de Data Mesh en AWS, utilizando servicios como S3, Redshift, Glue, Lambda y otros.\n- Conocimiento de cómo construir y gestionar dominios de datos dentro de un Data Mesh utilizando las herramientas de AWS.\n- Experiencia en el diseño y desarrollo de pipelines ETL en AWS, alineados con los principios de Data Mesh.\n- Conocimiento de seguridad de datos y cumplimiento normativo en AWS.\n- Habilidades de resolución de problemas.\n- Pensamiento analítico y crítico.\n- Experiencia con herramientas de control de versiones como Git.\n- Familiaridad con la gestión de metadatos y la gobernanza de datos en un entorno de Data Mesh.\nExperiencia requerida:\n- Profesional titulado en informática, ingeniería, estadística o matemáticas.\n- Inglés nivel B2 intermedio/avanzado (Excluyente).\n- 2 o más años de experiencia práctica en Data Engineering con AWS, con un enfoque en arquitecturas de Data Mesh.\n- Certificaciones en AWS.\n- Experiencia en la implementación y gestión de Data Mesh utilizando herramientas de AWS como Glue, S3, Redshift, Kinesis y Lambda.\n- Experiencia en la programación de pipelines de datos con lenguajes como Python, Java o Scala en AWS.\n- Conocimiento en el uso de bases de datos relacionales y NoSQL en AWS.\n- Experiencia con herramientas de orquestación de workflows como Airflow.\n- Experiencia trabajando con metodologías ágiles (Scrum, Kanban, Lean, etc.) (deseable).\nAlgunos de nuestros beneficios…\n- Jornada de trabajo hibrida.\n- Dia de cumpleaños libre.\n- Dia libre por mudanza.\n- Actividades de voluntariado.\n- Permiso sin goce de sueldo “Cumple tus sueños” (año sabático).\n- Deloitte Days (5 días libres al año).\n- Cuida de los tuyos, licencias para el cuidado de tus familiares.\n- Equipos deportivos.\n¿Qué impacto quieres hacer? \nNuestras ofertas laborales están abiertas a todos quienes, dentro del marco de la Ley de Inclusión, quieran formar parte de esta gran Firma, aportando con sus distintas capacidades y fortalezas, tanto humanas como profesionales.\n*Deloitte no efectúa ningún tipo de cobro por participar en los procesos de reclutamiento y selección y todas nuestras comunicaciones siempre son realizadas desde correos con dominio Deloitte”.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "consulting,_information_technology,_and_project_management",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4218749215/",
         "High",
         "Mid-Senior",
         "Yes",
         "Yes",
         "2+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data governance' 'Version control (GIT or similar)'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)']"
        ],
        [
         "28",
         "Data Engineer – Google Cloud Platform (GCP)",
         "Deloitte",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4218749139/",
         "¿Buscas generar impactos significativos con tu trabajo? ¿Te interesa la innovación y desarrollar tus habilidades de liderazgo?\nEn Deloitte, encontrarás un espacio intelectualmente desafiante, diverso, inclusivo, y comprometido con su entorno, que potenciará el camino profesional que hoy desees emprender.\nCon un equipo multidisciplinario y una red global de profesionales expertos que comparten las mejores prácticas y experiencias del mercado, Deloitte se destaca por la entrega de un servicio de excelencia, conectado con las últimas tendencias, que ofrece a sus colaboradores/as un espacio de aprendizaje único donde se incentiva el liderazgo en todo nivel.\nNos encontramos en búsqueda de las y los mejores profesionales que quieran integrar nuestra línea de Data Engineering en Google Cloud Platform (GCP).\nNuestro equipo de Data Engineering utiliza el poder de los datos y las tecnologías más avanzadas para ofrecer soluciones eficientes y escalables.\nDentro de las principales funciones se destacan:\n- Serás responsable de entregar soluciones eficientes, robustas y escalables en GCP. Tu rol implicará:\n- Diseñar, construir y mantener sistemas de procesamiento de datos escalables y de alto rendimiento en GCP.\n- Desarrollar y mantener pipelines de datos para la extracción, transformación y carga (ETL) de datos desde diversas fuentes en GCP.\n- Implementar soluciones para el almacenamiento y procesamiento eficiente de grandes volúmenes de datos utilizando las herramientas y servicios de GCP.\n- Colaborar con equipos multidisciplinarios para entender los requisitos y diseñar soluciones adecuadas en el contexto de GCP.\n- Optimizar el rendimiento de los sistemas de procesamiento de datos y garantizar la integridad de los datos en GCP.\nHabilidades requeridas:\n- Fuerte comprensión de conceptos de bases de datos y modelado de datos.\n- Competencia en SQL, experiencia en el diseño y optimización de consultas.\n- Conocimiento y experiencia en Google Cloud Platform (GCP).\n- Experiencia en herramientas de procesamiento de datos como BigQuery, Dataflow, Pub/Sub y Dataproc.\n- Habilidades de resolución de problemas.\n- Pensamiento analítico y crítico.\n- Conocimiento de seguridad de datos y cumplimiento normativo en GCP (deseable).\n- Experiencia con herramientas de control de versiones como Git.\nExperiencia requerida:\n- Profesional titulado en informática, ingeniería, estadística o matemáticas.\n- Inglés nivel B2 intermedio/avanzado (Excluyente).\n- 2 o más años de experiencia práctica en Data Engineering en GCP.\n- Certificaciones en Google Cloud Platform (GCP) para Data Engineer.\n- Experiencia sobre Apache Spark, Hadoop en el contexto de GCP.\n- Experiencia en consumo de datos mediante servicios de GCP como Pub/Sub, BigQuery, o Dataflow.\n- Experiencia en el uso de lenguajes de programación como Python, Java o Scala para el desarrollo de pipelines de datos en GCP.\n- Experiencia con herramientas de orquestación de workflows (Airflow, Luigi) en GCP.\n- Experiencia trabajando en proyectos con metodologías ágiles (Scrum, Kanban, Lean, etc.) (deseable).\nAlgunos de nuestros beneficios…\n- Jornada de trabajo hibrida.\n- Dia de cumpleaños libre.\n- Dia libre por mudanza.\n- Actividades de voluntariado.\n- Permiso sin goce de sueldo “Cumple tus sueños” (año sabático).\n- Deloitte Days (5 días libres al año).\n- Cuida de los tuyos, licencias para el cuidado de tus familiares.\n- Equipos deportivos.\n¿Qué impacto quieres hacer? \nNuestras ofertas laborales están abiertas a todos quienes, dentro del marco de la Ley de Inclusión, quieran formar parte de esta gran Firma, aportando con sus distintas capacidades y fortalezas, tanto humanas como profesionales.\n*Deloitte no efectúa ningún tipo de cobro por participar en los procesos de reclutamiento y selección y todas nuestras comunicaciones siempre son realizadas desde correos con dominio Deloitte”.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "consulting,_information_technology,_and_project_management",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4218749139/",
         "High",
         "Mid-Senior",
         "Yes",
         "Yes",
         "2+",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Version control (GIT or similar)' 'Spark knowledge'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)']"
        ],
        [
         "29",
         "INGENIERO DATOS SR",
         "LATAM Airlines",
         "Chile",
         "2025-04-28 00:00:00",
         "https://www.linkedin.com/jobs/view/4217041945/",
         "✈️ En \nLATAM \nte invitamos a elevar cada viaje, siempre. ¿Te gustaría ser parte de este propósito? 🌎¡Entonces esta oportunidad puede ser para ti!\n📢Estamos buscando nuestro próxim@ \nINGENIERO DE DATOS SR \npara la \nDirección de Estrategia de Datos \nen Santiago de Chile, quién será el responsable de liderar la implementación de soluciones avanzadas para la gestión y organización del ecosistema de datos, impulsando la construcción de Data Lakes y Data Lakehouses, además estará promoviendo enfoques modernos como Medallion y Data Mesh. Adicionamente deberá asegurar el desarrollo de modelos de datos analíticos estructurados alineados con las necesidades estratégicas del negocio, con una sólida estrategia de gobernanza de datos que facilite su uso y aprovechamiento en toda la organización.\n✨ En \nLATAM \ncreemos en un lugar de trabajo diverso, inclusivo y lleno de oportunidades. Esta posición está abierta a postulaciones de personas en situación de discapacidad, en el marco de la Ley de Inclusión Laboral en Chile.\n🚀 Aquí no hay rutina, cada día trae un nuevo reto. Entre tus principales responsabilidades estarán:\n- Liderar la definición y adopción de enfoques innovadores que optimicen la organización, integración y uso de los datos en la organización.\n- Establecer políticas y procedimientos de gobernanza y estructuración de datos, asegurando la calidad, seguridad y disponibilidad de la información.\n- Diseñar y mantener modelos de datos eficientes y escalables en conjunto con diversos equipos, garantizando su alineación con los objetivos estratégicos de la organización.\n- Desarrollar y aplicar políticas específicas para la creación, almacenamiento, uso, archivado y eliminación de datos, asegurando su alineación con la estrategia general de datos de la organización.\n🎯 ¿Qué necesitas para despegar en este rol?\n- Ser profesional de las carreras de Ingeniería Civil Informática, Computación, Eléctrica, Electrónica, Industrial o áreas afines.\n- Es indispensable que cuentes con al menos 3 años de experiencia en roles similares en áreas de datos.\n- Si te manejas al derecho y al revés con SQL (avanzado), Python (avanzado) y Modelamiento y Gobernanza de Datos ya estás un paso más adelante porque para esta posición ¡Es muy necesario!\n- We need you to have an advanced or intermediate level of English. If you understood this, we are on the right track.\n💼 ¿Qué te espera al ser parte de LATAM\n- Serás parte de una industria atractiva, global y multicultural.\n- Podrás conocer el mundo con nuestros beneficios de pasajes, Staff Travel.\n- Te integrarás a un ambiente dinámico, con constantes cambios y desafíos.\n- Tendrás amplias y múltiples opciones de desarrollo de carrera.\n- Programa de Bienestar que incluye Club de Descuentos con diferentes proveedores, con precios especiales para trabajadores LATAM en varios productos y servicios, en distintas partes del mundo.\n📍¿Dónde y cómo trabajarás?\n- Lugar de trabajo: Santiago de Chile.\n- Jornada: Completa.\n- Tipo de contrato: Plazo fijo de 3 meses con posibilidad de renovación.\n- Modalidad: Remoto.\n✋ Antes de postular, ten en cuenta:\n- En caso de encontrarte en una situación de discapacidad, indica si necesitas algún requerimiento para presentarte a la entrevista*\n- Recuerda crear tu perfil sólo con tu correo personal, evita el uso de correos corporativos o universitarios.\n- Carga tu CV solamente en formato PDF.\n- En LATAM \nNUNCA \nsolicitamos dinero o depósitos para participar en nuestros procesos de selección.\n¿Estás listo/a para sumarte al equipo?\n¡Postula!        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "airlines_and_aviation",
         "https://www.linkedin.com/jobs/view/4217041945/",
         "High",
         "Senior",
         "Yes",
         "No",
         "3+",
         "Yes",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data governance']"
        ],
        [
         "30",
         "Ingeniero de Datos",
         "Softline Consultores Gerenciales",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-29 00:00:00",
         "https://www.linkedin.com/jobs/view/4219332714/",
         "Company Description: WMPartner Consultores\nJob Description: WMPartner necesita contratar ingeniero de datos para ser parte del equipo de soporte y proyectos, es excluyente que cuenten con las siguientes certificaciones:\nAzure Administrator Associate certification.\nAzure Solutions Architect Expert.\nAzure Database Administrator Associate\nAzure AI Engineer Associate\nAzure Data Scientist Associate\nAzure Data Engineer Associate*\nData Analyst Associate\nDynamics 365 Customer Insights (Data) Specialty**\nAzure Cosmos DB Developer Specialty\nFabric Analytics Engineer Associate\nFabric Data Engineer Associate\nHorario de trabajo lunes a viernes de 8:30 am a 6:00 pm. Modalidad mixta de acuerdo a desempeño (presencial y remoto).\nSi te interesa y cumples con el perfil, enviar cv con pretensiones de renta y disponibilidad a [email]\n                \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4219332714/",
         "Low",
         "Mid-Senior",
         "No",
         "Yes",
         "Not Specified",
         "No",
         "Azure",
         "[]"
        ],
        [
         "31",
         "Ingeniero de Datos",
         "TecSystems",
         "Huechuraba, Santiago Metropolitan Region, Chile",
         "2025-05-12 00:00:00",
         "https://www.linkedin.com/jobs/view/4228080982/",
         "En \nTecSystems \nestamos en busca de un profesional\nINGENIERO DE DATOS\nPara unirse a nuestro equipo de expertos.\nMISIÓN DEL CARGO\n- Diseñar, construir, gestionar y optimizar la infraestructura de almacenamiento de datos de las plataformas de explotación de datos: Datawarehouse/DataMarts.\n- Definir e implementar los flujos de datos desde su extracción del origen hasta su explotación.\n- Garantizar datos accesibles y fiables, asegurando la seguridad y protección de la información.\nMODALIDAD Y CONDICIONES\n- Jornada: Lunes a Viernes Horario de Oficina\n- Modalidad: Híbrida (3 días presencial / 2 días remoto (Home Office)\n- Ubicación Oficinas: Ciudad Empresarial, Huechuraba. Santiago\n- Rango Renta Líquida Mensual: 1.400.000 - 1.600.000 (Pesos Chilenos)\nRESPONSABILIDADES DEL CARGO\n- Identificar y decidir de dónde y cómo extraer los datos, como deben ser extraídos y almacenados.\n- Desarrollar y optimizar los procesos de extracción y almacenamiento de datos para que sean confiables y de calidad.\n- Desarrollar, testear y mantener las arquitecturas de datos.\n- Identificar e implementar mejoras en temas de la calidad, eficiencia y confiabilidad de los datos.\n- Alinear la arquitectura de datos con las necesidades y objetivos comerciales de la compañía.\n- Utilizar grandes conjuntos de datos para resolver problemas comerciales.\n- Descubrir tendencias y patrones ocultos que puedan incidir en los objetivos de la compañía.\n- Interpretar los datos mediante gráficas y otros métodos visuales y no visuales.\n- Preparar datos para modelos predictivos y prescriptivos.\n- Garantizar el cumplimiento de normativas y leyes de seguridad y protección de datos\nREQUISITOS\nFormación\n:\nTitulados en Análisis de Datos, Ciencia de datos, Ingeniería de Datos, Ingeniería Civil en Informática o carreras afines.\nExperiencia\n: Mínimo 3 años de experiencia trabajando en analítica en temas relacionados a arquitectura de datos, bases de datos, ETL’s, lenguajes de programación SQL y Python, herramientas de inteligencia de negocios y automatización.\nIdiomas\n: Inglés intermedio, oral y escrito (excluyente)\nTecnologías:\n- Lenguajes: Python, SQL (avanzado), PowerShell\n- Manejo de Bases de Datos SQL y NO SQL\n- Dominio de herramientas de Azure: Azure Data Factory, Azure Synapse Analytics, Azure Event Hubs, Azure Functions, Azure Storage, Azure Monitor.\n- Experiencia con arquitecturas de datos modernas (Data Lake, Data Mesh, ELT).\n- Conocimiento de bases de datos relacionales y no relacionales (SQL Server, Cosmos DB, etc.).\n- Experiencia con herramientas de versionamiento\nCOMPETENCIAS\nHabilidades Blandas\n- Pensamiento estratégico y enfoque en la mejora continua.\n- Excelentes habilidades de comunicación y colaboración.\n- Capacidad para trabajar en entornos ágiles y multidisciplinarios.\n- Trabajo en equipo\n- Visión estratégica\n- Pensamiento crítico\n- Alta capacidad de aprendizaje\n- Alta capacidad analítica y de resolución de problemas\nHabilidades Duras\n- Análisis numérico\n- Nivel Avanzado en tecnologías de analítica “Cloud” y “On Premise” de Microsoft como MS Fabric, SQL Server, Azure SQL Server, Azure Data Factory, Azure Synapse y Power BI.\n- Familiaridad con marcos de seguridad y cumplimiento (ej. GDPR, ISO 27001).\n- Dominio de herramientas de procesamiento de Datos / ETL’s\n- Dominio de herramientas de visualización de datos\n- Dominio de herramientas de Inteligencia Artificial\n- Conocimientos en Big Data\nSi te apasiona el Análisis y Tratamiento de de Datos y estás interesado postula a este anuncio de empleo.\nBENEFICIOS estándar a nuestros colaboradores son:\n- - - - - - - - - - - SOBRE LA EMPRESA\nNuestro cliente es líder en distribución de combustibles y lubricantes tanto a nivel industrial como retail y tiene una importante participación en el negocio de tiendas de conveniencia.\nEs una compañía comprometida con la innovación, la sostenibilidad y la utilización de energías renovables, entregando soluciones integrales de eficiencia energética y electromovilidad a sus clientes.\nOfrece un amplio portafolio de productos y servicios de la más alta calidad y tecnología para clientes particulares y empresas, para el óptimo funcionamiento de sus vehículos y equipos, como también, para el apoyo a las compañías del sector industrial en la administración, seguridad, gestión y control de sus necesidades en los ámbitos de combustibles, lubricantes, asfaltos y químicos.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4228080982/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "3+",
         "Yes",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Data quality' 'Version control (GIT or similar)'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "32",
         "Ingeniero de Datos Cloud – Azure",
         "Kabeli Selección",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4222894601/",
         "En Kabeli, estamos en búsqueda de un Ingeniero de Datos Cloud - Azure para colaborar con un cliente del sector asegurador, participando en proyectos de análisis de datos estratégicos para la organización.\n¿Qué buscamos?\nProfesionales con al menos 3 a 5 años de experiencia en ingeniería de datos, idealmente en la industria de seguros, con formación en Informática, Ingeniería de Sistemas, Matemáticas, Estadística o áreas afines.\nConocimientos en:\nHabilidades Técnicas:\n• Experiencia con Azure Synapse Analytics y otros servicios de datos de Azure (Azure Data Factory, Azure Databricks, Azure SQL Database).\n• Conocimiento avanzado de SQL y habilidades en programación (Python, Scala, etc.).\n• Experiencia en la construcción y mantenimiento de ETL/ELT pipelines.\n• Familiaridad con herramientas de visualización de datos (Power BI).\n• Conocimiento de técnicas de modelado de datos y diseño de bases de datos.\nDeseable:\n• Certificaciones en Azure (Azure Data Engineer, Azure Solutions Architect).\n• Experiencia previa en análisis de datos en la industria de seguros.\nCondiciones\n• Modalidad Híbrida (3x2) Metro Manquehue\n• Jornada de lunes a viernes de 9:00 a 18:00 hrs.\n• Contrato plazo fijo con posibilidad de extensión.\n• 2 días administrativos al año.\n• Día libre en tu cumpleaños.\n• Bonos por Fiestas Patrias y Navidad.\n• Seguro complementario de salud, vida y dental.\n• Acceso a Udemy para potenciar tu desarrollo profesional.        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "information_technology",
         "staffing_and_recruiting_and_technology,_information_and_media",
         "https://www.linkedin.com/jobs/view/4222894601/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "Yes",
         "3+",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization']"
        ],
        [
         "33",
         "Ingeniero de datos",
         "Elitsoft",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4221748679/",
         "¡\nÚnete a Elitsoft!\n Somos una empresa consultora informática con más de 15 años en el mercado, y estamos en busca de nuevos talentos. Buscamos profesionales responsables, auto-gestionados, y comprometidos con la entrega de tareas de calidad en el tiempo estipulado.\nEstamos en búsqueda de un\n INGENIERO DE DATOS \nexperimentado para unirse a nuestro equipo. El candidato ideal será responsable de diseñar, construir y mantener nuestras infraestructuras de datos para asegurar que los datos sean accesibles y utilizables por toda la organización.\nDebe tener más de 5 años de experiencia en las siguientes tecnologías:\nPython\nSQL\nJava (básico\n)\nGCP:\n- Compute Engine\n- Cloud Run\n- Cloud Functions\n- App Engine\n- BigQuery\n- Cloud Storage\n- Pub/Sub\n- Cloud SQL\n- Bigtable\n- Kubernetes Engine (GKE)\n- Dataflow\nResponsabilidades:\n- Diseñar, implementar y mantener sistemas de almacenamiento y procesamiento de datos.\n- Construir y optimizar pipelines de datos para la recolección, procesamiento y análisis de grandes volúmenes de datos.\n- Colaborar con científicos de datos, analistas y otros ingenieros para entender los requisitos de datos y ofrecer soluciones adecuadas.\n- Garantizar la calidad y la integridad de los datos mediante la implementación de procesos de limpieza y validación de datos.\n- Desarrollar y mantener la documentación técnica relacionada con los sistemas de datos.\n- Monitorear el rendimiento de los sistemas de datos y realizar ajustes para mejorar la eficiencia.\n- Implementar y mantener políticas de seguridad y privacidad de datos.\n- TRABAJO REMOTO\n- Jornada completa        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4221748679/",
         "High",
         "Senior",
         "No",
         "No",
         "5+",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Collaboration with data scientists or analysts' 'Data monitoring']"
        ],
        [
         "34",
         "Ingeniero de datos",
         "SSI Outsourcing",
         "Los Andes, Valparaiso Region, Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4223028203/",
         "Ingeniero de Datos\nImagina ser parte de un equipo que transforma datos en decisiones estratégicas. En nuestra empresa, buscamos un Desarrollador de Aplicaciones / Ingeniero de Datos que no solo tenga habilidades técnicas, sino que también comparta nuestra pasión por la innovación en el sector minero. Tu misión será desarrollar soluciones que conecten, integren y visualicen datos, permitiendo que nuestros equipos operacionales tomen decisiones informadas y efectivas.\nFormación Requerida\nPara unirte a nosotros, necesitarás ser un Ingeniero de Ejecución o Civil en alguna de las siguientes áreas:\n- Informática\n- Sistemas\n- Computación\n- Carreras afines\nExperiencia Mínima Exigida\n- Buscamos personas con experiencia en el rubro minero o en áreas tecnológicas aplicadas a minería.\nRequisitos Técnicos Específicos\nDesarrollo de Soluciones\n- Lenguajes y Plataformas:\n- .NET / C#\n- SQL Server\n- Power BI\n- SharePoint\n- Azure DevOps\nGestión e Integración de Datos\nImagina poder conectar múltiples fuentes de datos y transformar esa información en conocimiento. Necesitamos que tengas experiencia con:\n- PI System\n- SAP AOP\n- Jigsaw\n- Dispatch\n- Mining Tag\n- DCS y APC\nTu habilidad para modelar y transformar datos será clave para el análisis y la visualización.\nVisualización e Interfaces\nAquí es donde tu creatividad puede brillar. Buscamos a alguien que pueda desarrollar dashboards interactivos que no solo informen, sino que cuenten una historia. Necesitarás:\n- Un buen manejo del diseño UX/UI.\n- La capacidad de traducir procesos operacionales en representaciones visuales intuitivas.\nMetodologías de Trabajo\nEn nuestro equipo, valoramos la agilidad. Aplicar metodologías como Scrum o Kanban te permitirá entregar soluciones de manera iterativa, siempre en colaboración con nuestros clientes.\nDeseables Adicionales\nSi tienes experiencia en modelos analíticos, minería de datos o machine learning, ¡queremos saber de ti! Tu capacidad para trabajar junto a áreas operacionales será fundamental para entender y traducir los requerimientos de negocio.        \n            \n                            \n            ",
         "associate",
         "full-time",
         "engineering_and_information_technology",
         "it_services_and_it_consulting_and_mining",
         "https://www.linkedin.com/jobs/view/4223028203/",
         "Medium",
         "Mid-Senior",
         "Yes",
         "No",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Data quality'\n 'None of the above mentioned skills']"
        ],
        [
         "35",
         "Ingeniero de datos",
         "Tecnoandina",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4227768803/",
         "Descripción de la Empresa:\n Tecnoandina se destaca por una cultura innovadora, colaborativa y orientada a resultados, impulsando la transformación digital e industrial con soluciones tecnológicas avanzadas y de alto impacto.\nDescripción del puesto\nBuscamos un ingeniero de datos o informático, con al menos 2 años de experiencia en desarrollo e implementación de soluciones tecnológicas avanzadas. Participará activamente en proyectos innovadores para la gestión inteligente de información en tiempo real mediante tecnologías cloud, IoT industrial e inteligencia artificial. Como Ingeniero de Datos en Tecnoandina, serás responsable de diseñar, construir y mantener infraestructuras de datos eficientes y escalables. Tus tareas diarias incluirán el desarrollo de modelos de datos, implementación de procesos ETL y administración de almacenes de datos. Este es un puesto de tiempo completo con modalidad híbrida, lo que significa que trabajarás desde nuestra oficina en Las Condes con la posibilidad de realizar algunas tareas desde casa.\nPrincipales Responsabilidades\n- Manejo especializado de bases de datos SQL, series temporales, Python y herramientas para extracción y procesamiento avanzado de datos.\n- Desarrollo y despliegue de soluciones con técnicas de machine learning (PyTorch, Keras, TensorFlow).\n- Identificación de técnicas y algoritmos relevantes según necesidades específicas.\n- Colaborar activamente en equipos multidisciplinarios para implementar soluciones efectivas en producción.\n- Diseño de arquitecturas robustas de microservicios, gestión de contenedores con Docker/Kubernetes y manejo eficiente del código con Git.\nRequisitos\n- Titulación en ingeniería informática, industrial, matemática o afines.\n- Al menos 2 años de experiencia desarrollando sistemas implementados en plataformas cloud.\n- Python, Docker, Dashboards interactivos (Dash, Plotly, Streamlit, Gradio), Git, SQL, NumPy, Pandas, FastAPI, REST APIs.\n- Experiencia en ingeniería de datos para crear y mantener infraestructuras robustas.\n- Dominio en modelado de datos y técnicas de diseño de bases de datos.\n- Conocimientos en procesos ETL (Extract Transform Load) y manejo de herramientas relacionadas.\n- Experiencia en la creación y gestión de almacenes de datos.\n- Aptitudes adicionales como la capacidad de trabajo en equipo, comunicación efectiva y adaptabilidad serán muy valoradas.\nDeseables:\n- Experiencia con plataformas cloud (preferentemente Azure).\n- Conocimiento en Kubernetes, manejo avanzado de series temporales y bases de datos como FluxDB.\n- Competencias básicas en gestión de proyectos informáticos.        \n            \n                            \n            ",
         "entry_level",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4227768803/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "2+",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Data analysis or visualization' 'Knowledge of Machine Learning or MLOps'\n 'Version control (GIT or similar)' 'APIs'\n 'Collaboration with data scientists or analysts']"
        ],
        [
         "36",
         "Ingeniero de datos Junior",
         "SII Group Chile",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4219697991/",
         "En \nSII Group\n estamos en búsqueda de un/a 🕵🏻‍♀️ \nIngeniero/a de Datos Jr\n para unirse a nuestro equipo 🚀\n¿Qué necesitas para postular? 🤔💭\n✔️ Conocimientos en Python\n✔️ Experiencia en Power Bi\n✔️ Experiencia o conocimientos en Azure\n✔️ Manejo de bases de datos SQL\n✔️ Ganas de aprender y crecer en un entorno dinámico y colaborativo\n✔️ Capacidad para trabajar en equipo y mantener una comunicación efectiva\n✔️ Disponibilidad para trabajar en modalidad presencial\nEn \nSII Group Chile\n:\n⏰ \nTu tiempo nos importa\n: días administrativos libres\n🏆 \nQueremos que te desarrolles\n: programas de capacitación a tu medida\n🚀 \nSer #fungineer es fun\n: FunFridays y eventos varios\nOfrecemos otros beneficios importantes\n: revísalos en el siguiente enlace:\n🔗 https://lnkd.in/eyn4vbKN\n👀 Visita nuestro sitio web:\n🔗 https://lnkd.in/dQNkS6Rz\n💡 ¡Anímate a formar parte del Team! Envíanos tu CV con pretensiones de renta a:\n📩 \nmatias.villagra@siigroup.cl\n y \nseleccion@siigroup.cl\n📢 Esta vacante está disponible para personas con discapacidad, conforme a la \nLey 21.015\n. Si presentas alguna condición y estás interesado/a en la posición, te agradeceremos que nos indiques las adecuaciones necesarias para que tu proceso de entrevista sea óptimo.        \n            \n                            \n            ",
         "associate",
         "full-time",
         "information_technology",
         "it_services_and_it_consulting",
         "https://www.linkedin.com/jobs/view/4219697991/",
         "Low",
         "Junior",
         "No",
         "No",
         "Not Specified",
         "No",
         "Azure",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization']"
        ],
        [
         "37",
         "Lead Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4167833411/",
         "We are seeking a \nLead Data Engineer\n to join our team.\nThis position combines engineering and analytical duties. You will develop and maintain data infrastructure while analyzing cost data, identifying trends, and collaborating with teams to optimize cloud expenses. Primarily focusing on AWS, this role demands a solid understanding of AWS services and cost structures. You will utilize tools such as Python, Databricks, Snowflake, Airflow, and Looker to provide impactful insights and solutions that enhance cost optimizations and support decision-making.\nResponsibilities\n- Design, construct, and sustain scalable ETL pipelines to process and adjust large volumes of cloud cost and usage data\n- Merge data from various sources, including AWS, into centralized data repositories like Snowflake\n- Create and uphold data models to aid cost analysis and reporting requirements\n- Enhance query output and storage effectiveness for vast datasets\n- Automate repetitive data processing jobs and set up robust monitoring for data pipelines\n- Guarantee data precision and dependability via validation techniques\n- Examine cloud cost data to pinpoint trends, anomalies, and opportunities for optimization\n- Engage intimately with teams to review expenditure changes and address cost anomalies\n- Cooperate with stakeholders to comprehend cost determinants and offer actionable insights\n- Assist teams in crafting dashboards and visualizations to monitor critical cost indicators\n- Compile reports and presentations to convey findings and suggestions to leadership\n- Collaborate with teams to formulate strategies for cost reduction and operational enhancement\nRequirements\n- Bachelor’s degree in Computer Science, Data Engineering, Data Analytics, or a related field\n- 5+ years of experience in data engineering, data analysis, or a combined role\n- 1+ years of relevant leadership experience\n- Expertise in AWS services (e.g., EC2, S3, RDS, Lambda) and their cost frameworks\n- Proficiency in SQL and skills in relational databases like Snowflake or Redshift\n- Familiarity with Databricks and Spark for large-scale data processing\n- Hands-on experience with ETL tools and frameworks (e.g., Databricks, Apache Airflow)\n- Programming experience in Python for data analysis and pipeline creation\n- Experience with BI tools like Looker or Tableau for constructing dashboards and visualizations\n- Excellent problem-solving abilities and the capability to interpret and analyze vast datasets\nNice to have\n- Background in Kubernetes, Docker, or containerization technology\n- Understanding of cloud cost management tools and techniques\n- Strong communication capabilities for presenting insights to stakeholders and leadership\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_media_and_telecommunications",
         "https://www.linkedin.com/jobs/view/4167833411/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "5+",
         "Yes",
         "AWS",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Data analysis or visualization' 'Data quality'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Spark knowledge']"
        ],
        [
         "38",
         "Lead Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225587960/",
         "We are looking for a\n remote Lead Data Engineer \nto join our team.\nThe candidate will be responsible for leading the design, development, and implementation of world-class data solutions and applications, ensuring their scalability and performance.\nResponsibilities\n- Lead the design, development, and implementation of world-class data solutions and applications, ensuring their scalability and performance\n- Understand business requirements and translate them into innovative data engineering strategies\n- Oversee end-to-end data pipeline development, from data ingestion to transformation and storage\n- Provide best practices for data engineering, ensuring data quality, security, and compliance\n- Provide continuous improvement of data engineering processes by adopting Agile methodologies and driving CI/CD practices\n- Lead the team and monitor and mentor junior members of the team,\nRequirements\n- 5+ years of relevant work experience with Data Engineering\n- 1+ years of relevant leadership experience\n- Expertise in Python\n- Strong knowledge of AWS, harnessing cloud resources to optimize data processing and storage.\n- Expertise in Databricks,\n- Proficiency in Apache Spark or Hive, enabling you to work with large-scale data processing and analysis.\n- Knowledge of CI/CD practices\n- Proficiency in SQL\n- Experience with Terraform,\n- Strong understanding of Agile methodologies\n- B2+ English level\nNice to have\n- Familiarity with Docker and Kubernetes\n- Knowledge of streaming data processing technologies like Apache Kafka or Apache Flink\n- Experience with data warehousing solutions like Amazon Redshift or Snowflake\n- Understanding of machine learning concepts\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering,_information_technology,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_technology,_information_and_internet",
         "https://www.linkedin.com/jobs/view/4225587960/",
         "High",
         "Lead or greater",
         "No",
         "No",
         "5+",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data quality' 'CI/CD'\n 'Spark knowledge' 'Version control (GIT or similar)']"
        ],
        [
         "39",
         "Lead, Data Engineer - AI, Insights & Solutions",
         "Bain & Company",
         "Las Condes, Santiago Metropolitan Region, Chile",
         "2025-05-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4216737267/",
         "WHAT MAKES US A GREAT PLACE TO WORK\nWe are proud to be consistently recognized as one of the world’s best places to work, a champion of diversity and a model of social responsibility. We are currently the #1 ranked consulting firm on Glassdoor’s Best Places to Work list and have maintained a spot in the top four for the last 13 years.\nWe believe diversity, inclusion, and collaboration are key to building extraordinary teams. We hire people with exceptional talents and potential, and create an environment where you can thrive professionally and personally. We’re recognized by Fortune, Vault, Working Mother, the Human Rights Campaign, and more for being a great place to work for diversity, women, LGBTQ+, and parents.\nWHO YOU’LL WORK WITH\nAs a member of Bain’s \nAI, Insights & Solutions (AIS)\n team, you’ll join a talented, cross-functional group of Data Engineers, Data Scientists, Machine Learning Engineers, and Consultants. Together, we design and implement robust, scalable solutions that address our clients' most pressing data challenges across diverse industries. Our collaborative and supportive environment enables creativity and continuous learning, helping us consistently deliver exceptional results.\nWHERE YOU’LL FIT WITHIN THE TEAM\nAs a \nLead Data Engineer\n, you’ll play a central role in developing and deploying scalable data solutions that fuel insight-driven transformation for our clients. You’ll lead high-impact projects that span cloud platforms, data governance, and modern data architectures.\nWHAT YOU’LL DO\n- Design and implement scalable data engineering solutions and infrastructure\n- Lead data governance projects and develop robust frameworks for client adoption\n- Build end-to-end data architectures on cloud platforms (AWS, Azure, GCP)\n- Engage in full engineering lifecycle: from design to deployment\n- Drive containerized, serverless data pipelines using Terraform and orchestration frameworks\n- Optimize performance and schema design in data lakes and warehouses\n- Collaborate across regions and disciplines to deliver high-impact outcomes\nABOUT YOU\nTechnical Skills and Knowledge:\n- 5+ years of data engineering experience (3+ years at senior or staff level)\n- Expertise in Python\n- Strong skills in modern ETL tools (e.g., Airflow, Beam, Spark)\n- Hands-on with cloud platforms, Kubernetes, and Terraform\n- Proficiency in SQL/NoSQL (e.g., PostgreSQL, MongoDB, Snowflake)\n- Background in DevOps, CI/CD, and Git workflows\n- Strong computer science fundamentals (algorithms, data structures, testing)\n- Agile development experience\n- Experience with data governance practices, including data cataloging, lineage tracking, metadata management, data quality frameworks and knowledge of Master Data Management (MDM) principles (desired)\nInterpersonal Skills:\n- - Professional proficiency in English\n- - Strong communication and cross-functional collaboration skills\n- - Critical thinking, curiosity, and a proactive mindset\nEducation:\n- Bachelor’s or Master’s degree in Computer Science, Engineering, or related field        \n            \n                            \n            ",
         "not_applicable",
         "full-time",
         "engineering",
         "business_consulting_and_services",
         "https://www.linkedin.com/jobs/view/4216737267/",
         "High",
         "Lead or greater",
         "Yes",
         "No",
         "5+",
         "Yes",
         "Multiple Clouds",
         "['Develop pipelines or ETL/ELT processes' 'Data governance' 'CI/CD'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "40",
         "Líder Técnico Data Engineer (GCP & Dataflow)",
         "Option",
         "Santiago Metropolitan Area",
         "2025-05-10 00:00:00",
         "https://www.linkedin.com/jobs/view/4227773489/",
         "¿Quiénes somos?\nEn \nOption\n creemos en un mundo donde las soluciones tecnológicas no tienen límites, donde triunfan los que se atreven, los que innovan, los que no tienen miedo a equivocarse y aprender de sus errores, creamos metodologías únicas para impulsar una Aceleración Digital exitosa, convirtiendo los problemas en oportunidades, solucionando de manera ágil y dinámica el proceso de transformación ¡Únete a un equipo de mentes creativas y apasionadas!\n¿Qué buscamos?\nBuscamos un Líder Técnico de Ingeniería de Datos apasionado por diseñar, escalar y optimizar arquitecturas de datos en la nube, con sólida experiencia en Google Cloud Platform (GCP) y al menos 4 años como desarrollador Fullstack. En este rol, estarás altamente involucrado en el desarrollo y liderazgo técnico, colaborando con equipos de ingeniería de datos, fullstack y data science. Serás responsable de migrar y mantener pipelines de datos usando Dataflow, Composer (Airflow), BigQuery y Dataform, aplicando buenas prácticas de particionamiento, clustering y optimización de costos.\n¿Qué te ofrece este puesto?\n- La oportunidad de trabajar en proyectos desafiantes y de vanguardia que tienen un impacto real en el mundo.\n- Un ambiente de trabajo colaborativo y dinámico donde podrás aprender de los mejores y desarrollar tus habilidades.\n- La posibilidad de formar parte de una comunidad de mentes creativas y apasionadas que comparten tu pasión por la tecnología.\n- Un paquete de compensación y beneficios competitivo.\n¿Cuáles serían tus principales desafíos?\n- Liderar el diseño e implementación de pipelines de datos escalables y eficientes en GCP, utilizando herramientas como Dataflow, Composer y BigQuery. \n-  Diseñar arquitecturas modernas en la nube priorizando rendimiento, seguridad y optimización de costos, utilizando servicios como Cloud Run, Spanner, Pub/Sub y Workflows. \n-  Monitorear y optimizar los procesos de ingestión, transformación y almacenamiento de datos, garantizando su fiabilidad y eficiencia operativa. \n-  Desarrollar y mantener modelos de datos alineados con las mejores prácticas y estándares de Gobierno de Datos. \n-  Acompañar y apoyar a equipos de ingeniería (Data, Fullstack, Data Science) asegurando la calidad del desarrollo y promoviendo buenas prácticas en automatización, testing y CI/CD. \n-  Generar activos de datos y APIs que faciliten la explotación de la información por parte de usuarios técnicos y de negocio. \n-  Promover la adopción de tecnologías como Terraform, GitLab CI/CD y herramientas de logging y observabilidad para aumentar la madurez operativa. \n-  Colaborar en iniciativas de innovación y modernización, incluyendo soluciones basadas en Inteligencia Artificial y automatización de procesos en la nube.\n¿Qué necesitas para ser nuestro próximo/a Líder Técnico Data Engineer?\n- Trabajar en soluciones de datos sobre GCP, liderando el diseño e implementación de pipelines con Dataflow, Composer y BigQuery.\n- Desarrollar APIs y procesos automatizados utilizando Cloud Run, Cloud Functions y bases de datos como Spanner y Firestore.\n- Dominio de Python, NodeJS y frameworks como FastAPI y NestJS, con experiencia en infraestructuras modernas basadas en microservicios.\n- Experiencia en herramientas de GCP como BigQuery, Dataform, Pub/Sub, Cloud Logging y Workflows.\n- Conocimiento de Docker, Kubernetes (GKE) y herramientas de infraestructura como código con Terraform.\n- Implementación de procesos CI/CD utilizando Cloud Build y GitLab.\n- Familiaridad con prácticas de testing (unitarios, integración, carga) y metodologías ágiles (Scrum).\n- Inglés intermedio/avanzado.\n- Ubicación: Latam\nModalidad de Trabajo: \n100% Remoto\n¡Conoce acerca de nosotros! \nhttps://www.option.tech\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "engineering_and_information_technology",
         "information_technology_&_services",
         "https://www.linkedin.com/jobs/view/4227773489/",
         "High",
         "Lead or greater",
         "No",
         "No",
         "Not Specified",
         "Yes",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling' 'CI/CD' 'APIs'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring' 'Version control (GIT or similar)']"
        ],
        [
         "41",
         "Product Data Engineer",
         "Ria Money Transfer",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-06 00:00:00",
         "https://www.linkedin.com/jobs/view/4210045636/",
         "Description\nRia Money Transfer, a business segment of Euronet Worldwide, Inc. (NASDAQ: EEFT), delivers innovative financial services including fast, secure, and affordable global money transfers to millions of customers along with currency exchange, mobile top-up, bill payment and check cashing services, offering a reliable omnichannel experience.  With over 600,000 locations in nearly 200 countries and territories, our purpose remains to open ways for a better everyday life.\nWe believe we can create a world in which people are empowered to build the life they dream of, no matter who they are or where they are. One customer, one family, one community at a time.\nAbout This Role\nA Product Data Engineer at a money transfer organization focuses on managing and optimizing the flow of financial data across systems. They design and maintain data pipelines that process transaction data, ensuring accuracy, security, and efficiency. The role involves collaborating with product and engineering teams to develop insights that improve user experience, detect fraud, and enhance financial product features. They also ensure compliance with financial regulations by safeguarding sensitive data and supporting analytics that drive business decisions.\nRoles & Responsibilities\n- Develop and maintain data pipelines to process and transfer large volumes of financial transaction data efficiently and securely.\n- Integrate data from various sources (payment gateways, customer databases, external financial systems) to provide a unified view of transactions and user behavior.\n- Monitor and maintain the accuracy, consistency, and integrity of financial data, identifying and addressing any issues or discrepancies.\n- Work closely with product managers, engineers, and analysts to provide data insights that help optimize features, improve user experience, and meet business goals.\n- Ensure compliance with security and privacy regulations to protect sensitive customer data during transfer and storage.\n- Build reporting tools and dashboards to provide key performance indicators (KPIs) and actionable insights for business stakeholders.\n- Leverage data to detect irregularities or patterns indicative of fraudulent activity and implement systems to minimize risk.\n- Continuously optimize the performance of data processing systems to handle large volumes of real-time financial transactions efficiently.\n- Automate routine data processes and monitor data workflows to ensure continuous operation and quick issue resolution.\n- Provide data-driven insights to support key business decisions related to product development, market expansion, and customer engagement.\nPosition Requirements\n- Bachelor’s degree in Computer Science, Engineering, Mathematics, Statistics, or a related field.\n- Master’s degree (optional, but preferred) in a relevant field for more advanced roles.\n- Strong programming skills in languages such as Python, SQL, Java, or Scala.\n- Experience with data processing frameworks\n- Proficiency in SQL and NoSQL databases\n- Experience with data integration tools\n- 5+ years of experience working with data engineering, preferably in a financial or fintech environment.\n- Experience with real-time data processing and handling high volumes of transactions.\n- Experience working with financial or transactional data\n- Experience with data analytics platforms \n- Strong problem-solving abilities and a detail-oriented mindset.\n- Ability to work in a collaborative team environment with product managers, engineers, and business analysts.\n- Good communication skills to translate technical concepts to non-technical stakeholders.\n- Adaptability to changing business needs and new technologies.\n- English as second language is required \n- Must be able to work a hybrid schedule Monday thru Friday ( 9am-18hrs) \nPerks & Benefits\n- Medical, Dental & Life Insurance\n- Employee Stock Purchase Plan\n- Paid day off for birthday\n- Hybrid Work Schedule\n- Growth Opportunities\n- Corporate gatherings, team bonding events, and much more!\nRia Money Transfer is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\nCheck out our website to learn more about the company at: http://www.riamoneytransfer.com/\nThe position responsibilities outlined above are intended to define the general contents and requirements to perform this job. It is not to be taken as a complete statement of responsibilities or requirements. This job description does not restrict the Company’s right to assign or reassign duties and responsibilities to this job as needed.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "financial_services",
         "https://www.linkedin.com/jobs/view/4210045636/",
         "High",
         "Mid-Senior",
         "Yes",
         "No",
         "5+",
         "Yes",
         "No Mention",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization'\n 'Data quality']"
        ],
        [
         "42",
         "Senior Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-02 00:00:00",
         "https://www.linkedin.com/jobs/view/4167834410/",
         "We are seeking a \nSenior Data Engineer\n to join our team.\nThis role involves a blend of engineering and analytical responsibilities. You will develop and maintain data infrastructure while also analyzing cost data, identifying trends, and working closely with teams to optimize cloud spend. With a heavy focus on AWS, this position requires a strong understanding of AWS services and cost structures. You will leverage tools like Python, Databricks, Snowflake, Airflow, and Looker to deliver impactful insights and solutions that drive cost optimizations and decision-making.\nResponsibilities\n- Design, build, and maintain scalable ETL pipelines to process and transform large volumes of cloud cost and usage data\n- Integrate data from multiple sources, including AWS, into centralized data lakes or warehouses like Snowflake\n- Develop and maintain data models to support cost analysis and reporting needs\n- Optimize query performance and storage efficiency for large-scale datasets\n- Automate recurring data processing tasks and implement robust monitoring for data pipelines\n- Ensure data accuracy and reliability through validation processes\n- Analyze cloud cost data to identify trends, anomalies, and optimization opportunities\n- Work closely with teams to investigate spending changes and resolve cost anomalies\n- Collaborate with stakeholders to understand cost drivers and provide actionable insights\n- Support teams in building dashboards and visualizations to track key cost metrics\n- Create reports and presentations to communicate findings and recommendations to leadership\n- Partner with teams to develop strategies for cost reduction and operational efficiency\nRequirements\n- Bachelor’s degree in Computer Science, Data Engineering, Data Analytics, or a related field\n- 3+ years of experience in a data engineering, data analysis, or hybrid role\n- Knowledge of AWS services (e.g., EC2, S3, RDS, Lambda) and their cost structures\n- Proficiency in SQL and experience with relational databases like Snowflake or Redshift\n- Familiarity with Databricks and Spark for large-scale data processing\n- Hands-on experience with ETL tools and frameworks (e.g., Databricks, Apache Airflow)\n- Programming experience in Python for data analysis and pipeline development\n- Experience with BI tools like Looker or Tableau for creating dashboards and visualizations\n- Excellent problem-solving skills and the ability to interpret and analyze large datasets\nNice to have\n- Experience with Kubernetes, Docker, or containerization technologies\n- Understanding of cloud cost management tools and strategies\n- Strong communication skills for presenting insights to stakeholders and leadership\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_media_and_telecommunications",
         "https://www.linkedin.com/jobs/view/4167834410/",
         "High",
         "Senior",
         "Yes",
         "No",
         "3+",
         "Yes",
         "AWS",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'Data analysis or visualization' 'Data monitoring'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Spark knowledge']"
        ],
        [
         "43",
         "Senior Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-07 00:00:00",
         "https://www.linkedin.com/jobs/view/4225588935/",
         "We are currently looking for a\n remote Senior Data Engineer\n with ETL basics, Alteryx knowledge, familiarity with AWS Data Analytics platform tools to join our team.\nThe customer is a global medical company specializing in eye care products with headquarters in Geneva, Switzerland.\nResponsibilities\n- ETL development based on various data sources (data bases, files)\n- Building of data sources for Tableau dashboards\n- Data Analysis\n- Direct interaction with customer business representatives (US, EU)\nRequirements\n- Experience with ETL\n- Alteryx knowledge & skills\n- Strong Tableau knowledge\n- Familiarity with AWS Data Analytics platform tools leading to lake formation with data catalog\n- Data Analysis skills\n- Strong English\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology,_engineering,_and_research",
         "software_development,_it_services_and_it_consulting,_and_pharmaceutical_manufacturing",
         "https://www.linkedin.com/jobs/view/4225588935/",
         "Medium",
         "Senior",
         "No",
         "No",
         "Not Specified",
         "Yes",
         "AWS",
         "['Develop pipelines or ETL/ELT processes' 'Data analysis or visualization']"
        ],
        [
         "44",
         "Senior Data Engineer",
         "EPAM Systems",
         "Chile",
         "2025-05-09 00:00:00",
         "https://www.linkedin.com/jobs/view/4212352065/",
         "Join our dynamic team as a \nSenior Data Engineer\n, where you will work independently and collaboratively to deliver high-quality data integration solutions.\nYour expertise in investment financial data and agile methodologies will be crucial as you engage with cross-functional teams to enhance our trading platform and reporting capabilities. If you are driven to succeed and ready to make an impact, we invite you to apply.\nResponsibilities\n- Collaborate with the Product Owner, Scrum Master, Subject Matter Experts and Development team to define and analyze user stories tracked in Jira\n- Participate in the design, development, and implementation of software solutions using a modern technology stack\n- Demonstrate solutions and articulate business value to business partners during sprint showcases\n- Engage actively in configuration to support data integration efforts\n- Work independently when required while maintaining a collaborative team spirit\n- Utilize agile methodologies to iteratively deliver high-quality products\n- Contribute to the continuous improvement of the data engineering processes\nRequirements\n- Bachelor’s degree in a technical discipline or equivalent professional experience\n- Minimum 3 years of experience as a professional Data Engineer\n- Expertise in SQL, including programming views, stored procedures, and functions\n- Background in data modeling with a strong emphasis on best practices\n- Proficiency in Snowflake for data warehousing and analysis\n- Understanding of data warehouse fundamentals and architecture\n- Familiarity with Amazon Web Services and its data-related services\n- Experience in Python, particularly with ETL tools like Pandas and NumPy\n- Capability to implement and maintain CI/CD pipelines using BitBucket/GitHub and Bamboo\n- Fluency in English\nNice to have\n- Experience with financial systems and reporting, particularly in investments accounting or general ledger\n- Knowledge of integrating large-scale financial investment or ERP systems\n- Demonstrated experience delivering data solutions via agile methodologies using AWS resources such as S3, Glue, Athena, and Lambda\n- Familiarity with business intelligence tools and reporting solutions\n- Experience in agile software development practices including story sizing and sprint planning\nWe offer\n- International projects with top brands\n- Work with global teams of highly skilled, diverse peers\n- Healthcare benefits\n- Employee financial programs\n- Paid time off and sick leave\n- Upskilling, reskilling and certification courses\n- Unlimited access to the LinkedIn Learning library and 22,000+ courses\n- Global career opportunities\n- Volunteer and community involvement opportunities\n- EPAM Employee Groups\n- Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology,_engineering,_and_business_development",
         "software_development,_it_services_and_it_consulting,_and_insurance",
         "https://www.linkedin.com/jobs/view/4212352065/",
         "Medium",
         "Senior",
         "Yes",
         "No",
         "3+",
         "Yes",
         "AWS",
         "['Databricks or snowflake' 'Develop pipelines or ETL/ELT processes'\n 'Data modeling' 'CI/CD' 'Version control (GIT or similar)']"
        ],
        [
         "45",
         "Senior Data Engineer",
         "Grupo Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-04-30 00:00:00",
         "https://www.linkedin.com/jobs/view/4219923815/",
         "Descripción Empresa\nSomos más de 90 mil personas que, día a día, dedicamos nuestra pasión y energía a cumplir nuestro Propósito de “Simplificar y Disfrutar Más la Vida”. Propósito que hoy vive a través de nuestro ecosistema físico y digital en todas nuestras empresas (Falabella Retail, Sodimac, IKEA, Tottus, Mallplaza, Falabella Inmobiliario, Falabella.com, Linio, Falabella Financiero, Banco Falabella, Falabella Soriana, Seguros Falabella, Fazil, Fpay y Falabella Corporativo) y países (Argentina, Brasil, Chile, China, Colombia, India, México, Perú y Uruguay).\nValoramos las distintas miradas porque entendemos que la diversidad es la clave de nuestra innovación. Queremos ir más allá de cualquier límite, desafiarnos constantemente, divertirnos haciendo lo que nos gusta y dejar huella en lo que hacemos. Y sabemos que existe una forma de hacerlo: como UN SOLO EQUIPO.\nConoce más oportunidades para vivir la #ExperienciaFalabella en https://muevete.falabella.com/\nMisión Del Cargo\n¡Únete a Falabella Retail y lleva tus habilidades de Ingeniería de Datos al próximo nivel!\nFunciones Del Cargo\nFormarás parte de un equipo de alto impacto que lidera la transformación digital, trabajando en proyectos críticos para optimizar y escalar nuestras operaciones en uno de los ecosistemas más dinámicos de la región.\nMision: Responsable de diseñar, construir y mantener sistemas de datos escalables para el análisis y la toma de decisiones en la organización.\n-  Conectar y consumir datos de distintas fuentes, principalmente a través de Google Cloud Platform (GCP).\n-  Ingestar y preparar datos para que los equipos puedan explotarlos a nivel de análisis.\n-  Asegurar la correcta conexión y flujo de datos con proveedores externos, como en el caso de productos patrocinados.\n-  Trabajar en el negocio de retail media, gestionando datos relacionados con publicidad y servicios comerciales.\n-  Colaborar con el área de arquitectura para acordar requerimientos necesarios para las distintas iniciativas.\n-  Analizar e implementar modelos de bases de datos (estructurados y no estructurados).\n-  Sugerir mejoras para evolucionar la arquitectura de las soluciones, buscando procesos desacoplados, serverless y stateless.\n-  Mantener el nivel de servicio para los entregables del equipo y las herramientas de datos desarrolladas.\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros! Somos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\nRequisitos\n- Profesional titulado en Ingeniería Civil Computación, Ingeniería Industrial en Computación.\n- Experiencia demostrable como Data Engineer por mas de 03 años\n- Conocimiento profundo de Google Cloud Platform Composer, Cloud Functions, BigQuery, Dataproc, etc.\n- Experiencia en lenguajes de programación, especialmente Python.\n- Habilidades en SQL y bases de datos relacionales y no relacionales.\n- Experiencia utilizando sistemas de control de versiones como Git.\n- Deseable: Certificaciones relevantes en tecnologías de datos y cloud\n- Disponibilidad para trabajar modalidad hibrida.\n- Dominio de Ingles conversacional nivel intermedio.\nCondiciones Oferta\nDescripción proceso de selección:\nSúmate a nuestro equipo y se parte de la transformación de Falabella:\n-  Postula a la oferta.\n-  Revisa tu email.        \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "retail",
         "https://www.linkedin.com/jobs/view/4219923815/",
         "High",
         "Mid-Senior",
         "Yes",
         "Yes",
         "3+",
         "Yes",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data modeling'\n 'Collaboration with data scientists or analysts'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Version control (GIT or similar)']"
        ],
        [
         "46",
         "Senior Data Engineer",
         "Grupo Falabella",
         "Santiago, Santiago Metropolitan Region, Chile",
         "2025-05-01 00:00:00",
         "https://www.linkedin.com/jobs/view/4205465977/",
         "Descripción Empresa\nSomos más de 90 mil personas que, día a día, dedicamos nuestra pasión y energía a cumplir nuestro Propósito de “Simplificar y Disfrutar Más la Vida”. Propósito que hoy vive a través de nuestro ecosistema físico y digital en todas nuestras empresas (Falabella Retail, Sodimac, IKEA, Tottus, Mallplaza, Falabella Inmobiliario, Falabella.com, Linio, Falabella Financiero, Banco Falabella, Falabella Soriana, Seguros Falabella, Fazil, Fpay y Falabella Corporativo) y países (Argentina, Brasil, Chile, China, Colombia, India, México, Perú y Uruguay).\nValoramos las distintas miradas porque entendemos que la diversidad es la clave de nuestra innovación. Queremos ir más allá de cualquier límite, desafiarnos constantemente, divertirnos haciendo lo que nos gusta y dejar huella en lo que hacemos. Y sabemos que existe una forma de hacerlo: como UN SOLO EQUIPO.\nMisión Del Cargo\n¡Únete a Falabella Retail y lleva tus habilidades de Ingeniería de Datos al próximo nivel!\nFunciones Del Cargo\nFormarás parte de un equipo de alto impacto que lidera la transformación digital, trabajando en proyectos críticos para optimizar y escalar nuestras operaciones en uno de los ecosistemas más dinámicos de la región.\nMision: Responsable de diseñar, construir y mantener sistemas de datos escalables para el análisis y la toma de decisiones en la organización.\nResponsabilidades\nDiseñar y desarrollar canalizaciones de datos (ETL/ELT) robustas y eficientes.\nColaborar con equipos de ingeniería de datos.\nImplementar y gestionar bases de datos y data warehouses.\nOptimizar el rendimiento de las bases de datos y sistemas de almacenamiento de datos.\nGarantizar la calidad, integridad y seguridad de los datos.\nAutomatizar procesos de ingestión y transformación de datos.\nMonitorizar y solucionar problemas relacionados con los sistemas de datos.\nDocumentar procesos, arquitecturas y mejores prácticas relacionadas con el manejo de datos.\nSi disfrutas nuevos desafíos con alta responsabilidad y exposición en el epicentro de la transformación del retail en Latinoamérica, ¡súmate a trabajar con nosotros! Somos una empresa que apoya la Ley 21015, apoyamos la diversidad y la inclusión en todas sus formas, sin importar religión, raza, género, situación de discapacidad, nacionalidad.\nConoce más oportunidades para vivir la #ExperienciaFalabella en https://muevete.falabella.com/\nRequisitos\n- Profesional Titulado en Ingeniería Civil Computación, Industrial, matemático u eléctrico.\n- Experiencia demostrable como Data Engineer por mas de 04 años\n- Conocimientos en lenguajes de programación Python.\n- Experiencia con Google Cloud Platform (Composer, Cloud Functions, Bigquery, Dataproc, etc)\n- Experiencia utilizando git\n- Deseable: Conocimientos profundos en SQL y en bases de datos relacionales y no relacionales\n- Deseable: Certificaciones relevantes en tecnologías de datos y cloud.\nCondiciones Oferta\nDescripción proceso de selección:\nEl proceso de selección se realiza a través de Aira - plataforma de reclutamiento diseñado para mejorar tu experiencia de postulación.\nPara Postular Solo Necesitas\n-  Postular a la oferta\n-  Revisar tu email\n-  Ingresar a Aira y contestar las preguntas y/o pruebas solicitadas\nLuego, si vemos que tu perfil se ajusta a lo que estamos buscando, te contactaremos por email (a través de Aira) para seguir a la etapa presencial.\n                \n            \n                            \n            ",
         "mid-senior_level",
         "full-time",
         "information_technology",
         "retail",
         "https://www.linkedin.com/jobs/view/4205465977/",
         "Medium",
         "Senior",
         "Yes",
         "Yes",
         "5+",
         "No",
         "GCP",
         "['Develop pipelines or ETL/ELT processes' 'Data quality'\n 'Version control (GIT or similar)'\n 'Automation/Orchestration (Airflow, Prefect, Dagster, etc.)'\n 'Data monitoring']"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 47
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>job_url</th>\n",
       "      <th>job_description</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>job_url_1</th>\n",
       "      <th>task_clarity</th>\n",
       "      <th>seniority_level_ai</th>\n",
       "      <th>requires_degree_it</th>\n",
       "      <th>mentions_certifications</th>\n",
       "      <th>years_of_experience</th>\n",
       "      <th>is_in_english</th>\n",
       "      <th>cloud_preference</th>\n",
       "      <th>skills_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4220611480/</td>\n",
       "      <td>We are seeking an experienced \\nChief Data Eng...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering,_information_technology,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4220611480/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212167524/</td>\n",
       "      <td>We are seeking an experienced Chief Data Engin...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering,_information_technology,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212167524/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consultor Data Engineer</td>\n",
       "      <td>MAS Analytics</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223641021/</td>\n",
       "      <td>MAS Analytics\\n es una consultora de datos e i...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet_and_infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223641021/</td>\n",
       "      <td>High</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>2Brains</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216874808/</td>\n",
       "      <td>2Brains es una empresa dedicada a construir y ...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet_and_infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216874808/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5+</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>ARKHO</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223640231/</td>\n",
       "      <td>ARKHO es una consultora experta en tecnologías...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet_and_infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223640231/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225594752/</td>\n",
       "      <td>We are seeking a highly skilled and motivated ...</td>\n",
       "      <td>associate</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology,_engineering,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225594752/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Spark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Falabella</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4207045620/</td>\n",
       "      <td>Descripción Empresa\\nSomos más de 80 mil perso...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>retail</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4207045620/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Falabella</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225586690/</td>\n",
       "      <td>Descripción Empresa\\nSomos más de 80 mil perso...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>retail</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225586690/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Genesys</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218817757/</td>\n",
       "      <td>Somos una empresa con más de 34 años de experi...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>software_development_and_financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218817757/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>No</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>NeuralWorks</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216875700/</td>\n",
       "      <td>NeuralWorks es una compañía de alto crecimient...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>technology,_information_and_internet_and_infor...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216875700/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, APIs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seeds</td>\n",
       "      <td>Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4189755534/</td>\n",
       "      <td>¿Sos \\nData Engineer\\n? Entonces… ¿Qué estás e...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>consulting</td>\n",
       "      <td>technology,_information_and_media</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4189755534/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Xepelin</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205474021/</td>\n",
       "      <td>Somos una FinTech que busca democratizar los s...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>hospitality,_food_and_beverage_services,_and_r...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205474021/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Autom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Xepelin</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4224174809/</td>\n",
       "      <td>Somos una FinTech que busca democratizar los s...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>hospitality,_food_and_beverage_services,_and_r...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4224174809/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Autom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer ( Azure Cloud &amp; Python &amp; Databri...</td>\n",
       "      <td>Option</td>\n",
       "      <td>Santiago Metropolitan Area</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219824203/</td>\n",
       "      <td>¿Quiénes somos?\\nEn Option, creemos en un mund...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>information_technology_&amp;_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219824203/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Spark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer - Databricks - Mid Level</td>\n",
       "      <td>Lumenalta</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4228057809/</td>\n",
       "      <td>Experience Remote done Right. With over 20 yea...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering,_information_technology,_and_consu...</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4228057809/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Spark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer - Inglés Avanzado</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221350232/</td>\n",
       "      <td>¿Buscas generar impactos significativos con tu...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>consulting,_information_technology,_and_projec...</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221350232/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Engineer - LATAM (100% Remoto)</td>\n",
       "      <td>Imagemaker</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219332744/</td>\n",
       "      <td>Company Description: Imagemaker\\nJob Descripti...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219332744/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer - Trainee</td>\n",
       "      <td>Soluciones - Data &amp; Analytics Consulting</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226385320/</td>\n",
       "      <td>✔️\\n¿Quiénes Somos?\\nSomos una consultora enfo...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology_and_consulting</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226385320/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>No Mention</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Engineer Azure DataBricks</td>\n",
       "      <td>Accenture Chile</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4157214213/</td>\n",
       "      <td>WORK AT THE HEART OF THE CHANGE\\nEmprende una ...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4157214213/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1+</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer GCP</td>\n",
       "      <td>Soluciones - Data &amp; Analytics Consulting</td>\n",
       "      <td>Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219126754/</td>\n",
       "      <td>.✔️\\n¿Quiénes Somos?\\nSomos una consultora enf...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting_and_information_...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219126754/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer Jr</td>\n",
       "      <td>SII Group Chile</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-05</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223983495/</td>\n",
       "      <td>🚀 ¡En SII Group estamos en búsqueda de un/a Da...</td>\n",
       "      <td>associate</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223983495/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, CI/CD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Engineer Junior</td>\n",
       "      <td>Isapre Consalud</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226383768/</td>\n",
       "      <td>Trabajamos para brindar tranquilidad y segurid...</td>\n",
       "      <td>associate</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>banking_and_health_and_human_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226383768/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1+</td>\n",
       "      <td>No</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer MACHBANK</td>\n",
       "      <td>MACHBANK</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219005225/</td>\n",
       "      <td>El rol de Senior Data Engineer en el equipo de...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219005225/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Engineer MACHBANK</td>\n",
       "      <td>MACHBANK</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226352405/</td>\n",
       "      <td>El rol de Senior Data Engineer en el equipo de...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4226352405/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Engineer MACHBANK (Plazo Fijo)</td>\n",
       "      <td>MACHBANK</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221378934/</td>\n",
       "      <td>El rol de Data Engineer en el equipo de Data E...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221378934/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2+</td>\n",
       "      <td>No</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Engineer Senior</td>\n",
       "      <td>Equifax</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4144902157/</td>\n",
       "      <td>Como Data Engineer, estarás a cargo de integra...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4144902157/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1+</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Autom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer with GCP Senior-Level or Expert-...</td>\n",
       "      <td>Globant</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4222832762/</td>\n",
       "      <td>At Globant, we are working to make the world a...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4222832762/</td>\n",
       "      <td>Low</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Yes</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Spark knowledge, None of the above mentioned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Engineer – Amazon Web Services (AWS)</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218749215/</td>\n",
       "      <td>¿Buscas generar impactos significativos con tu...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>consulting,_information_technology,_and_projec...</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218749215/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer – Google Cloud Platform (GCP)</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218749139/</td>\n",
       "      <td>¿Buscas generar impactos significativos con tu...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>consulting,_information_technology,_and_projec...</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4218749139/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2+</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>INGENIERO DATOS SR</td>\n",
       "      <td>LATAM Airlines</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4217041945/</td>\n",
       "      <td>✈️ En \\nLATAM \\nte invitamos a elevar cada via...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>airlines_and_aviation</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4217041945/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Mention</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ingeniero de Datos</td>\n",
       "      <td>Softline Consultores Gerenciales</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219332714/</td>\n",
       "      <td>Company Description: WMPartner Consultores\\nJo...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219332714/</td>\n",
       "      <td>Low</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ingeniero de Datos</td>\n",
       "      <td>TecSystems</td>\n",
       "      <td>Huechuraba, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-12</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4228080982/</td>\n",
       "      <td>En \\nTecSystems \\nestamos en busca de un profe...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4228080982/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ingeniero de Datos Cloud – Azure</td>\n",
       "      <td>Kabeli Selección</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4222894601/</td>\n",
       "      <td>En Kabeli, estamos en búsqueda de un Ingeniero...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>staffing_and_recruiting_and_technology,_inform...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4222894601/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ingeniero de datos</td>\n",
       "      <td>Elitsoft</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221748679/</td>\n",
       "      <td>¡\\nÚnete a Elitsoft!\\n Somos una empresa consu...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4221748679/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5+</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ingeniero de datos</td>\n",
       "      <td>SSI Outsourcing</td>\n",
       "      <td>Los Andes, Valparaiso Region, Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223028203/</td>\n",
       "      <td>Ingeniero de Datos\\nImagina ser parte de un eq...</td>\n",
       "      <td>associate</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering_and_information_technology</td>\n",
       "      <td>it_services_and_it_consulting_and_mining</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4223028203/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ingeniero de datos</td>\n",
       "      <td>Tecnoandina</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-10</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4227768803/</td>\n",
       "      <td>Descripción de la Empresa:\\n Tecnoandina se de...</td>\n",
       "      <td>entry_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4227768803/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2+</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ingeniero de datos Junior</td>\n",
       "      <td>SII Group Chile</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219697991/</td>\n",
       "      <td>En \\nSII Group\\n estamos en búsqueda de un/a 🕵...</td>\n",
       "      <td>associate</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>it_services_and_it_consulting</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219697991/</td>\n",
       "      <td>Low</td>\n",
       "      <td>Junior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>No</td>\n",
       "      <td>Azure</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4167833411/</td>\n",
       "      <td>We are seeking a \\nLead Data Engineer\\n to joi...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology,_engineering,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4167833411/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>5+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225587960/</td>\n",
       "      <td>We are looking for a\\n remote Lead Data Engine...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering,_information_technology,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225587960/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Lead, Data Engineer - AI, Insights &amp; Solutions</td>\n",
       "      <td>Bain &amp; Company</td>\n",
       "      <td>Las Condes, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216737267/</td>\n",
       "      <td>WHAT MAKES US A GREAT PLACE TO WORK\\nWe are pr...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering</td>\n",
       "      <td>business_consulting_and_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4216737267/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>5+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Multiple Clouds</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Líder Técnico Data Engineer (GCP &amp; Dataflow)</td>\n",
       "      <td>Option</td>\n",
       "      <td>Santiago Metropolitan Area</td>\n",
       "      <td>2025-05-10</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4227773489/</td>\n",
       "      <td>¿Quiénes somos?\\nEn \\nOption\\n creemos en un m...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>engineering_and_information_technology</td>\n",
       "      <td>information_technology_&amp;_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4227773489/</td>\n",
       "      <td>High</td>\n",
       "      <td>Lead or greater</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Yes</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Product Data Engineer</td>\n",
       "      <td>Ria Money Transfer</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210045636/</td>\n",
       "      <td>Description\\nRia Money Transfer, a business se...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4210045636/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>5+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Mention</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4167834410/</td>\n",
       "      <td>We are seeking a \\nSenior Data Engineer\\n to j...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology,_engineering,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4167834410/</td>\n",
       "      <td>High</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225588935/</td>\n",
       "      <td>We are currently looking for a\\n remote Senior...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology,_engineering,_and_research</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4225588935/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212352065/</td>\n",
       "      <td>Join our dynamic team as a \\nSenior Data Engin...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology,_engineering,_and_busin...</td>\n",
       "      <td>software_development,_it_services_and_it_consu...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4212352065/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWS</td>\n",
       "      <td>[Databricks or snowflake, Develop pipelines or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Grupo Falabella</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219923815/</td>\n",
       "      <td>Descripción Empresa\\nSomos más de 90 mil perso...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>retail</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4219923815/</td>\n",
       "      <td>High</td>\n",
       "      <td>Mid-Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Yes</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Grupo Falabella</td>\n",
       "      <td>Santiago, Santiago Metropolitan Region, Chile</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205465977/</td>\n",
       "      <td>Descripción Empresa\\nSomos más de 90 mil perso...</td>\n",
       "      <td>mid-senior_level</td>\n",
       "      <td>full-time</td>\n",
       "      <td>information_technology</td>\n",
       "      <td>retail</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4205465977/</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5+</td>\n",
       "      <td>No</td>\n",
       "      <td>GCP</td>\n",
       "      <td>[Develop pipelines or ETL/ELT processes, Data ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title   \n",
       "0                                 Chief Data Engineer  \\\n",
       "1                                 Chief Data Engineer   \n",
       "2                             Consultor Data Engineer   \n",
       "3                                       Data Engineer   \n",
       "4                                       Data Engineer   \n",
       "5                                       Data Engineer   \n",
       "6                                       Data Engineer   \n",
       "7                                       Data Engineer   \n",
       "8                                       Data Engineer   \n",
       "9                                       Data Engineer   \n",
       "10                                      Data Engineer   \n",
       "11                                      Data Engineer   \n",
       "12                                      Data Engineer   \n",
       "13  Data Engineer ( Azure Cloud & Python & Databri...   \n",
       "14             Data Engineer - Databricks - Mid Level   \n",
       "15                    Data Engineer - Inglés Avanzado   \n",
       "16                Data Engineer - LATAM (100% Remoto)   \n",
       "17                            Data Engineer - Trainee   \n",
       "18                     Data Engineer Azure DataBricks   \n",
       "19                                  Data Engineer GCP   \n",
       "20                                   Data Engineer Jr   \n",
       "21                               Data Engineer Junior   \n",
       "22                             Data Engineer MACHBANK   \n",
       "23                             Data Engineer MACHBANK   \n",
       "24                Data Engineer MACHBANK (Plazo Fijo)   \n",
       "25                               Data Engineer Senior   \n",
       "26  Data Engineer with GCP Senior-Level or Expert-...   \n",
       "27          Data Engineer – Amazon Web Services (AWS)   \n",
       "28        Data Engineer – Google Cloud Platform (GCP)   \n",
       "29                                 INGENIERO DATOS SR   \n",
       "30                                 Ingeniero de Datos   \n",
       "31                                 Ingeniero de Datos   \n",
       "32                   Ingeniero de Datos Cloud – Azure   \n",
       "33                                 Ingeniero de datos   \n",
       "34                                 Ingeniero de datos   \n",
       "35                                 Ingeniero de datos   \n",
       "36                          Ingeniero de datos Junior   \n",
       "37                                 Lead Data Engineer   \n",
       "38                                 Lead Data Engineer   \n",
       "39     Lead, Data Engineer - AI, Insights & Solutions   \n",
       "40       Líder Técnico Data Engineer (GCP & Dataflow)   \n",
       "41                              Product Data Engineer   \n",
       "42                               Senior Data Engineer   \n",
       "43                               Senior Data Engineer   \n",
       "44                               Senior Data Engineer   \n",
       "45                               Senior Data Engineer   \n",
       "46                               Senior Data Engineer   \n",
       "\n",
       "                                     company   \n",
       "0                               EPAM Systems  \\\n",
       "1                               EPAM Systems   \n",
       "2                              MAS Analytics   \n",
       "3                                    2Brains   \n",
       "4                                      ARKHO   \n",
       "5                               EPAM Systems   \n",
       "6                                  Falabella   \n",
       "7                                  Falabella   \n",
       "8                                    Genesys   \n",
       "9                                NeuralWorks   \n",
       "10                                     Seeds   \n",
       "11                                   Xepelin   \n",
       "12                                   Xepelin   \n",
       "13                                    Option   \n",
       "14                                 Lumenalta   \n",
       "15                                  Deloitte   \n",
       "16                                Imagemaker   \n",
       "17  Soluciones - Data & Analytics Consulting   \n",
       "18                           Accenture Chile   \n",
       "19  Soluciones - Data & Analytics Consulting   \n",
       "20                           SII Group Chile   \n",
       "21                           Isapre Consalud   \n",
       "22                                  MACHBANK   \n",
       "23                                  MACHBANK   \n",
       "24                                  MACHBANK   \n",
       "25                                   Equifax   \n",
       "26                                   Globant   \n",
       "27                                  Deloitte   \n",
       "28                                  Deloitte   \n",
       "29                            LATAM Airlines   \n",
       "30          Softline Consultores Gerenciales   \n",
       "31                                TecSystems   \n",
       "32                          Kabeli Selección   \n",
       "33                                  Elitsoft   \n",
       "34                           SSI Outsourcing   \n",
       "35                               Tecnoandina   \n",
       "36                           SII Group Chile   \n",
       "37                              EPAM Systems   \n",
       "38                              EPAM Systems   \n",
       "39                            Bain & Company   \n",
       "40                                    Option   \n",
       "41                        Ria Money Transfer   \n",
       "42                              EPAM Systems   \n",
       "43                              EPAM Systems   \n",
       "44                              EPAM Systems   \n",
       "45                           Grupo Falabella   \n",
       "46                           Grupo Falabella   \n",
       "\n",
       "                                           location       date   \n",
       "0                                             Chile 2025-05-01  \\\n",
       "1                                             Chile 2025-05-09   \n",
       "2     Santiago, Santiago Metropolitan Region, Chile 2025-05-07   \n",
       "3                                             Chile 2025-04-29   \n",
       "4                                             Chile 2025-05-07   \n",
       "5                                             Chile 2025-05-07   \n",
       "6     Santiago, Santiago Metropolitan Region, Chile 2025-05-02   \n",
       "7     Santiago, Santiago Metropolitan Region, Chile 2025-05-07   \n",
       "8     Santiago, Santiago Metropolitan Region, Chile 2025-04-30   \n",
       "9     Santiago, Santiago Metropolitan Region, Chile 2025-04-29   \n",
       "10              Santiago Metropolitan Region, Chile 2025-04-28   \n",
       "11    Santiago, Santiago Metropolitan Region, Chile 2025-05-01   \n",
       "12    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "13                       Santiago Metropolitan Area 2025-04-30   \n",
       "14                                            Chile 2025-05-11   \n",
       "15    Santiago, Santiago Metropolitan Region, Chile 2025-05-02   \n",
       "16    Santiago, Santiago Metropolitan Region, Chile 2025-04-29   \n",
       "17    Santiago, Santiago Metropolitan Region, Chile 2025-05-08   \n",
       "18    Santiago, Santiago Metropolitan Region, Chile 2025-05-01   \n",
       "19              Santiago Metropolitan Region, Chile 2025-04-29   \n",
       "20    Santiago, Santiago Metropolitan Region, Chile 2025-05-05   \n",
       "21  Las Condes, Santiago Metropolitan Region, Chile 2025-05-08   \n",
       "22  Las Condes, Santiago Metropolitan Region, Chile 2025-04-28   \n",
       "23  Las Condes, Santiago Metropolitan Region, Chile 2025-05-08   \n",
       "24  Las Condes, Santiago Metropolitan Region, Chile 2025-05-02   \n",
       "25  Las Condes, Santiago Metropolitan Region, Chile 2025-05-02   \n",
       "26    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "27    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "28    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "29                                            Chile 2025-04-28   \n",
       "30    Santiago, Santiago Metropolitan Region, Chile 2025-04-29   \n",
       "31  Huechuraba, Santiago Metropolitan Region, Chile 2025-05-12   \n",
       "32    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "33    Santiago, Santiago Metropolitan Region, Chile 2025-05-02   \n",
       "34              Los Andes, Valparaiso Region, Chile 2025-05-07   \n",
       "35  Las Condes, Santiago Metropolitan Region, Chile 2025-05-10   \n",
       "36    Santiago, Santiago Metropolitan Region, Chile 2025-04-30   \n",
       "37                                            Chile 2025-05-02   \n",
       "38                                            Chile 2025-05-07   \n",
       "39  Las Condes, Santiago Metropolitan Region, Chile 2025-05-09   \n",
       "40                       Santiago Metropolitan Area 2025-05-10   \n",
       "41    Santiago, Santiago Metropolitan Region, Chile 2025-05-06   \n",
       "42                                            Chile 2025-05-02   \n",
       "43                                            Chile 2025-05-07   \n",
       "44                                            Chile 2025-05-09   \n",
       "45    Santiago, Santiago Metropolitan Region, Chile 2025-04-30   \n",
       "46    Santiago, Santiago Metropolitan Region, Chile 2025-05-01   \n",
       "\n",
       "                                           job_url   \n",
       "0   https://www.linkedin.com/jobs/view/4220611480/  \\\n",
       "1   https://www.linkedin.com/jobs/view/4212167524/   \n",
       "2   https://www.linkedin.com/jobs/view/4223641021/   \n",
       "3   https://www.linkedin.com/jobs/view/4216874808/   \n",
       "4   https://www.linkedin.com/jobs/view/4223640231/   \n",
       "5   https://www.linkedin.com/jobs/view/4225594752/   \n",
       "6   https://www.linkedin.com/jobs/view/4207045620/   \n",
       "7   https://www.linkedin.com/jobs/view/4225586690/   \n",
       "8   https://www.linkedin.com/jobs/view/4218817757/   \n",
       "9   https://www.linkedin.com/jobs/view/4216875700/   \n",
       "10  https://www.linkedin.com/jobs/view/4189755534/   \n",
       "11  https://www.linkedin.com/jobs/view/4205474021/   \n",
       "12  https://www.linkedin.com/jobs/view/4224174809/   \n",
       "13  https://www.linkedin.com/jobs/view/4219824203/   \n",
       "14  https://www.linkedin.com/jobs/view/4228057809/   \n",
       "15  https://www.linkedin.com/jobs/view/4221350232/   \n",
       "16  https://www.linkedin.com/jobs/view/4219332744/   \n",
       "17  https://www.linkedin.com/jobs/view/4226385320/   \n",
       "18  https://www.linkedin.com/jobs/view/4157214213/   \n",
       "19  https://www.linkedin.com/jobs/view/4219126754/   \n",
       "20  https://www.linkedin.com/jobs/view/4223983495/   \n",
       "21  https://www.linkedin.com/jobs/view/4226383768/   \n",
       "22  https://www.linkedin.com/jobs/view/4219005225/   \n",
       "23  https://www.linkedin.com/jobs/view/4226352405/   \n",
       "24  https://www.linkedin.com/jobs/view/4221378934/   \n",
       "25  https://www.linkedin.com/jobs/view/4144902157/   \n",
       "26  https://www.linkedin.com/jobs/view/4222832762/   \n",
       "27  https://www.linkedin.com/jobs/view/4218749215/   \n",
       "28  https://www.linkedin.com/jobs/view/4218749139/   \n",
       "29  https://www.linkedin.com/jobs/view/4217041945/   \n",
       "30  https://www.linkedin.com/jobs/view/4219332714/   \n",
       "31  https://www.linkedin.com/jobs/view/4228080982/   \n",
       "32  https://www.linkedin.com/jobs/view/4222894601/   \n",
       "33  https://www.linkedin.com/jobs/view/4221748679/   \n",
       "34  https://www.linkedin.com/jobs/view/4223028203/   \n",
       "35  https://www.linkedin.com/jobs/view/4227768803/   \n",
       "36  https://www.linkedin.com/jobs/view/4219697991/   \n",
       "37  https://www.linkedin.com/jobs/view/4167833411/   \n",
       "38  https://www.linkedin.com/jobs/view/4225587960/   \n",
       "39  https://www.linkedin.com/jobs/view/4216737267/   \n",
       "40  https://www.linkedin.com/jobs/view/4227773489/   \n",
       "41  https://www.linkedin.com/jobs/view/4210045636/   \n",
       "42  https://www.linkedin.com/jobs/view/4167834410/   \n",
       "43  https://www.linkedin.com/jobs/view/4225588935/   \n",
       "44  https://www.linkedin.com/jobs/view/4212352065/   \n",
       "45  https://www.linkedin.com/jobs/view/4219923815/   \n",
       "46  https://www.linkedin.com/jobs/view/4205465977/   \n",
       "\n",
       "                                      job_description   seniority_level   \n",
       "0   We are seeking an experienced \\nChief Data Eng...  mid-senior_level  \\\n",
       "1   We are seeking an experienced Chief Data Engin...  mid-senior_level   \n",
       "2   MAS Analytics\\n es una consultora de datos e i...    not_applicable   \n",
       "3   2Brains es una empresa dedicada a construir y ...  mid-senior_level   \n",
       "4   ARKHO es una consultora experta en tecnologías...  mid-senior_level   \n",
       "5   We are seeking a highly skilled and motivated ...         associate   \n",
       "6   Descripción Empresa\\nSomos más de 80 mil perso...  mid-senior_level   \n",
       "7   Descripción Empresa\\nSomos más de 80 mil perso...  mid-senior_level   \n",
       "8   Somos una empresa con más de 34 años de experi...  mid-senior_level   \n",
       "9   NeuralWorks es una compañía de alto crecimient...    not_applicable   \n",
       "10  ¿Sos \\nData Engineer\\n? Entonces… ¿Qué estás e...  mid-senior_level   \n",
       "11  Somos una FinTech que busca democratizar los s...    not_applicable   \n",
       "12  Somos una FinTech que busca democratizar los s...    not_applicable   \n",
       "13  ¿Quiénes somos?\\nEn Option, creemos en un mund...       entry_level   \n",
       "14  Experience Remote done Right. With over 20 yea...  mid-senior_level   \n",
       "15  ¿Buscas generar impactos significativos con tu...  mid-senior_level   \n",
       "16  Company Description: Imagemaker\\nJob Descripti...  mid-senior_level   \n",
       "17  ✔️\\n¿Quiénes Somos?\\nSomos una consultora enfo...       entry_level   \n",
       "18  WORK AT THE HEART OF THE CHANGE\\nEmprende una ...       entry_level   \n",
       "19  .✔️\\n¿Quiénes Somos?\\nSomos una consultora enf...  mid-senior_level   \n",
       "20  🚀 ¡En SII Group estamos en búsqueda de un/a Da...         associate   \n",
       "21  Trabajamos para brindar tranquilidad y segurid...         associate   \n",
       "22  El rol de Senior Data Engineer en el equipo de...  mid-senior_level   \n",
       "23  El rol de Senior Data Engineer en el equipo de...  mid-senior_level   \n",
       "24  El rol de Data Engineer en el equipo de Data E...  mid-senior_level   \n",
       "25  Como Data Engineer, estarás a cargo de integra...  mid-senior_level   \n",
       "26  At Globant, we are working to make the world a...  mid-senior_level   \n",
       "27  ¿Buscas generar impactos significativos con tu...  mid-senior_level   \n",
       "28  ¿Buscas generar impactos significativos con tu...  mid-senior_level   \n",
       "29  ✈️ En \\nLATAM \\nte invitamos a elevar cada via...  mid-senior_level   \n",
       "30  Company Description: WMPartner Consultores\\nJo...       entry_level   \n",
       "31  En \\nTecSystems \\nestamos en busca de un profe...  mid-senior_level   \n",
       "32  En Kabeli, estamos en búsqueda de un Ingeniero...    not_applicable   \n",
       "33  ¡\\nÚnete a Elitsoft!\\n Somos una empresa consu...  mid-senior_level   \n",
       "34  Ingeniero de Datos\\nImagina ser parte de un eq...         associate   \n",
       "35  Descripción de la Empresa:\\n Tecnoandina se de...       entry_level   \n",
       "36  En \\nSII Group\\n estamos en búsqueda de un/a 🕵...         associate   \n",
       "37  We are seeking a \\nLead Data Engineer\\n to joi...  mid-senior_level   \n",
       "38  We are looking for a\\n remote Lead Data Engine...  mid-senior_level   \n",
       "39  WHAT MAKES US A GREAT PLACE TO WORK\\nWe are pr...    not_applicable   \n",
       "40  ¿Quiénes somos?\\nEn \\nOption\\n creemos en un m...  mid-senior_level   \n",
       "41  Description\\nRia Money Transfer, a business se...  mid-senior_level   \n",
       "42  We are seeking a \\nSenior Data Engineer\\n to j...  mid-senior_level   \n",
       "43  We are currently looking for a\\n remote Senior...  mid-senior_level   \n",
       "44  Join our dynamic team as a \\nSenior Data Engin...  mid-senior_level   \n",
       "45  Descripción Empresa\\nSomos más de 90 mil perso...  mid-senior_level   \n",
       "46  Descripción Empresa\\nSomos más de 90 mil perso...  mid-senior_level   \n",
       "\n",
       "   employment_type                                       job_function   \n",
       "0        full-time  engineering,_information_technology,_and_busin...  \\\n",
       "1        full-time  engineering,_information_technology,_and_busin...   \n",
       "2        full-time                             information_technology   \n",
       "3        full-time                             information_technology   \n",
       "4        full-time                             information_technology   \n",
       "5        full-time  information_technology,_engineering,_and_busin...   \n",
       "6        full-time                             information_technology   \n",
       "7        full-time                             information_technology   \n",
       "8        full-time                             information_technology   \n",
       "9        full-time                             information_technology   \n",
       "10       full-time                                         consulting   \n",
       "11       full-time                             information_technology   \n",
       "12       full-time                             information_technology   \n",
       "13       full-time                             information_technology   \n",
       "14       full-time  engineering,_information_technology,_and_consu...   \n",
       "15       full-time  consulting,_information_technology,_and_projec...   \n",
       "16       full-time                             information_technology   \n",
       "17       full-time              information_technology_and_consulting   \n",
       "18       full-time                             information_technology   \n",
       "19       full-time                             information_technology   \n",
       "20       full-time                             information_technology   \n",
       "21       full-time                             information_technology   \n",
       "22       full-time                             information_technology   \n",
       "23       full-time                             information_technology   \n",
       "24       full-time                             information_technology   \n",
       "25       full-time                             information_technology   \n",
       "26       full-time                             information_technology   \n",
       "27       full-time  consulting,_information_technology,_and_projec...   \n",
       "28       full-time  consulting,_information_technology,_and_projec...   \n",
       "29       full-time                             information_technology   \n",
       "30       full-time                             information_technology   \n",
       "31       full-time                             information_technology   \n",
       "32       full-time                             information_technology   \n",
       "33       full-time                             information_technology   \n",
       "34       full-time             engineering_and_information_technology   \n",
       "35       full-time                             information_technology   \n",
       "36       full-time                             information_technology   \n",
       "37       full-time  information_technology,_engineering,_and_busin...   \n",
       "38       full-time  engineering,_information_technology,_and_busin...   \n",
       "39       full-time                                        engineering   \n",
       "40       full-time             engineering_and_information_technology   \n",
       "41       full-time                             information_technology   \n",
       "42       full-time  information_technology,_engineering,_and_busin...   \n",
       "43       full-time  information_technology,_engineering,_and_research   \n",
       "44       full-time  information_technology,_engineering,_and_busin...   \n",
       "45       full-time                             information_technology   \n",
       "46       full-time                             information_technology   \n",
       "\n",
       "                                           industries   \n",
       "0   software_development,_it_services_and_it_consu...  \\\n",
       "1   software_development,_it_services_and_it_consu...   \n",
       "2   technology,_information_and_internet_and_infor...   \n",
       "3   technology,_information_and_internet_and_infor...   \n",
       "4   technology,_information_and_internet_and_infor...   \n",
       "5   software_development,_it_services_and_it_consu...   \n",
       "6                                              retail   \n",
       "7                                              retail   \n",
       "8         software_development_and_financial_services   \n",
       "9   technology,_information_and_internet_and_infor...   \n",
       "10                  technology,_information_and_media   \n",
       "11  hospitality,_food_and_beverage_services,_and_r...   \n",
       "12  hospitality,_food_and_beverage_services,_and_r...   \n",
       "13                  information_technology_&_services   \n",
       "14                      it_services_and_it_consulting   \n",
       "15                   business_consulting_and_services   \n",
       "16                      it_services_and_it_consulting   \n",
       "17                      it_services_and_it_consulting   \n",
       "18                   business_consulting_and_services   \n",
       "19  it_services_and_it_consulting_and_information_...   \n",
       "20                      it_services_and_it_consulting   \n",
       "21              banking_and_health_and_human_services   \n",
       "22                                 financial_services   \n",
       "23                                 financial_services   \n",
       "24                                 financial_services   \n",
       "25                                 financial_services   \n",
       "26                      it_services_and_it_consulting   \n",
       "27                   business_consulting_and_services   \n",
       "28                   business_consulting_and_services   \n",
       "29                              airlines_and_aviation   \n",
       "30                                 financial_services   \n",
       "31                      it_services_and_it_consulting   \n",
       "32  staffing_and_recruiting_and_technology,_inform...   \n",
       "33                      it_services_and_it_consulting   \n",
       "34           it_services_and_it_consulting_and_mining   \n",
       "35                      it_services_and_it_consulting   \n",
       "36                      it_services_and_it_consulting   \n",
       "37  software_development,_it_services_and_it_consu...   \n",
       "38  software_development,_it_services_and_it_consu...   \n",
       "39                   business_consulting_and_services   \n",
       "40                  information_technology_&_services   \n",
       "41                                 financial_services   \n",
       "42  software_development,_it_services_and_it_consu...   \n",
       "43  software_development,_it_services_and_it_consu...   \n",
       "44  software_development,_it_services_and_it_consu...   \n",
       "45                                             retail   \n",
       "46                                             retail   \n",
       "\n",
       "                                         job_url_1 task_clarity   \n",
       "0   https://www.linkedin.com/jobs/view/4220611480/         High  \\\n",
       "1   https://www.linkedin.com/jobs/view/4212167524/         High   \n",
       "2   https://www.linkedin.com/jobs/view/4223641021/         High   \n",
       "3   https://www.linkedin.com/jobs/view/4216874808/         High   \n",
       "4   https://www.linkedin.com/jobs/view/4223640231/       Medium   \n",
       "5   https://www.linkedin.com/jobs/view/4225594752/         High   \n",
       "6   https://www.linkedin.com/jobs/view/4207045620/         High   \n",
       "7   https://www.linkedin.com/jobs/view/4225586690/       Medium   \n",
       "8   https://www.linkedin.com/jobs/view/4218817757/         High   \n",
       "9   https://www.linkedin.com/jobs/view/4216875700/         High   \n",
       "10  https://www.linkedin.com/jobs/view/4189755534/       Medium   \n",
       "11  https://www.linkedin.com/jobs/view/4205474021/       Medium   \n",
       "12  https://www.linkedin.com/jobs/view/4224174809/       Medium   \n",
       "13  https://www.linkedin.com/jobs/view/4219824203/       Medium   \n",
       "14  https://www.linkedin.com/jobs/view/4228057809/       Medium   \n",
       "15  https://www.linkedin.com/jobs/view/4221350232/       Medium   \n",
       "16  https://www.linkedin.com/jobs/view/4219332744/         High   \n",
       "17  https://www.linkedin.com/jobs/view/4226385320/       Medium   \n",
       "18  https://www.linkedin.com/jobs/view/4157214213/       Medium   \n",
       "19  https://www.linkedin.com/jobs/view/4219126754/         High   \n",
       "20  https://www.linkedin.com/jobs/view/4223983495/         High   \n",
       "21  https://www.linkedin.com/jobs/view/4226383768/       Medium   \n",
       "22  https://www.linkedin.com/jobs/view/4219005225/         High   \n",
       "23  https://www.linkedin.com/jobs/view/4226352405/         High   \n",
       "24  https://www.linkedin.com/jobs/view/4221378934/         High   \n",
       "25  https://www.linkedin.com/jobs/view/4144902157/       Medium   \n",
       "26  https://www.linkedin.com/jobs/view/4222832762/          Low   \n",
       "27  https://www.linkedin.com/jobs/view/4218749215/         High   \n",
       "28  https://www.linkedin.com/jobs/view/4218749139/         High   \n",
       "29  https://www.linkedin.com/jobs/view/4217041945/         High   \n",
       "30  https://www.linkedin.com/jobs/view/4219332714/          Low   \n",
       "31  https://www.linkedin.com/jobs/view/4228080982/         High   \n",
       "32  https://www.linkedin.com/jobs/view/4222894601/       Medium   \n",
       "33  https://www.linkedin.com/jobs/view/4221748679/         High   \n",
       "34  https://www.linkedin.com/jobs/view/4223028203/       Medium   \n",
       "35  https://www.linkedin.com/jobs/view/4227768803/         High   \n",
       "36  https://www.linkedin.com/jobs/view/4219697991/          Low   \n",
       "37  https://www.linkedin.com/jobs/view/4167833411/         High   \n",
       "38  https://www.linkedin.com/jobs/view/4225587960/         High   \n",
       "39  https://www.linkedin.com/jobs/view/4216737267/         High   \n",
       "40  https://www.linkedin.com/jobs/view/4227773489/         High   \n",
       "41  https://www.linkedin.com/jobs/view/4210045636/         High   \n",
       "42  https://www.linkedin.com/jobs/view/4167834410/         High   \n",
       "43  https://www.linkedin.com/jobs/view/4225588935/       Medium   \n",
       "44  https://www.linkedin.com/jobs/view/4212352065/       Medium   \n",
       "45  https://www.linkedin.com/jobs/view/4219923815/         High   \n",
       "46  https://www.linkedin.com/jobs/view/4205465977/       Medium   \n",
       "\n",
       "   seniority_level_ai requires_degree_it mentions_certifications   \n",
       "0     Lead or greater                Yes                      No  \\\n",
       "1     Lead or greater                Yes                      No   \n",
       "2              Junior                Yes                      No   \n",
       "3              Senior                 No                     Yes   \n",
       "4          Mid-Senior                 No                     Yes   \n",
       "5          Mid-Senior                 No                      No   \n",
       "6          Mid-Senior                Yes                      No   \n",
       "7          Mid-Senior                Yes                      No   \n",
       "8          Mid-Senior                 No                      No   \n",
       "9          Mid-Senior                Yes                      No   \n",
       "10             Senior                 No                      No   \n",
       "11         Mid-Senior                 No                      No   \n",
       "12         Mid-Senior                 No                      No   \n",
       "13         Mid-Senior                 No                     Yes   \n",
       "14             Senior                 No                      No   \n",
       "15         Mid-Senior                Yes                     Yes   \n",
       "16             Senior                 No                      No   \n",
       "17             Junior                Yes                      No   \n",
       "18         Mid-Senior                 No                     Yes   \n",
       "19         Mid-Senior                 No                      No   \n",
       "20         Mid-Senior                 No                      No   \n",
       "21             Junior                Yes                      No   \n",
       "22             Senior                 No                      No   \n",
       "23             Senior                 No                      No   \n",
       "24         Mid-Senior                 No                      No   \n",
       "25             Senior                 No                     Yes   \n",
       "26             Senior                 No                      No   \n",
       "27         Mid-Senior                Yes                     Yes   \n",
       "28         Mid-Senior                Yes                     Yes   \n",
       "29             Senior                Yes                      No   \n",
       "30         Mid-Senior                 No                     Yes   \n",
       "31         Mid-Senior                Yes                      No   \n",
       "32         Mid-Senior                Yes                     Yes   \n",
       "33             Senior                 No                      No   \n",
       "34         Mid-Senior                Yes                      No   \n",
       "35         Mid-Senior                Yes                      No   \n",
       "36             Junior                 No                      No   \n",
       "37    Lead or greater                Yes                      No   \n",
       "38    Lead or greater                 No                      No   \n",
       "39    Lead or greater                Yes                      No   \n",
       "40    Lead or greater                 No                      No   \n",
       "41         Mid-Senior                Yes                      No   \n",
       "42             Senior                Yes                      No   \n",
       "43             Senior                 No                      No   \n",
       "44             Senior                Yes                      No   \n",
       "45         Mid-Senior                Yes                     Yes   \n",
       "46             Senior                Yes                     Yes   \n",
       "\n",
       "   years_of_experience is_in_english cloud_preference   \n",
       "0                   7+           Yes              AWS  \\\n",
       "1                   7+           Yes              AWS   \n",
       "2        Not Specified            No  Multiple Clouds   \n",
       "3                   5+            No  Multiple Clouds   \n",
       "4        Not Specified            No              AWS   \n",
       "5                   2+           Yes            Azure   \n",
       "6        Not Specified            No              GCP   \n",
       "7        Not Specified            No              GCP   \n",
       "8                   3+            No              AWS   \n",
       "9                   3+           Yes  Multiple Clouds   \n",
       "10                  3+            No  Multiple Clouds   \n",
       "11       Not Specified            No              GCP   \n",
       "12       Not Specified            No              GCP   \n",
       "13       Not Specified            No            Azure   \n",
       "14                  3+           Yes              AWS   \n",
       "15                  3+           Yes            Azure   \n",
       "16       Not Specified            No              GCP   \n",
       "17       Not Specified            No       No Mention   \n",
       "18                  1+            No            Azure   \n",
       "19       Not Specified            No              GCP   \n",
       "20       Not Specified            No            Azure   \n",
       "21                  1+            No              AWS   \n",
       "22                  3+            No  Multiple Clouds   \n",
       "23                  3+            No  Multiple Clouds   \n",
       "24                  2+            No  Multiple Clouds   \n",
       "25                  1+            No              GCP   \n",
       "26       Not Specified           Yes              GCP   \n",
       "27                  2+           Yes              AWS   \n",
       "28                  2+            No              GCP   \n",
       "29                  3+           Yes       No Mention   \n",
       "30       Not Specified            No            Azure   \n",
       "31                  3+           Yes            Azure   \n",
       "32                  3+            No            Azure   \n",
       "33                  5+            No              GCP   \n",
       "34       Not Specified            No            Azure   \n",
       "35                  2+            No            Azure   \n",
       "36       Not Specified            No            Azure   \n",
       "37                  5+           Yes              AWS   \n",
       "38                  5+           Yes              AWS   \n",
       "39                  5+           Yes  Multiple Clouds   \n",
       "40       Not Specified           Yes              GCP   \n",
       "41                  5+           Yes       No Mention   \n",
       "42                  3+           Yes              AWS   \n",
       "43       Not Specified           Yes              AWS   \n",
       "44                  3+           Yes              AWS   \n",
       "45                  3+           Yes              GCP   \n",
       "46                  5+            No              GCP   \n",
       "\n",
       "                                     skills_mentioned  \n",
       "0   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "1   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "2   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "3   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "4   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "5   [Develop pipelines or ETL/ELT processes, Spark...  \n",
       "6   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "7   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "8   [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "9   [Develop pipelines or ETL/ELT processes, APIs,...  \n",
       "10  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "11  [Develop pipelines or ETL/ELT processes, Autom...  \n",
       "12  [Develop pipelines or ETL/ELT processes, Autom...  \n",
       "13  [Develop pipelines or ETL/ELT processes, Spark...  \n",
       "14  [Develop pipelines or ETL/ELT processes, Spark...  \n",
       "15  [Databricks or snowflake, Develop pipelines or...  \n",
       "16  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "17           [Develop pipelines or ETL/ELT processes]  \n",
       "18  [Develop pipelines or ETL/ELT processes, Versi...  \n",
       "19  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "20  [Develop pipelines or ETL/ELT processes, CI/CD...  \n",
       "21  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "22  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "23  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "24  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "25  [Develop pipelines or ETL/ELT processes, Autom...  \n",
       "26  [Spark knowledge, None of the above mentioned ...  \n",
       "27  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "28  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "29  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "30                                                 []  \n",
       "31  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "32  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "33  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "34  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "35  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "36  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "37  [Databricks or snowflake, Develop pipelines or...  \n",
       "38  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "39  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "40  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "41  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "42  [Databricks or snowflake, Develop pipelines or...  \n",
       "43  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "44  [Databricks or snowflake, Develop pipelines or...  \n",
       "45  [Develop pipelines or ETL/ELT processes, Data ...  \n",
       "46  [Develop pipelines or ETL/ELT processes, Data ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con2.sql(\"\"\"\n",
    "        SELECT t1.*, t2.*\n",
    "        FROM base_table t1\n",
    "        JOIN genai_table t2\n",
    "        ON t1.job_url = t2.job_url\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY t1.title, t1.company, t1.date ORDER BY t1.date DESC) = 1\n",
    "        ORDER BY t1.title, t1.company, t1.date\n",
    "        \"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b1346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"\"\"INSERT INTO test_table \n",
    "        SELECT * FROM output_db.base_table\n",
    "        WHERE job_url NOT IN (SELECT job_url FROM test_table)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d23230",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"\"\"INSERT INTO genai_table \n",
    "        SELECT * FROM output_db.genai_table\n",
    "        WHERE job_url NOT IN (SELECT job_url FROM genai_table)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b8d6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┐\n",
       "│ count_star() │\n",
       "│    int64     │\n",
       "├──────────────┤\n",
       "│          131 │\n",
       "└──────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"SELECT COUNT(*) FROM genai_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f7d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf51d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3291500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
